{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stock-wmt-blog', 'stock-ibm-441', 'stock-msft']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uncomment below line of code if you want to calculate features and save dataframe\n",
    "# this script prints the path at which dataframe with calculated features is saved.\n",
    "# train.py calls the DataGenerator class to \n",
    "\n",
    "# %run ./train.py WMT original\n",
    "\n",
    "# this notebook was trained on cloud compute. So use your own paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_cell_guid": "",
    "_uuid": "",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adjusted_close</th>\n",
       "      <th>volume</th>\n",
       "      <th>rsi_6</th>\n",
       "      <th>rsi_7</th>\n",
       "      <th>rsi_8</th>\n",
       "      <th>...</th>\n",
       "      <th>eom_19</th>\n",
       "      <th>eom_20</th>\n",
       "      <th>eom_21</th>\n",
       "      <th>eom_22</th>\n",
       "      <th>eom_23</th>\n",
       "      <th>eom_24</th>\n",
       "      <th>eom_25</th>\n",
       "      <th>eom_26</th>\n",
       "      <th>volume_delta</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-03-07</td>\n",
       "      <td>49.63</td>\n",
       "      <td>49.63</td>\n",
       "      <td>47.13</td>\n",
       "      <td>47.56</td>\n",
       "      <td>33.0302</td>\n",
       "      <td>9724700</td>\n",
       "      <td>58.964317</td>\n",
       "      <td>47.627417</td>\n",
       "      <td>50.391499</td>\n",
       "      <td>...</td>\n",
       "      <td>-58.613633</td>\n",
       "      <td>-58.613633</td>\n",
       "      <td>-58.613633</td>\n",
       "      <td>-58.613633</td>\n",
       "      <td>-58.613633</td>\n",
       "      <td>-58.613633</td>\n",
       "      <td>-58.613633</td>\n",
       "      <td>-58.613633</td>\n",
       "      <td>-768900.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-03-08</td>\n",
       "      <td>47.31</td>\n",
       "      <td>49.31</td>\n",
       "      <td>45.63</td>\n",
       "      <td>48.31</td>\n",
       "      <td>33.5511</td>\n",
       "      <td>10018700</td>\n",
       "      <td>59.145850</td>\n",
       "      <td>63.978686</td>\n",
       "      <td>51.540226</td>\n",
       "      <td>...</td>\n",
       "      <td>-33.425494</td>\n",
       "      <td>-33.425494</td>\n",
       "      <td>-33.425494</td>\n",
       "      <td>-33.425494</td>\n",
       "      <td>-33.425494</td>\n",
       "      <td>-33.425494</td>\n",
       "      <td>-33.425494</td>\n",
       "      <td>-33.425494</td>\n",
       "      <td>294000.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-03-09</td>\n",
       "      <td>49.00</td>\n",
       "      <td>49.44</td>\n",
       "      <td>46.88</td>\n",
       "      <td>48.94</td>\n",
       "      <td>33.9887</td>\n",
       "      <td>7944900</td>\n",
       "      <td>51.511094</td>\n",
       "      <td>60.984994</td>\n",
       "      <td>65.439673</td>\n",
       "      <td>...</td>\n",
       "      <td>22.233131</td>\n",
       "      <td>22.233131</td>\n",
       "      <td>22.233131</td>\n",
       "      <td>22.233131</td>\n",
       "      <td>22.233131</td>\n",
       "      <td>22.233131</td>\n",
       "      <td>22.233131</td>\n",
       "      <td>22.233131</td>\n",
       "      <td>-2073800.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-03-10</td>\n",
       "      <td>49.75</td>\n",
       "      <td>49.75</td>\n",
       "      <td>47.44</td>\n",
       "      <td>47.94</td>\n",
       "      <td>33.2942</td>\n",
       "      <td>7005500</td>\n",
       "      <td>44.414894</td>\n",
       "      <td>45.752392</td>\n",
       "      <td>55.726357</td>\n",
       "      <td>...</td>\n",
       "      <td>14.343730</td>\n",
       "      <td>14.343730</td>\n",
       "      <td>14.343730</td>\n",
       "      <td>14.343730</td>\n",
       "      <td>14.343730</td>\n",
       "      <td>14.343730</td>\n",
       "      <td>14.343730</td>\n",
       "      <td>14.343730</td>\n",
       "      <td>-939400.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-03-13</td>\n",
       "      <td>46.25</td>\n",
       "      <td>47.94</td>\n",
       "      <td>45.88</td>\n",
       "      <td>47.69</td>\n",
       "      <td>33.1205</td>\n",
       "      <td>7727900</td>\n",
       "      <td>35.132383</td>\n",
       "      <td>44.056866</td>\n",
       "      <td>45.283483</td>\n",
       "      <td>...</td>\n",
       "      <td>-44.916471</td>\n",
       "      <td>-44.916471</td>\n",
       "      <td>-44.916471</td>\n",
       "      <td>-44.916471</td>\n",
       "      <td>-44.916471</td>\n",
       "      <td>-44.916471</td>\n",
       "      <td>-44.916471</td>\n",
       "      <td>-44.916471</td>\n",
       "      <td>722400.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 450 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp   open   high    low  close  adjusted_close    volume  \\\n",
       "0  2000-03-07  49.63  49.63  47.13  47.56         33.0302   9724700   \n",
       "1  2000-03-08  47.31  49.31  45.63  48.31         33.5511  10018700   \n",
       "2  2000-03-09  49.00  49.44  46.88  48.94         33.9887   7944900   \n",
       "3  2000-03-10  49.75  49.75  47.44  47.94         33.2942   7005500   \n",
       "4  2000-03-13  46.25  47.94  45.88  47.69         33.1205   7727900   \n",
       "\n",
       "       rsi_6      rsi_7      rsi_8  ...     eom_19     eom_20     eom_21  \\\n",
       "0  58.964317  47.627417  50.391499  ... -58.613633 -58.613633 -58.613633   \n",
       "1  59.145850  63.978686  51.540226  ... -33.425494 -33.425494 -33.425494   \n",
       "2  51.511094  60.984994  65.439673  ...  22.233131  22.233131  22.233131   \n",
       "3  44.414894  45.752392  55.726357  ...  14.343730  14.343730  14.343730   \n",
       "4  35.132383  44.056866  45.283483  ... -44.916471 -44.916471 -44.916471   \n",
       "\n",
       "      eom_22     eom_23     eom_24     eom_25     eom_26  volume_delta  labels  \n",
       "0 -58.613633 -58.613633 -58.613633 -58.613633 -58.613633     -768900.0       2  \n",
       "1 -33.425494 -33.425494 -33.425494 -33.425494 -33.425494      294000.0       2  \n",
       "2  22.233131  22.233131  22.233131  22.233131  22.233131    -2073800.0       2  \n",
       "3  14.343730  14.343730  14.343730  14.343730  14.343730     -939400.0       0  \n",
       "4 -44.916471 -44.916471 -44.916471 -44.916471 -44.916471      722400.0       2  \n",
       "\n",
       "[5 rows x 450 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle \n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "\n",
    "np.random.seed(2)\n",
    "company_code = 'WMT'\n",
    "strategy_type = 'original'\n",
    "# use the path printed in above output cell after running stock_cnn.py. It's in below format\n",
    "df = pd.read_csv(\"../input/stock-wmt-blog/df_\"+company_code+\".csv\")\n",
    "df['labels'] = df['labels'].astype(np.int8)\n",
    "if 'dividend_amount' in df.columns:\n",
    "    df.drop(columns=['dividend_amount', 'split_coefficient'], inplace=True)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into Training, Validation and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of features 447\n",
      "train_split = 0.7\n",
      "Shape of x, y train/cv/test (2797, 447) (2797,) (1199, 447) (1199,) (1000, 447) (1000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n",
    "from collections import Counter\n",
    "\n",
    "list_features = list(df.loc[:, 'open':'eom_26'].columns)\n",
    "print('Total number of features', len(list_features))\n",
    "x_train, x_test, y_train, y_test = train_test_split(df.loc[:, 'open':'eom_26'].values, df['labels'].values, train_size=0.8, \n",
    "                                                    test_size=0.2, random_state=2, shuffle=True, stratify=df['labels'].values)\n",
    "\n",
    "# smote = RandomOverSampler(random_state=42, sampling_strategy='not majority')\n",
    "# x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "# print('Resampled dataset shape %s' % Counter(y_train))\n",
    "\n",
    "if 0.7*x_train.shape[0] < 2500:\n",
    "    train_split = 0.8\n",
    "else:\n",
    "    train_split = 0.7\n",
    "# train_split = 0.7\n",
    "print('train_split =',train_split)\n",
    "x_train, x_cv, y_train, y_cv = train_test_split(x_train, y_train, train_size=train_split, test_size=1-train_split, \n",
    "                                                random_state=2, shuffle=True, stratify=y_train)\n",
    "mm_scaler = MinMaxScaler(feature_range=(0, 1)) # or StandardScaler?\n",
    "x_train = mm_scaler.fit_transform(x_train)\n",
    "x_cv = mm_scaler.transform(x_cv)\n",
    "x_test = mm_scaler.transform(x_test)\n",
    "\n",
    "x_main = x_train.copy()\n",
    "print(\"Shape of x, y train/cv/test {} {} {} {} {} {}\".format(x_train.shape, y_train.shape, x_cv.shape, y_cv.shape, x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of total 441+ features select top 'N' features (let's include base features like close, adjusted_close etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 225  # should be a perfect square\n",
    "selection_method = 'all'\n",
    "topk = 320 if selection_method == 'all' else num_features\n",
    "# if train_split >= 0.8:\n",
    "#     topk = 400\n",
    "# else:\n",
    "#     topk = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('rsi_6', 'rsi_7', 'wr_6', 'wr_7', 'wr_8', 'wr_9', 'wr_10', 'wr_11', 'wr_12', 'wr_13', 'wr_14', 'wr_15', 'wr_16', 'wr_17', 'wr_18', 'wr_19', 'wr_20', 'wr_21', 'wr_22', 'wr_23', 'wr_24', 'wr_25', 'wr_26', 'mfi_6', 'mfi_7', 'roc_6', 'roc_7', 'roc_8', 'cmf_6', 'cmf_7', 'cmo_6', 'cmo_7', 'cmo_8', 'close_sma_6', 'close_sma_7', 'close_sma_8', 'close_sma_9', 'close_sma_10', 'close_sma_11', 'close_sma_12', 'close_sma_13', 'close_sma_14', 'close_sma_15', 'close_sma_16', 'close_sma_17', 'close_sma_18', 'close_sma_19', 'close_sma_20', 'close_sma_21', 'close_sma_22', 'close_sma_23', 'close_sma_24', 'open_sma_6', 'open_sma_7', 'open_sma_8', 'open_sma_9', 'open_sma_10', 'open_sma_11', 'open_sma_12', 'open_sma_13', 'open_sma_14', 'open_sma_15', 'open_sma_16', 'open_sma_17', 'open_sma_18', 'open_sma_19', 'open_sma_20', 'open_sma_21', 'open_sma_22', 'open_sma_23', 'open_sma_24', 'ema_6', 'ema_7', 'ema_8', 'ema_9', 'ema_10', 'ema_11', 'ema_12', 'ema_13', 'ema_14', 'ema_15', 'ema_16', 'ema_17', 'ema_18', 'ema_19', 'ema_20', 'ema_21', 'ema_22', 'ema_23', 'ema_24', 'ema_25', 'ema_26', 'wma_6', 'wma_7', 'wma_8', 'wma_9', 'wma_10', 'wma_11', 'wma_12', 'wma_13', 'wma_14', 'wma_15', 'wma_16', 'wma_17', 'wma_18', 'wma_19', 'wma_20', 'wma_21', 'wma_22', 'wma_23', 'wma_24', 'wma_25', 'wma_26', 'hma_5', 'hma_6', 'hma_7', 'hma_8', 'hma_9', 'hma_10', 'hma_11', 'hma_12', 'hma_13', 'hma_14', 'hma_15', 'hma_16', 'hma_17', 'hma_18', 'hma_19', 'hma_20', 'trix_7', 'trix_8', 'trix_9', 'trix_10', 'trix_11', 'trix_12', 'trix_13', 'trix_14', 'trix_15', 'trix_16', 'trix_17', 'trix_18', 'trix_19', 'trix_20', 'trix_21', 'trix_22', 'trix_23', 'trix_24', 'trix_25', 'trix_26', 'cci_6', 'cci_7', 'cci_8', 'cci_9', 'cci_10', 'cci_11', 'cci_12', 'cci_13', 'cci_14', 'cci_15', 'cci_16', 'cci_17', 'cci_18', 'cci_19', 'cci_20', 'cci_21', 'cci_22', 'cci_23', 'cci_24', 'cci_25', 'cci_26', 'dpo_6', 'dpo_7', 'dpo_8', 'dpo_9', 'dpo_10', 'dpo_11', 'dpo_12', 'dpo_13', 'dpo_14', 'dpo_15', 'dpo_16', 'dpo_17', 'dpo_18', 'dpo_19', 'dpo_20', 'dpo_21', 'dpo_22', 'dpo_23', 'dpo_24', 'dpo_25', 'kst_6', 'kst_7', 'kst_8', 'kst_9', 'kst_10', 'kst_11', 'kst_12', 'kst_13', 'kst_14', 'kst_15', 'kst_16', 'kst_17', 'kst_18', 'kst_19', 'kst_20', 'kst_21', 'kst_22', 'kst_23', 'kst_24', 'kst_25', 'kst_26', 'dmi_6', 'dmi_7', 'dmi_8', 'dmi_9', 'dmi_10', 'dmi_11', 'dmi_12', 'dmi_13', 'dmi_14', 'dmi_15', 'dmi_16', 'dmi_17', 'dmi_18', 'dmi_19', 'dmi_20', 'dmi_21', 'dmi_22', 'dmi_23', 'dmi_24', 'dmi_25', 'dmi_26', 'bb_6', 'bb_7', 'bb_8', 'bb_9', 'bb_10', 'bb_11', 'bb_12', 'bb_13', 'bb_14', 'bb_15', 'bb_16', 'bb_17', 'bb_18', 'bb_19', 'bb_20', 'bb_21', 'bb_22', 'bb_23', 'bb_24', 'fi_6', 'fi_7', 'fi_8', 'fi_9', 'fi_10', 'fi_11', 'fi_12', 'fi_13', 'fi_14', 'fi_15', 'fi_16', 'fi_17', 'fi_18', 'fi_19', 'fi_20', 'fi_21', 'fi_22', 'fi_23', 'fi_24', 'fi_25', 'fi_26', 'rsv_6', 'kdjk_6', 'rsv_7', 'kdjk_7', 'rsv_8', 'kdjk_8', 'rsv_9', 'kdjk_9', 'rsv_10', 'kdjk_10', 'rsv_11', 'kdjk_11', 'rsv_12', 'rsv_13', 'rsv_14', 'rsv_15', 'rsv_16', 'rsv_17', 'rsv_18', 'rsv_19', 'rsv_20', 'rsv_21', 'rsv_22', 'rsv_23', 'rsv_24', 'rsv_25', 'rsv_26', 'eom_6', 'eom_7', 'eom_8', 'eom_9', 'eom_10', 'eom_11', 'eom_12', 'eom_13', 'eom_14', 'eom_15', 'eom_16', 'eom_17', 'eom_18', 'eom_19', 'eom_20', 'eom_21', 'eom_22', 'eom_23', 'eom_24', 'eom_25', 'eom_26')\n",
      "[  6   7  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42\n",
      "  43  44  45  46  47  48  49  69  70  71  90  91 111 112 113 132 133 134\n",
      " 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 153 154\n",
      " 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 174\n",
      " 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192\n",
      " 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210\n",
      " 211 212 213 214 215 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252\n",
      " 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270\n",
      " 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288\n",
      " 289 290 291 292 293 294 295 296 297 298 300 301 302 303 304 305 306 307\n",
      " 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325\n",
      " 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343\n",
      " 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 363\n",
      " 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381\n",
      " 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 398 400 402\n",
      " 404 406 408 410 412 414 416 418 420 422 424 426 427 428 429 430 431 432\n",
      " 433 434 435 436 437 438 439 440 441 442 443 444 445 446]\n",
      "****************************************\n",
      "320 ('close', 'volume', 'rsi_6', 'rsi_7', 'rsi_8', 'rsi_9', 'rsi_10', 'rsi_11', 'rsi_12', 'rsi_13', 'rsi_14', 'rsi_15', 'rsi_16', 'rsi_17', 'rsi_18', 'rsi_19', 'rsi_20', 'rsi_21', 'rsi_22', 'rsi_23', 'rsi_25', 'wr_6', 'wr_7', 'wr_8', 'wr_9', 'wr_10', 'wr_11', 'wr_12', 'wr_13', 'wr_14', 'wr_15', 'wr_16', 'wr_17', 'wr_18', 'wr_19', 'wr_21', 'wr_22', 'wr_23', 'wr_24', 'wr_25', 'wr_26', 'mfi_6', 'mfi_7', 'mfi_8', 'mfi_9', 'mfi_10', 'mfi_11', 'mfi_12', 'mfi_13', 'mfi_15', 'mfi_16', 'mfi_17', 'mfi_18', 'mfi_19', 'mfi_20', 'mfi_22', 'mfi_24', 'mfi_25', 'roc_6', 'roc_7', 'roc_8', 'roc_9', 'roc_10', 'roc_11', 'roc_12', 'roc_13', 'roc_14', 'roc_17', 'roc_18', 'roc_19', 'roc_20', 'roc_21', 'roc_23', 'cmf_6', 'cmf_7', 'cmf_8', 'cmf_10', 'cmf_11', 'cmf_12', 'cmf_13', 'cmf_15', 'cmf_17', 'cmf_19', 'cmf_23', 'cmf_24', 'cmf_25', 'cmf_26', 'cmo_6', 'cmo_7', 'cmo_8', 'cmo_9', 'cmo_10', 'cmo_11', 'cmo_12', 'cmo_13', 'cmo_14', 'cmo_15', 'cmo_16', 'cmo_17', 'cmo_18', 'cmo_19', 'cmo_22', 'cmo_24', 'cmo_26', 'close_sma_11', 'ema_10', 'ema_12', 'ema_15', 'ema_16', 'ema_17', 'wma_7', 'wma_11', 'wma_19', 'wma_20', 'hma_14', 'hma_15', 'trix_6', 'trix_7', 'trix_8', 'trix_9', 'trix_10', 'trix_12', 'trix_17', 'trix_18', 'trix_20', 'trix_21', 'trix_22', 'trix_23', 'trix_24', 'trix_25', 'trix_26', 'cci_6', 'cci_7', 'cci_8', 'cci_9', 'cci_10', 'cci_11', 'cci_12', 'cci_13', 'cci_14', 'cci_15', 'cci_16', 'cci_17', 'cci_18', 'cci_19', 'cci_20', 'cci_21', 'cci_22', 'cci_23', 'cci_24', 'cci_25', 'cci_26', 'dpo_6', 'dpo_7', 'dpo_8', 'dpo_9', 'dpo_10', 'dpo_11', 'dpo_12', 'dpo_13', 'dpo_14', 'dpo_15', 'dpo_16', 'dpo_17', 'dpo_18', 'dpo_19', 'dpo_20', 'dpo_21', 'dpo_22', 'dpo_23', 'dpo_24', 'dpo_25', 'dpo_26', 'kst_6', 'kst_7', 'kst_8', 'kst_9', 'kst_10', 'kst_11', 'kst_12', 'kst_13', 'kst_14', 'kst_15', 'kst_16', 'kst_17', 'kst_18', 'kst_19', 'kst_20', 'kst_21', 'kst_22', 'kst_23', 'kst_24', 'kst_25', 'kst_26', 'dmi_6', 'dmi_7', 'dmi_8', 'dmi_9', 'dmi_10', 'dmi_11', 'dmi_12', 'dmi_13', 'dmi_14', 'dmi_15', 'dmi_16', 'dmi_17', 'dmi_18', 'dmi_19', 'dmi_20', 'dmi_21', 'dmi_22', 'dmi_23', 'dmi_24', 'dmi_25', 'dmi_26', 'bb_6', 'bb_7', 'bb_8', 'bb_9', 'bb_10', 'bb_11', 'bb_12', 'bb_13', 'bb_14', 'bb_15', 'bb_16', 'bb_17', 'bb_18', 'bb_19', 'bb_20', 'bb_21', 'bb_22', 'bb_23', 'bb_24', 'bb_25', 'bb_26', 'fi_6', 'fi_7', 'fi_8', 'fi_9', 'fi_10', 'fi_11', 'fi_12', 'fi_13', 'fi_14', 'fi_15', 'fi_16', 'fi_17', 'fi_18', 'fi_19', 'fi_20', 'fi_21', 'fi_22', 'fi_23', 'fi_24', 'fi_25', 'fi_26', 'rsv_6', 'kdjk_6', 'rsv_7', 'kdjk_7', 'rsv_8', 'kdjk_8', 'rsv_9', 'kdjk_9', 'rsv_10', 'kdjk_10', 'rsv_11', 'kdjk_11', 'rsv_12', 'kdjk_12', 'rsv_13', 'kdjk_13', 'rsv_14', 'kdjk_14', 'rsv_15', 'kdjk_15', 'rsv_16', 'kdjk_16', 'rsv_17', 'kdjk_17', 'rsv_18', 'kdjk_18', 'rsv_19', 'kdjk_19', 'rsv_20', 'kdjk_20', 'rsv_21', 'kdjk_21', 'rsv_22', 'kdjk_22', 'rsv_23', 'kdjk_23', 'rsv_24', 'kdjk_24', 'rsv_25', 'kdjk_25', 'rsv_26', 'kdjk_26', 'eom_6', 'eom_7', 'eom_8', 'eom_9', 'eom_10', 'eom_11', 'eom_12', 'eom_13', 'eom_14', 'eom_15', 'eom_16', 'eom_17', 'eom_18', 'eom_19', 'eom_20', 'eom_21', 'eom_22', 'eom_23', 'eom_24', 'eom_25', 'eom_26')\n",
      "[  3   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21\n",
      "  22  23  25  27  28  29  30  31  32  33  34  35  36  37  38  39  40  42\n",
      "  43  44  45  46  47  48  49  50  51  52  53  54  55  57  58  59  60  61\n",
      "  62  64  66  67  69  70  71  72  73  74  75  76  77  80  81  82  83  84\n",
      "  86  90  91  92  94  95  96  97  99 101 103 107 108 109 110 111 112 113\n",
      " 114 115 116 117 118 119 120 121 122 123 124 127 129 131 137 178 180 183\n",
      " 184 185 196 200 208 209 230 231 237 238 239 240 241 243 248 249 251 252\n",
      " 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270\n",
      " 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288\n",
      " 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306\n",
      " 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324\n",
      " 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342\n",
      " 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360\n",
      " 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378\n",
      " 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396\n",
      " 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414\n",
      " 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432\n",
      " 433 434 435 436 437 438 439 440 441 442 443 444 445 446]\n",
      "CPU times: user 10.9 s, sys: 0 ns, total: 10.9 s\n",
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from operator import itemgetter\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "\n",
    "if selection_method == 'anova' or selection_method == 'all':\n",
    "    select_k_best = SelectKBest(f_classif, k=topk)\n",
    "    if selection_method != 'all':\n",
    "        x_train = select_k_best.fit_transform(x_main, y_train)\n",
    "        x_cv = select_k_best.transform(x_cv)\n",
    "        x_test = select_k_best.transform(x_test)\n",
    "    else:\n",
    "        select_k_best.fit(x_main, y_train)\n",
    "    \n",
    "    selected_features_anova = itemgetter(*select_k_best.get_support(indices=True))(list_features)\n",
    "    print(selected_features_anova)\n",
    "    print(select_k_best.get_support(indices=True))\n",
    "    print(\"****************************************\")\n",
    "    \n",
    "if selection_method == 'mutual_info' or selection_method == 'all':\n",
    "    select_k_best = SelectKBest(mutual_info_classif, k=topk)\n",
    "    if selection_method != 'all':\n",
    "        x_train = select_k_best.fit_transform(x_main, y_train)\n",
    "        x_cv = select_k_best.transform(x_cv)\n",
    "        x_test = select_k_best.transform(x_test)\n",
    "    else:\n",
    "        select_k_best.fit(x_main, y_train)\n",
    "\n",
    "    selected_features_mic = itemgetter(*select_k_best.get_support(indices=True))(list_features)\n",
    "    print(len(selected_features_mic), selected_features_mic)\n",
    "    print(select_k_best.get_support(indices=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common selected featues 229 ['wr_22', 'dpo_9', 'trix_8', 'trix_22', 'cci_21', 'cci_23', 'dmi_8', 'dmi_13', 'bb_14', 'cci_26', 'bb_22', 'kst_25', 'eom_22', 'mfi_6', 'kst_26', 'ema_12', 'trix_26', 'cci_25', 'kst_20', 'fi_24', 'eom_17', 'dpo_12', 'trix_17', 'kst_21', 'bb_7', 'eom_18', 'dpo_15', 'kdjk_10', 'ema_15', 'trix_12', 'kst_12', 'bb_19', 'wr_7', 'eom_26', 'bb_15', 'bb_20', 'eom_23', 'cci_12', 'dmi_17', 'fi_19', 'cci_7', 'eom_13', 'rsv_9', 'kst_24', 'dmi_26', 'fi_21', 'ema_10', 'bb_23', 'roc_8', 'eom_12', 'rsv_6', 'eom_20', 'fi_22', 'dpo_20', 'wr_18', 'cci_14', 'dmi_11', 'dmi_12', 'dpo_17', 'dpo_10', 'kst_10', 'cci_8', 'dpo_24', 'kdjk_9', 'dpo_6', 'dmi_16', 'eom_11', 'eom_24', 'roc_7', 'fi_15', 'cci_22', 'wr_17', 'wr_12', 'wr_13', 'dpo_11', 'cci_15', 'fi_26', 'rsv_11', 'rsv_25', 'rsi_6', 'trix_20', 'kst_13', 'rsv_12', 'eom_7', 'ema_16', 'fi_10', 'wma_11', 'dpo_8', 'wr_8', 'cmf_6', 'wma_19', 'rsv_23', 'fi_13', 'hma_14', 'eom_15', 'wr_11', 'wma_7', 'fi_11', 'fi_12', 'cmo_7', 'wr_23', 'dpo_22', 'dmi_14', 'fi_16', 'dpo_18', 'kst_14', 'rsv_22', 'eom_19', 'dmi_7', 'dpo_14', 'wr_21', 'eom_10', 'rsv_17', 'bb_10', 'rsv_20', 'dmi_24', 'kst_11', 'cci_11', 'dmi_10', 'rsv_24', 'fi_23', 'rsv_26', 'fi_18', 'kst_8', 'dpo_13', 'kst_22', 'cci_19', 'rsv_10', 'trix_7', 'eom_16', 'cci_10', 'eom_21', 'trix_21', 'wr_6', 'cci_6', 'cci_9', 'dmi_15', 'bb_21', 'kst_18', 'bb_12', 'kdjk_8', 'eom_9', 'bb_16', 'kst_6', 'dmi_18', 'dpo_16', 'bb_17', 'trix_25', 'trix_23', 'trix_10', 'rsv_18', 'cci_13', 'dmi_19', 'fi_6', 'wr_26', 'trix_24', 'cci_17', 'dmi_22', 'wr_24', 'dmi_6', 'eom_25', 'rsv_19', 'bb_11', 'wr_10', 'wr_9', 'dpo_25', 'rsv_7', 'dmi_20', 'hma_15', 'dpo_23', 'kst_9', 'wr_19', 'kst_17', 'fi_8', 'cmo_8', 'rsv_16', 'fi_25', 'rsv_13', 'fi_17', 'cci_24', 'close_sma_11', 'wr_25', 'wr_16', 'dmi_21', 'fi_14', 'kst_15', 'cci_20', 'dpo_21', 'kdjk_11', 'roc_6', 'dpo_19', 'bb_18', 'rsv_21', 'kst_23', 'bb_8', 'rsv_14', 'trix_18', 'kdjk_7', 'wr_15', 'bb_24', 'eom_6', 'dmi_23', 'trix_9', 'kdjk_6', 'eom_14', 'fi_7', 'mfi_7', 'eom_8', 'fi_9', 'kst_19', 'ema_17', 'dpo_7', 'bb_13', 'cmf_7', 'wr_14', 'cci_18', 'wma_20', 'rsv_8', 'cmo_6', 'dmi_9', 'bb_6', 'rsv_15', 'kst_7', 'bb_9', 'fi_20', 'cci_16', 'kst_16', 'dmi_25', 'rsi_7']\n",
      "[6, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 69, 70, 71, 90, 91, 111, 112, 113, 137, 178, 180, 183, 184, 185, 196, 200, 208, 209, 230, 231, 238, 239, 240, 241, 243, 248, 249, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414, 416, 418, 420, 422, 424, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446]\n"
     ]
    }
   ],
   "source": [
    "if selection_method == 'all':\n",
    "    common = list(set(selected_features_anova).intersection(selected_features_mic))\n",
    "    print(\"common selected featues\", len(common), common)\n",
    "    if len(common) < num_features:\n",
    "        raise Exception('number of common features found {} < {} required features. Increase \"topk variable\"'.format(len(common), num_features))\n",
    "    feat_idx = []\n",
    "    for c in common:\n",
    "        feat_idx.append(list_features.index(c))\n",
    "    feat_idx = sorted(feat_idx[0:225])\n",
    "    print(feat_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x, y train/cv/test (2797, 225) (2797,) (1199, 225) (1199,) (1000, 225) (1000,)\n"
     ]
    }
   ],
   "source": [
    "if selection_method == 'all':\n",
    "    x_train = x_train[:, feat_idx]\n",
    "    x_cv = x_cv[:, feat_idx]\n",
    "    x_test = x_test[:, feat_idx]\n",
    "\n",
    "print(\"Shape of x, y train/cv/test {} {} {} {} {} {}\".format(x_train.shape, \n",
    "                                                             y_train.shape, x_cv.shape, y_cv.shape, x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of class 0 = 6.363961387200572, class 1 = 6.149445834823024\n"
     ]
    }
   ],
   "source": [
    "_labels, _counts = np.unique(y_train, return_counts=True)\n",
    "print(\"percentage of class 0 = {}, class 1 = {}\".format(_counts[0]/len(y_train) * 100, _counts[1]/len(y_train) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "\n",
    "def get_sample_weights(y):\n",
    "    \"\"\"\n",
    "    calculate the sample weights based on class weights. Used for models with\n",
    "    imbalanced data and one hot encoding prediction.\n",
    "\n",
    "    params:\n",
    "        y: class labels as integers\n",
    "    \"\"\"\n",
    "\n",
    "    y = y.astype(int)  # compute_class_weight needs int labels\n",
    "    class_weights = compute_class_weight('balanced', np.unique(y), y)\n",
    "    \n",
    "    print(\"real class weights are {}\".format(class_weights), np.unique(y))\n",
    "    print(\"value_counts\", np.unique(y, return_counts=True))\n",
    "    sample_weights = y.copy().astype(float)\n",
    "    for i in np.unique(y):\n",
    "        sample_weights[sample_weights == i] = class_weights[i]  # if i == 2 else 0.8 * class_weights[i]\n",
    "        # sample_weights = np.where(sample_weights == i, class_weights[int(i)], y_)\n",
    "\n",
    "    return sample_weights\n",
    "\n",
    "def reshape_as_image(x, img_width, img_height):\n",
    "    x_temp = np.zeros((len(x), img_height, img_width))\n",
    "    for i in range(x.shape[0]):\n",
    "        # print(type(x), type(x_temp), x.shape)\n",
    "        x_temp[i] = np.reshape(x[i], (img_height, img_width))\n",
    "\n",
    "    return x_temp\n",
    "\n",
    "def f1_weighted(y_true, y_pred):\n",
    "    y_true_class = tf.math.argmax(y_true, axis=1, output_type=tf.dtypes.int32)\n",
    "    y_pred_class = tf.math.argmax(y_pred, axis=1, output_type=tf.dtypes.int32)\n",
    "    conf_mat = tf.math.confusion_matrix(y_true_class, y_pred_class)  # can use conf_mat[0, :], tf.slice()\n",
    "    # precision = TP/TP+FP, recall = TP/TP+FN\n",
    "    rows, cols = conf_mat.get_shape()\n",
    "    size = y_true_class.get_shape()[0]\n",
    "    precision = tf.constant([0, 0, 0])  # change this to use rows/cols as size\n",
    "    recall = tf.constant([0, 0, 0])\n",
    "    class_counts = tf.constant([0, 0, 0])\n",
    "\n",
    "    def get_precision(i, conf_mat):\n",
    "        print(\"prec check\", conf_mat, conf_mat[i, i], tf.reduce_sum(conf_mat[:, i]))\n",
    "        precision[i].assign(conf_mat[i, i] / tf.reduce_sum(conf_mat[:, i]))\n",
    "        recall[i].assign(conf_mat[i, i] / tf.reduce_sum(conf_mat[i, :]))\n",
    "        tf.add(i, 1)\n",
    "        return i, conf_mat, precision, recall\n",
    "\n",
    "    def tf_count(i):\n",
    "        elements_equal_to_value = tf.equal(y_true_class, i)\n",
    "        as_ints = tf.cast(elements_equal_to_value, tf.int32)\n",
    "        count = tf.reduce_sum(as_ints)\n",
    "        class_counts[i].assign(count)\n",
    "        tf.add(i, 1)\n",
    "        return count\n",
    "\n",
    "    def condition(i, conf_mat):\n",
    "        return tf.less(i, 3)\n",
    "\n",
    "    i = tf.constant(3)\n",
    "    i, conf_mat = tf.while_loop(condition, get_precision, [i, conf_mat])\n",
    "\n",
    "    i = tf.constant(3)\n",
    "    c = lambda i: tf.less(i, 3)\n",
    "    b = tf_count(i)\n",
    "    tf.while_loop(c, b, [i])\n",
    "\n",
    "    weights = tf.math.divide(class_counts, size)\n",
    "    numerators = tf.math.multiply(tf.math.multiply(precision, recall), tf.constant(2))\n",
    "    denominators = tf.math.add(precision, recall)\n",
    "    f1s = tf.math.divide(numerators, denominators)\n",
    "    weighted_f1 = tf.reduce_sum(f.math.multiply(f1s, weights))\n",
    "    return weighted_f1\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    this calculates precision & recall \n",
    "    \"\"\"\n",
    "\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))  # mistake: y_pred of 0.3 is also considered 1\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    # y_true_class = tf.math.argmax(y_true, axis=1, output_type=tf.dtypes.int32)\n",
    "    # y_pred_class = tf.math.argmax(y_pred, axis=1, output_type=tf.dtypes.int32)\n",
    "    # conf_mat = tf.math.confusion_matrix(y_true_class, y_pred_class)\n",
    "    # tf.Print(conf_mat, [conf_mat], \"confusion_matrix\")\n",
    "\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "\n",
    "get_custom_objects().update({\"f1_metric\": f1_metric, \"f1_weighted\": f1_weighted})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real class weights are [5.23782772 5.42054264 0.38101076] [0 1 2]\n",
      "value_counts (array([0, 1, 2]), array([ 178,  172, 2447]))\n",
      "Test sample_weights\n",
      "[2 0 2 0 2 2 2 2 2 2 1 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "[0.38101076 5.23782772 0.38101076 5.23782772 0.38101076 0.38101076\n",
      " 0.38101076 0.38101076 0.38101076 0.38101076 5.42054264 0.38101076\n",
      " 0.38101076 5.42054264 0.38101076 0.38101076 0.38101076 0.38101076\n",
      " 0.38101076 0.38101076 0.38101076 0.38101076 0.38101076 0.38101076\n",
      " 0.38101076 0.38101076 0.38101076 0.38101076 0.38101076 0.38101076]\n"
     ]
    }
   ],
   "source": [
    "sample_weights = get_sample_weights(y_train)\n",
    "print(\"Test sample_weights\")\n",
    "rand_idx = np.random.randint(0, 1000, 30)\n",
    "print(y_train[rand_idx])\n",
    "print(sample_weights[rand_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train (2797, 3)\n"
     ]
    }
   ],
   "source": [
    "one_hot_enc = OneHotEncoder(sparse=False, categories='auto')  # , categories='auto'\n",
    "y_train = one_hot_enc.fit_transform(y_train.reshape(-1, 1))\n",
    "print(\"y_train\",y_train.shape)\n",
    "y_cv = one_hot_enc.transform(y_cv.reshape(-1, 1))\n",
    "y_test = one_hot_enc.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final shape of x, y train/test (2797, 15, 15, 3) (2797, 3) (1000, 15, 15, 3) (1000, 3)\n"
     ]
    }
   ],
   "source": [
    "dim = int(np.sqrt(num_features))\n",
    "x_train = reshape_as_image(x_train, dim, dim)\n",
    "x_cv = reshape_as_image(x_cv, dim, dim)\n",
    "x_test = reshape_as_image(x_test, dim, dim)\n",
    "# adding a 1-dim for channels (3)\n",
    "x_train = np.stack((x_train,) * 3, axis=-1)\n",
    "x_test = np.stack((x_test,) * 3, axis=-1)\n",
    "x_cv = np.stack((x_cv,) * 3, axis=-1)\n",
    "print(\"final shape of x, y train/test {} {} {} {}\".format(x_train.shape, y_train.shape, x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "columns = rows = 3\n",
    "for i in range(1, columns*rows +1):\n",
    "    index = np.random.randint(len(x_train))\n",
    "    img = x_train[index]\n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title('image_'+str(index)+'_class_'+str(np.argmax(y_train[index])), fontsize=10)\n",
    "    plt.subplots_adjust(wspace=0.2, hspace=0.2)\n",
    "    plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, LeakyReLU\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger, Callback\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.regularizers import l2, l1, l1_l2\n",
    "from tensorflow.keras.initializers import RandomUniform, RandomNormal\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "params = {'batch_size': 80, 'conv2d_layers': {'conv2d_do_1': 0.22, 'conv2d_filters_1': 35, 'conv2d_kernel_size_1': 2, 'conv2d_mp_1': 2, \n",
    "                                              'conv2d_strides_1': 1, 'kernel_regularizer_1': 0.0, 'conv2d_do_2': 0.05, \n",
    "                                              'conv2d_filters_2': 20, 'conv2d_kernel_size_2': 2, 'conv2d_mp_2': 2, 'conv2d_strides_2': 2, \n",
    "                                              'kernel_regularizer_2': 0.0, 'layers': 'two'}, \n",
    "          'dense_layers': {'dense_do_1': 0.22, 'dense_nodes_1': 100, 'kernel_regularizer_1': 0.0, 'layers': 'one'},\n",
    "          'epochs': 3000, 'lr': 0.001, 'optimizer': 'adam'}\n",
    "\n",
    "# Original paper CNN params: input layer (15x15), two convolutional layers (15x15x32, 15x15x64),\n",
    "# a max pooling (7x7x64), two dropout (0.25, 0.50), fully connected layers (128), and an out-\n",
    "# put layer (3). stride?\n",
    "# params = {'batch_size': 80, 'conv2d_layers': {'conv2d_do_1': 0.25, 'conv2d_filters_1': 32, 'conv2d_kernel_size_1': 3, 'conv2d_mp_1': 0, \n",
    "#                                                'conv2d_strides_1': 1, 'kernel_regularizer_1': 0.0, 'conv2d_do_2': 0.5, \n",
    "#                                                'conv2d_filters_2': 64, 'conv2d_kernel_size_2': 3, 'conv2d_mp_2': 7, 'conv2d_strides_2': 1, \n",
    "#                                                'kernel_regularizer_2': 0.0, 'layers': 'two'}, \n",
    "#            'dense_layers': {'dense_do_1': 0.0, 'dense_nodes_1': 128, 'kernel_regularizer_1': 0.0, 'layers': 'one'},\n",
    "#            'epochs': 3000, 'lr': 0.001, 'optimizer': 'adam'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import *\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.metrics import AUC\n",
    "\n",
    "def f1_custom(y_true, y_pred):\n",
    "    y_t = np.argmax(y_true, axis=1)\n",
    "    y_p = np.argmax(y_pred, axis=1)\n",
    "    f1_score(y_t, y_p, labels=None, average='weighted', sample_weight=None, zero_division='warn')\n",
    "\n",
    "def create_model_cnn(params):\n",
    "    model = Sequential()\n",
    "\n",
    "    print(\"Training with params {}\".format(params))\n",
    "    # (batch_size, timesteps, data_dim)\n",
    "    # x_train, y_train = get_data_cnn(df, df.head(1).iloc[0][\"timestamp\"])[0:2]\n",
    "    conv2d_layer1 = Conv2D(params[\"conv2d_layers\"][\"conv2d_filters_1\"],\n",
    "                           params[\"conv2d_layers\"][\"conv2d_kernel_size_1\"],\n",
    "                           strides=params[\"conv2d_layers\"][\"conv2d_strides_1\"],\n",
    "                           kernel_regularizer=regularizers.l2(params[\"conv2d_layers\"][\"kernel_regularizer_1\"]), \n",
    "                           padding='valid',activation=\"relu\", use_bias=True,\n",
    "                           kernel_initializer='glorot_uniform',\n",
    "                           input_shape=(x_train[0].shape[0],\n",
    "                                        x_train[0].shape[1], x_train[0].shape[2]))\n",
    "    model.add(conv2d_layer1)\n",
    "    if params[\"conv2d_layers\"]['conv2d_mp_1'] == 1:\n",
    "        model.add(MaxPool2D(pool_size=2))\n",
    "    model.add(Dropout(params['conv2d_layers']['conv2d_do_1']))\n",
    "    if params[\"conv2d_layers\"]['layers'] == 'two':\n",
    "        conv2d_layer2 = Conv2D(params[\"conv2d_layers\"][\"conv2d_filters_2\"],\n",
    "                               params[\"conv2d_layers\"][\"conv2d_kernel_size_2\"],\n",
    "                               strides=params[\"conv2d_layers\"][\"conv2d_strides_2\"],\n",
    "                               kernel_regularizer=regularizers.l2(params[\"conv2d_layers\"][\"kernel_regularizer_2\"]),\n",
    "                               padding='valid',activation=\"relu\", use_bias=True,\n",
    "                               kernel_initializer='glorot_uniform')\n",
    "        model.add(conv2d_layer2)\n",
    "        if params[\"conv2d_layers\"]['conv2d_mp_2'] == 1:\n",
    "            model.add(MaxPool2D(pool_size=2))\n",
    "        model.add(Dropout(params['conv2d_layers']['conv2d_do_2']))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(params['dense_layers'][\"dense_nodes_1\"], activation='relu'))\n",
    "    model.add(Dropout(params['dense_layers']['dense_do_1']))\n",
    "\n",
    "    if params['dense_layers'][\"layers\"] == 'two':\n",
    "        model.add(Dense(params['dense_layers'][\"dense_nodes_2\"], activation='relu', \n",
    "                        kernel_regularizer=params['dense_layers'][\"kernel_regularizer_1\"]))\n",
    "        model.add(Dropout(params['dense_layers']['dense_do_2']))\n",
    "\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    if params[\"optimizer\"] == 'rmsprop':\n",
    "        optimizer = optimizers.RMSprop(lr=params[\"lr\"])\n",
    "    elif params[\"optimizer\"] == 'sgd':\n",
    "        optimizer = optimizers.SGD(lr=params[\"lr\"], decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    elif params[\"optimizer\"] == 'adam':\n",
    "        optimizer = optimizers.Adam(learning_rate=params[\"lr\"], beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy', f1_metric])\n",
    "    # from keras.utils.vis_utils import plot_model use this too for diagram with plot\n",
    "    # model.summary(print_fn=lambda x: print(x + '\\n'))\n",
    "    return model\n",
    "\n",
    "def check_baseline(pred, y_test):\n",
    "    print(\"size of test set\", len(y_test))\n",
    "    e = np.equal(pred, y_test)\n",
    "    print(\"TP class counts\", np.unique(y_test[e], return_counts=True))\n",
    "    print(\"True class counts\", np.unique(y_test, return_counts=True))\n",
    "    print(\"Pred class counts\", np.unique(pred, return_counts=True))\n",
    "    holds = np.unique(y_test, return_counts=True)[1][2]  # number 'hold' predictions\n",
    "    print(\"baseline acc:\", (holds/len(y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with params {'batch_size': 80, 'conv2d_layers': {'conv2d_do_1': 0.22, 'conv2d_filters_1': 35, 'conv2d_kernel_size_1': 2, 'conv2d_mp_1': 2, 'conv2d_strides_1': 1, 'kernel_regularizer_1': 0.0, 'conv2d_do_2': 0.05, 'conv2d_filters_2': 20, 'conv2d_kernel_size_2': 2, 'conv2d_mp_2': 2, 'conv2d_strides_2': 2, 'kernel_regularizer_2': 0.0, 'layers': 'two'}, 'dense_layers': {'dense_do_1': 0.22, 'dense_nodes_1': 100, 'kernel_regularizer_1': 0.0, 'layers': 'one'}, 'epochs': 3000, 'lr': 0.001, 'optimizer': 'adam'}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from tensorflow.keras.utils import model_to_dot, plot_model\n",
    "\n",
    "model = create_model_cnn(params)\n",
    "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=False)\n",
    "\n",
    "# SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "best_model_path = os.path.join('.', 'best_model_keras')\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "                   patience=100, min_delta=0.0001)\n",
    "# csv_logger = CSVLogger(os.path.join(OUTPUT_PATH, 'log_training_batch.log'), append=True)\n",
    "rlp = ReduceLROnPlateau(monitor='val_loss', factor=0.02, patience=20, verbose=1, mode='min',\n",
    "                        min_delta=0.001, cooldown=1, min_lr=0.0001)\n",
    "mcp = ModelCheckpoint(best_model_path, monitor='val_f1_metric', verbose=1,\n",
    "                      save_best_only=True, save_weights_only=False, mode='max', period=1)  # val_f1_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2797 samples, validate on 1199 samples\n",
      "Epoch 1/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 1.0426 - accuracy: 0.4595 - f1_metric: 0.0116 \n",
      "Epoch 00001: val_f1_metric improved from -inf to 0.07458, saving model to ./best_model_keras\n",
      "2797/2797 [==============================] - 2s 705us/sample - loss: 1.0483 - accuracy: 0.4623 - f1_metric: 0.0316 - val_loss: 1.1090 - val_accuracy: 0.1726 - val_f1_metric: 0.0746\n",
      "Epoch 2/3000\n",
      "2624/2797 [===========================>..] - ETA: 0s - loss: 0.9162 - accuracy: 0.2915 - f1_metric: 0.1358\n",
      "Epoch 00002: val_f1_metric improved from 0.07458 to 0.15685, saving model to ./best_model_keras\n",
      "2797/2797 [==============================] - 1s 417us/sample - loss: 0.9024 - accuracy: 0.2935 - f1_metric: 0.1378 - val_loss: 1.0621 - val_accuracy: 0.3061 - val_f1_metric: 0.1568\n",
      "Epoch 3/3000\n",
      "2496/2797 [=========================>....] - ETA: 0s - loss: 0.7724 - accuracy: 0.4275 - f1_metric: 0.2841\n",
      "Epoch 00003: val_f1_metric did not improve from 0.15685\n",
      "2797/2797 [==============================] - 0s 114us/sample - loss: 0.7869 - accuracy: 0.4297 - f1_metric: 0.2876 - val_loss: 1.2021 - val_accuracy: 0.2310 - val_f1_metric: 0.1416\n",
      "Epoch 4/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.7327 - accuracy: 0.3889 - f1_metric: 0.3067\n",
      "Epoch 00004: val_f1_metric improved from 0.15685 to 0.24698, saving model to ./best_model_keras\n",
      "2797/2797 [==============================] - 1s 419us/sample - loss: 0.7397 - accuracy: 0.4094 - f1_metric: 0.3257 - val_loss: 1.0873 - val_accuracy: 0.3403 - val_f1_metric: 0.2470\n",
      "Epoch 5/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.6918 - accuracy: 0.4181 - f1_metric: 0.3527\n",
      "Epoch 00005: val_f1_metric improved from 0.24698 to 0.42736, saving model to ./best_model_keras\n",
      "2797/2797 [==============================] - 1s 466us/sample - loss: 0.6771 - accuracy: 0.4222 - f1_metric: 0.3556 - val_loss: 0.9062 - val_accuracy: 0.4971 - val_f1_metric: 0.4274\n",
      "Epoch 6/3000\n",
      "2112/2797 [=====================>........] - ETA: 0s - loss: 0.6466 - accuracy: 0.5241 - f1_metric: 0.4749\n",
      "Epoch 00006: val_f1_metric did not improve from 0.42736\n",
      "2797/2797 [==============================] - 0s 101us/sample - loss: 0.6483 - accuracy: 0.5055 - f1_metric: 0.4514 - val_loss: 0.9049 - val_accuracy: 0.4704 - val_f1_metric: 0.4099\n",
      "Epoch 7/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.5861 - accuracy: 0.5654 - f1_metric: 0.5228\n",
      "Epoch 00007: val_f1_metric improved from 0.42736 to 0.49829, saving model to ./best_model_keras\n",
      "2797/2797 [==============================] - 1s 395us/sample - loss: 0.5945 - accuracy: 0.5449 - f1_metric: 0.5021 - val_loss: 0.8746 - val_accuracy: 0.5338 - val_f1_metric: 0.4983\n",
      "Epoch 8/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.5585 - accuracy: 0.5807 - f1_metric: 0.5542\n",
      "Epoch 00008: val_f1_metric improved from 0.49829 to 0.50524, saving model to ./best_model_keras\n",
      "2797/2797 [==============================] - 1s 395us/sample - loss: 0.5556 - accuracy: 0.5792 - f1_metric: 0.5519 - val_loss: 0.8786 - val_accuracy: 0.5329 - val_f1_metric: 0.5052\n",
      "Epoch 9/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.5222 - accuracy: 0.6018 - f1_metric: 0.5798\n",
      "Epoch 00009: val_f1_metric did not improve from 0.50524\n",
      "2797/2797 [==============================] - 0s 96us/sample - loss: 0.5321 - accuracy: 0.5931 - f1_metric: 0.5706 - val_loss: 1.0946 - val_accuracy: 0.4087 - val_f1_metric: 0.3586\n",
      "Epoch 10/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.4986 - accuracy: 0.6115 - f1_metric: 0.5948\n",
      "Epoch 00010: val_f1_metric improved from 0.50524 to 0.71803, saving model to ./best_model_keras\n",
      "2797/2797 [==============================] - 1s 392us/sample - loss: 0.4860 - accuracy: 0.6203 - f1_metric: 0.6057 - val_loss: 0.5818 - val_accuracy: 0.7181 - val_f1_metric: 0.7180\n",
      "Epoch 11/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.4936 - accuracy: 0.6188 - f1_metric: 0.6028\n",
      "Epoch 00011: val_f1_metric did not improve from 0.71803\n",
      "2797/2797 [==============================] - 0s 93us/sample - loss: 0.4864 - accuracy: 0.6371 - f1_metric: 0.6237 - val_loss: 0.6483 - val_accuracy: 0.6656 - val_f1_metric: 0.6628\n",
      "Epoch 12/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.4630 - accuracy: 0.6508 - f1_metric: 0.6461\n",
      "Epoch 00012: val_f1_metric did not improve from 0.71803\n",
      "2797/2797 [==============================] - 0s 91us/sample - loss: 0.4589 - accuracy: 0.6371 - f1_metric: 0.6298 - val_loss: 0.7384 - val_accuracy: 0.6030 - val_f1_metric: 0.5998\n",
      "Epoch 13/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.4161 - accuracy: 0.6936 - f1_metric: 0.6916\n",
      "Epoch 00013: val_f1_metric did not improve from 0.71803\n",
      "2797/2797 [==============================] - 0s 95us/sample - loss: 0.4264 - accuracy: 0.7004 - f1_metric: 0.6986 - val_loss: 0.7004 - val_accuracy: 0.6414 - val_f1_metric: 0.6381\n",
      "Epoch 14/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.4285 - accuracy: 0.6482 - f1_metric: 0.6440\n",
      "Epoch 00014: val_f1_metric did not improve from 0.71803\n",
      "2797/2797 [==============================] - 0s 93us/sample - loss: 0.4280 - accuracy: 0.6600 - f1_metric: 0.6564 - val_loss: 0.5935 - val_accuracy: 0.7006 - val_f1_metric: 0.6994\n",
      "Epoch 15/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.4243 - accuracy: 0.6850 - f1_metric: 0.6837\n",
      "Epoch 00015: val_f1_metric did not improve from 0.71803\n",
      "2797/2797 [==============================] - 0s 92us/sample - loss: 0.4222 - accuracy: 0.6707 - f1_metric: 0.6692 - val_loss: 0.6682 - val_accuracy: 0.6539 - val_f1_metric: 0.6514\n",
      "Epoch 16/3000\n",
      "2112/2797 [=====================>........] - ETA: 0s - loss: 0.4015 - accuracy: 0.7206 - f1_metric: 0.7192\n",
      "Epoch 00016: val_f1_metric did not improve from 0.71803\n",
      "2797/2797 [==============================] - 0s 101us/sample - loss: 0.3983 - accuracy: 0.7172 - f1_metric: 0.7157 - val_loss: 0.6390 - val_accuracy: 0.7031 - val_f1_metric: 0.7042\n",
      "Epoch 17/3000\n",
      "2112/2797 [=====================>........] - ETA: 0s - loss: 0.3898 - accuracy: 0.7202 - f1_metric: 0.7191\n",
      "Epoch 00017: val_f1_metric did not improve from 0.71803\n",
      "2797/2797 [==============================] - 0s 99us/sample - loss: 0.4068 - accuracy: 0.7111 - f1_metric: 0.7091 - val_loss: 0.6162 - val_accuracy: 0.6981 - val_f1_metric: 0.6983\n",
      "Epoch 18/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.3903 - accuracy: 0.7233 - f1_metric: 0.7235\n",
      "Epoch 00018: val_f1_metric did not improve from 0.71803\n",
      "2797/2797 [==============================] - 0s 93us/sample - loss: 0.3919 - accuracy: 0.7111 - f1_metric: 0.7108 - val_loss: 0.7860 - val_accuracy: 0.6055 - val_f1_metric: 0.6049\n",
      "Epoch 19/3000\n",
      "2688/2797 [===========================>..] - ETA: 0s - loss: 0.3893 - accuracy: 0.7154 - f1_metric: 0.7125\n",
      "Epoch 00019: val_f1_metric did not improve from 0.71803\n",
      "2797/2797 [==============================] - 0s 107us/sample - loss: 0.3887 - accuracy: 0.7140 - f1_metric: 0.7100 - val_loss: 0.6830 - val_accuracy: 0.6530 - val_f1_metric: 0.6503\n",
      "Epoch 20/3000\n",
      "2112/2797 [=====================>........] - ETA: 0s - loss: 0.3521 - accuracy: 0.7287 - f1_metric: 0.7277\n",
      "Epoch 00020: val_f1_metric did not improve from 0.71803\n",
      "2797/2797 [==============================] - 0s 101us/sample - loss: 0.3654 - accuracy: 0.7347 - f1_metric: 0.7329 - val_loss: 0.7596 - val_accuracy: 0.6239 - val_f1_metric: 0.6236\n",
      "Epoch 21/3000\n",
      "2560/2797 [==========================>...] - ETA: 0s - loss: 0.3531 - accuracy: 0.7336 - f1_metric: 0.7327\n",
      "Epoch 00021: val_f1_metric improved from 0.71803 to 0.80053, saving model to ./best_model_keras\n",
      "2797/2797 [==============================] - 1s 407us/sample - loss: 0.3468 - accuracy: 0.7401 - f1_metric: 0.7395 - val_loss: 0.4298 - val_accuracy: 0.8007 - val_f1_metric: 0.8005\n",
      "Epoch 22/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.3870 - accuracy: 0.7122 - f1_metric: 0.7121\n",
      "Epoch 00022: val_f1_metric did not improve from 0.80053\n",
      "2797/2797 [==============================] - 0s 92us/sample - loss: 0.3770 - accuracy: 0.7193 - f1_metric: 0.7196 - val_loss: 0.4882 - val_accuracy: 0.7748 - val_f1_metric: 0.7739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.3248 - accuracy: 0.7788 - f1_metric: 0.7783\n",
      "Epoch 00023: val_f1_metric did not improve from 0.80053\n",
      "2797/2797 [==============================] - 0s 132us/sample - loss: 0.3356 - accuracy: 0.7651 - f1_metric: 0.7631 - val_loss: 0.6704 - val_accuracy: 0.6722 - val_f1_metric: 0.6726\n",
      "Epoch 24/3000\n",
      "2048/2797 [====================>.........] - ETA: 0s - loss: 0.3331 - accuracy: 0.7515 - f1_metric: 0.7502\n",
      "Epoch 00024: val_f1_metric did not improve from 0.80053\n",
      "2797/2797 [==============================] - 0s 101us/sample - loss: 0.3260 - accuracy: 0.7608 - f1_metric: 0.7596 - val_loss: 0.4648 - val_accuracy: 0.7857 - val_f1_metric: 0.7854\n",
      "Epoch 25/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.3396 - accuracy: 0.7589 - f1_metric: 0.7589\n",
      "Epoch 00025: val_f1_metric did not improve from 0.80053\n",
      "2797/2797 [==============================] - 0s 97us/sample - loss: 0.3416 - accuracy: 0.7562 - f1_metric: 0.7550 - val_loss: 0.5554 - val_accuracy: 0.7389 - val_f1_metric: 0.7393\n",
      "Epoch 26/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.3235 - accuracy: 0.7429 - f1_metric: 0.7429\n",
      "Epoch 00026: val_f1_metric did not improve from 0.80053\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.3337 - accuracy: 0.7547 - f1_metric: 0.7551 - val_loss: 0.5396 - val_accuracy: 0.7264 - val_f1_metric: 0.7264\n",
      "Epoch 27/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.3242 - accuracy: 0.7552 - f1_metric: 0.7549\n",
      "Epoch 00027: val_f1_metric did not improve from 0.80053\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.3249 - accuracy: 0.7487 - f1_metric: 0.7485 - val_loss: 0.5690 - val_accuracy: 0.7264 - val_f1_metric: 0.7258\n",
      "Epoch 28/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.3212 - accuracy: 0.7504 - f1_metric: 0.7487\n",
      "Epoch 00028: val_f1_metric did not improve from 0.80053\n",
      "2797/2797 [==============================] - 0s 95us/sample - loss: 0.3187 - accuracy: 0.7612 - f1_metric: 0.7602 - val_loss: 0.4660 - val_accuracy: 0.7781 - val_f1_metric: 0.7771\n",
      "Epoch 29/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.3235 - accuracy: 0.7661 - f1_metric: 0.7655\n",
      "Epoch 00029: val_f1_metric did not improve from 0.80053\n",
      "2797/2797 [==============================] - 0s 95us/sample - loss: 0.3292 - accuracy: 0.7601 - f1_metric: 0.7600 - val_loss: 0.6284 - val_accuracy: 0.6997 - val_f1_metric: 0.6998\n",
      "Epoch 30/3000\n",
      "2048/2797 [====================>.........] - ETA: 0s - loss: 0.3181 - accuracy: 0.7666 - f1_metric: 0.7661\n",
      "Epoch 00030: val_f1_metric improved from 0.80053 to 0.80130, saving model to ./best_model_keras\n",
      "2797/2797 [==============================] - 1s 442us/sample - loss: 0.3137 - accuracy: 0.7665 - f1_metric: 0.7670 - val_loss: 0.4422 - val_accuracy: 0.8015 - val_f1_metric: 0.8013\n",
      "Epoch 31/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.2945 - accuracy: 0.7776 - f1_metric: 0.7777\n",
      "Epoch 00031: val_f1_metric did not improve from 0.80130\n",
      "2797/2797 [==============================] - 0s 101us/sample - loss: 0.3063 - accuracy: 0.7794 - f1_metric: 0.7802 - val_loss: 0.4907 - val_accuracy: 0.7640 - val_f1_metric: 0.7649\n",
      "Epoch 32/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.3256 - accuracy: 0.7630 - f1_metric: 0.7622\n",
      "Epoch 00032: val_f1_metric did not improve from 0.80130\n",
      "2797/2797 [==============================] - 0s 92us/sample - loss: 0.3224 - accuracy: 0.7590 - f1_metric: 0.7580 - val_loss: 0.5271 - val_accuracy: 0.7515 - val_f1_metric: 0.7514\n",
      "Epoch 33/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.2911 - accuracy: 0.7905 - f1_metric: 0.7905\n",
      "Epoch 00033: val_f1_metric did not improve from 0.80130\n",
      "2797/2797 [==============================] - 0s 92us/sample - loss: 0.2953 - accuracy: 0.7991 - f1_metric: 0.7994 - val_loss: 0.6171 - val_accuracy: 0.7189 - val_f1_metric: 0.7190\n",
      "Epoch 34/3000\n",
      "2560/2797 [==========================>...] - ETA: 0s - loss: 0.3074 - accuracy: 0.7570 - f1_metric: 0.7574\n",
      "Epoch 00034: val_f1_metric did not improve from 0.80130\n",
      "2797/2797 [==============================] - 0s 88us/sample - loss: 0.3168 - accuracy: 0.7615 - f1_metric: 0.7619 - val_loss: 0.4327 - val_accuracy: 0.7907 - val_f1_metric: 0.7906\n",
      "Epoch 35/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.2867 - accuracy: 0.7878 - f1_metric: 0.7878\n",
      "Epoch 00035: val_f1_metric did not improve from 0.80130\n",
      "2797/2797 [==============================] - 0s 101us/sample - loss: 0.2902 - accuracy: 0.7858 - f1_metric: 0.7861 - val_loss: 0.4469 - val_accuracy: 0.7940 - val_f1_metric: 0.7942\n",
      "Epoch 36/3000\n",
      "2624/2797 [===========================>..] - ETA: 0s - loss: 0.3260 - accuracy: 0.7599 - f1_metric: 0.7600\n",
      "Epoch 00036: val_f1_metric did not improve from 0.80130\n",
      "2797/2797 [==============================] - 0s 106us/sample - loss: 0.3233 - accuracy: 0.7601 - f1_metric: 0.7609 - val_loss: 0.4830 - val_accuracy: 0.7540 - val_f1_metric: 0.7535\n",
      "Epoch 37/3000\n",
      "2752/2797 [============================>.] - ETA: 0s - loss: 0.3057 - accuracy: 0.7805 - f1_metric: 0.7805\n",
      "Epoch 00037: val_f1_metric did not improve from 0.80130\n",
      "2797/2797 [==============================] - 0s 102us/sample - loss: 0.3047 - accuracy: 0.7801 - f1_metric: 0.7800 - val_loss: 0.5428 - val_accuracy: 0.7364 - val_f1_metric: 0.7363\n",
      "Epoch 38/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.2998 - accuracy: 0.7825 - f1_metric: 0.7827\n",
      "Epoch 00038: val_f1_metric did not improve from 0.80130\n",
      "2797/2797 [==============================] - 0s 90us/sample - loss: 0.3013 - accuracy: 0.7912 - f1_metric: 0.7914 - val_loss: 0.4588 - val_accuracy: 0.7890 - val_f1_metric: 0.7890\n",
      "Epoch 39/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.2830 - accuracy: 0.7899 - f1_metric: 0.7898\n",
      "Epoch 00039: val_f1_metric did not improve from 0.80130\n",
      "2797/2797 [==============================] - 0s 91us/sample - loss: 0.2930 - accuracy: 0.7848 - f1_metric: 0.7845 - val_loss: 0.5623 - val_accuracy: 0.7256 - val_f1_metric: 0.7253\n",
      "Epoch 40/3000\n",
      "2496/2797 [=========================>....] - ETA: 0s - loss: 0.2949 - accuracy: 0.7784 - f1_metric: 0.7785\n",
      "Epoch 00040: val_f1_metric did not improve from 0.80130\n",
      "2797/2797 [==============================] - 0s 90us/sample - loss: 0.2880 - accuracy: 0.7851 - f1_metric: 0.7860 - val_loss: 0.4598 - val_accuracy: 0.7915 - val_f1_metric: 0.7917\n",
      "Epoch 41/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.2632 - accuracy: 0.8062 - f1_metric: 0.8062\n",
      "Epoch 00041: val_f1_metric did not improve from 0.80130\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "2797/2797 [==============================] - 0s 93us/sample - loss: 0.2711 - accuracy: 0.8037 - f1_metric: 0.8026 - val_loss: 0.6614 - val_accuracy: 0.6964 - val_f1_metric: 0.6962\n",
      "Epoch 42/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.2585 - accuracy: 0.7736 - f1_metric: 0.7738\n",
      "Epoch 00042: val_f1_metric improved from 0.80130 to 0.80982, saving model to ./best_model_keras\n",
      "2797/2797 [==============================] - 1s 387us/sample - loss: 0.2501 - accuracy: 0.7873 - f1_metric: 0.7877 - val_loss: 0.3991 - val_accuracy: 0.8098 - val_f1_metric: 0.8098\n",
      "Epoch 43/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.2487 - accuracy: 0.8303 - f1_metric: 0.8303\n",
      "Epoch 00043: val_f1_metric did not improve from 0.80982\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.2584 - accuracy: 0.8155 - f1_metric: 0.8156 - val_loss: 0.4661 - val_accuracy: 0.7898 - val_f1_metric: 0.7901\n",
      "Epoch 44/3000\n",
      "2496/2797 [=========================>....] - ETA: 0s - loss: 0.2640 - accuracy: 0.8193 - f1_metric: 0.8195\n",
      "Epoch 00044: val_f1_metric did not improve from 0.80982\n",
      "2797/2797 [==============================] - 0s 90us/sample - loss: 0.2648 - accuracy: 0.8180 - f1_metric: 0.8174 - val_loss: 0.4562 - val_accuracy: 0.7882 - val_f1_metric: 0.7884\n",
      "Epoch 45/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.2275 - accuracy: 0.8326 - f1_metric: 0.8326\n",
      "Epoch 00045: val_f1_metric did not improve from 0.80982\n",
      "2797/2797 [==============================] - 0s 93us/sample - loss: 0.2404 - accuracy: 0.8280 - f1_metric: 0.8280 - val_loss: 0.4289 - val_accuracy: 0.8048 - val_f1_metric: 0.8046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.2367 - accuracy: 0.8228 - f1_metric: 0.8228\n",
      "Epoch 00046: val_f1_metric improved from 0.80982 to 0.81393, saving model to ./best_model_keras\n",
      "2797/2797 [==============================] - 1s 390us/sample - loss: 0.2351 - accuracy: 0.8270 - f1_metric: 0.8272 - val_loss: 0.4096 - val_accuracy: 0.8140 - val_f1_metric: 0.8139\n",
      "Epoch 47/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.2428 - accuracy: 0.8290 - f1_metric: 0.8290\n",
      "Epoch 00047: val_f1_metric did not improve from 0.81393\n",
      "2797/2797 [==============================] - 0s 103us/sample - loss: 0.2415 - accuracy: 0.8309 - f1_metric: 0.8316 - val_loss: 0.4263 - val_accuracy: 0.8040 - val_f1_metric: 0.8041\n",
      "Epoch 48/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.2362 - accuracy: 0.8375 - f1_metric: 0.8375\n",
      "Epoch 00048: val_f1_metric did not improve from 0.81393\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.2379 - accuracy: 0.8298 - f1_metric: 0.8290 - val_loss: 0.4407 - val_accuracy: 0.8007 - val_f1_metric: 0.8008\n",
      "Epoch 49/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.2403 - accuracy: 0.8264 - f1_metric: 0.8264\n",
      "Epoch 00049: val_f1_metric did not improve from 0.81393\n",
      "2797/2797 [==============================] - 0s 104us/sample - loss: 0.2367 - accuracy: 0.8295 - f1_metric: 0.8296 - val_loss: 0.4165 - val_accuracy: 0.8098 - val_f1_metric: 0.8098\n",
      "Epoch 50/3000\n",
      "2688/2797 [===========================>..] - ETA: 0s - loss: 0.2402 - accuracy: 0.8270 - f1_metric: 0.8270\n",
      "Epoch 00050: val_f1_metric did not improve from 0.81393\n",
      "2797/2797 [==============================] - 0s 137us/sample - loss: 0.2405 - accuracy: 0.8266 - f1_metric: 0.8266 - val_loss: 0.4199 - val_accuracy: 0.8065 - val_f1_metric: 0.8065\n",
      "Epoch 51/3000\n",
      "2560/2797 [==========================>...] - ETA: 0s - loss: 0.2387 - accuracy: 0.8359 - f1_metric: 0.8359\n",
      "Epoch 00051: val_f1_metric did not improve from 0.81393\n",
      "2797/2797 [==============================] - 0s 108us/sample - loss: 0.2350 - accuracy: 0.8352 - f1_metric: 0.8349 - val_loss: 0.4311 - val_accuracy: 0.7982 - val_f1_metric: 0.7983\n",
      "Epoch 52/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.2297 - accuracy: 0.8350 - f1_metric: 0.8350\n",
      "Epoch 00052: val_f1_metric did not improve from 0.81393\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.2359 - accuracy: 0.8291 - f1_metric: 0.8289 - val_loss: 0.4099 - val_accuracy: 0.8132 - val_f1_metric: 0.8131\n",
      "Epoch 53/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.2470 - accuracy: 0.8220 - f1_metric: 0.8220\n",
      "Epoch 00053: val_f1_metric did not improve from 0.81393\n",
      "2797/2797 [==============================] - 0s 94us/sample - loss: 0.2419 - accuracy: 0.8169 - f1_metric: 0.8165 - val_loss: 0.4260 - val_accuracy: 0.8015 - val_f1_metric: 0.8016\n",
      "Epoch 54/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.2277 - accuracy: 0.8383 - f1_metric: 0.8383\n",
      "Epoch 00054: val_f1_metric did not improve from 0.81393\n",
      "2797/2797 [==============================] - 0s 97us/sample - loss: 0.2339 - accuracy: 0.8348 - f1_metric: 0.8347 - val_loss: 0.4177 - val_accuracy: 0.8082 - val_f1_metric: 0.8082\n",
      "Epoch 55/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.2522 - accuracy: 0.8062 - f1_metric: 0.8062\n",
      "Epoch 00055: val_f1_metric did not improve from 0.81393\n",
      "2797/2797 [==============================] - 0s 96us/sample - loss: 0.2473 - accuracy: 0.8144 - f1_metric: 0.8148 - val_loss: 0.4271 - val_accuracy: 0.8082 - val_f1_metric: 0.8082\n",
      "Epoch 56/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.2469 - accuracy: 0.8290 - f1_metric: 0.8290\n",
      "Epoch 00056: val_f1_metric did not improve from 0.81393\n",
      "2797/2797 [==============================] - 0s 95us/sample - loss: 0.2488 - accuracy: 0.8252 - f1_metric: 0.8251 - val_loss: 0.4213 - val_accuracy: 0.8082 - val_f1_metric: 0.8082\n",
      "Epoch 57/3000\n",
      "2112/2797 [=====================>........] - ETA: 0s - loss: 0.2407 - accuracy: 0.8291 - f1_metric: 0.8291\n",
      "Epoch 00057: val_f1_metric did not improve from 0.81393\n",
      "2797/2797 [==============================] - 0s 102us/sample - loss: 0.2327 - accuracy: 0.8298 - f1_metric: 0.8290 - val_loss: 0.4167 - val_accuracy: 0.8123 - val_f1_metric: 0.8123\n",
      "Epoch 58/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.2540 - accuracy: 0.8322 - f1_metric: 0.8322\n",
      "Epoch 00058: val_f1_metric did not improve from 0.81393\n",
      "2797/2797 [==============================] - 0s 93us/sample - loss: 0.2608 - accuracy: 0.8298 - f1_metric: 0.8292 - val_loss: 0.4350 - val_accuracy: 0.8007 - val_f1_metric: 0.8008\n",
      "Epoch 59/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.2350 - accuracy: 0.8222 - f1_metric: 0.8222\n",
      "Epoch 00059: val_f1_metric did not improve from 0.81393\n",
      "2797/2797 [==============================] - 0s 95us/sample - loss: 0.2297 - accuracy: 0.8284 - f1_metric: 0.8274 - val_loss: 0.4135 - val_accuracy: 0.8098 - val_f1_metric: 0.8098\n",
      "Epoch 60/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.2234 - accuracy: 0.8328 - f1_metric: 0.8328\n",
      "Epoch 00060: val_f1_metric improved from 0.81393 to 0.81476, saving model to ./best_model_keras\n",
      "2797/2797 [==============================] - 1s 395us/sample - loss: 0.2330 - accuracy: 0.8345 - f1_metric: 0.8347 - val_loss: 0.3959 - val_accuracy: 0.8148 - val_f1_metric: 0.8148\n",
      "Epoch 61/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.2395 - accuracy: 0.8223 - f1_metric: 0.8225\n",
      "Epoch 00061: val_f1_metric improved from 0.81476 to 0.81805, saving model to ./best_model_keras\n",
      "2797/2797 [==============================] - 1s 401us/sample - loss: 0.2369 - accuracy: 0.8245 - f1_metric: 0.8246 - val_loss: 0.3916 - val_accuracy: 0.8182 - val_f1_metric: 0.8180\n",
      "Epoch 62/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.2507 - accuracy: 0.8176 - f1_metric: 0.8176\n",
      "Epoch 00062: val_f1_metric did not improve from 0.81805\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.2520 - accuracy: 0.8205 - f1_metric: 0.8204 - val_loss: 0.4181 - val_accuracy: 0.8032 - val_f1_metric: 0.8033\n",
      "Epoch 63/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.2425 - accuracy: 0.8254 - f1_metric: 0.8248\n",
      "Epoch 00063: val_f1_metric did not improve from 0.81805\n",
      "2797/2797 [==============================] - 0s 99us/sample - loss: 0.2384 - accuracy: 0.8280 - f1_metric: 0.8276 - val_loss: 0.4351 - val_accuracy: 0.8048 - val_f1_metric: 0.8049\n",
      "Epoch 64/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.2312 - accuracy: 0.8254 - f1_metric: 0.8254\n",
      "Epoch 00064: val_f1_metric did not improve from 0.81805\n",
      "2797/2797 [==============================] - 0s 99us/sample - loss: 0.2389 - accuracy: 0.8277 - f1_metric: 0.8276 - val_loss: 0.4242 - val_accuracy: 0.8032 - val_f1_metric: 0.8032\n",
      "Epoch 65/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.2348 - accuracy: 0.8201 - f1_metric: 0.8201\n",
      "Epoch 00065: val_f1_metric did not improve from 0.81805\n",
      "2797/2797 [==============================] - 0s 95us/sample - loss: 0.2307 - accuracy: 0.8245 - f1_metric: 0.8238 - val_loss: 0.3998 - val_accuracy: 0.8148 - val_f1_metric: 0.8148\n",
      "Epoch 66/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.2425 - accuracy: 0.8294 - f1_metric: 0.8291\n",
      "Epoch 00066: val_f1_metric did not improve from 0.81805\n",
      "2797/2797 [==============================] - 0s 95us/sample - loss: 0.2376 - accuracy: 0.8266 - f1_metric: 0.8265 - val_loss: 0.4259 - val_accuracy: 0.8032 - val_f1_metric: 0.8032\n",
      "Epoch 67/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.2253 - accuracy: 0.8357 - f1_metric: 0.8357\n",
      "Epoch 00067: val_f1_metric did not improve from 0.81805\n",
      "2797/2797 [==============================] - 0s 97us/sample - loss: 0.2250 - accuracy: 0.8395 - f1_metric: 0.8403 - val_loss: 0.4070 - val_accuracy: 0.8107 - val_f1_metric: 0.8106\n",
      "Epoch 68/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.2416 - accuracy: 0.8248 - f1_metric: 0.8248\n",
      "Epoch 00068: val_f1_metric did not improve from 0.81805\n",
      "2797/2797 [==============================] - 0s 92us/sample - loss: 0.2411 - accuracy: 0.8237 - f1_metric: 0.8239 - val_loss: 0.4334 - val_accuracy: 0.8007 - val_f1_metric: 0.8008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.2224 - accuracy: 0.8331 - f1_metric: 0.8331\n",
      "Epoch 00069: val_f1_metric did not improve from 0.81805\n",
      "2797/2797 [==============================] - 0s 92us/sample - loss: 0.2253 - accuracy: 0.8352 - f1_metric: 0.8355 - val_loss: 0.4087 - val_accuracy: 0.8107 - val_f1_metric: 0.8103\n",
      "Epoch 70/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.2331 - accuracy: 0.8290 - f1_metric: 0.8290\n",
      "Epoch 00070: val_f1_metric did not improve from 0.81805\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.2332 - accuracy: 0.8309 - f1_metric: 0.8310 - val_loss: 0.4105 - val_accuracy: 0.8098 - val_f1_metric: 0.8098\n",
      "Epoch 71/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.2496 - accuracy: 0.8251 - f1_metric: 0.8251\n",
      "Epoch 00071: val_f1_metric did not improve from 0.81805\n",
      "2797/2797 [==============================] - 0s 95us/sample - loss: 0.2470 - accuracy: 0.8223 - f1_metric: 0.8223 - val_loss: 0.4488 - val_accuracy: 0.7915 - val_f1_metric: 0.7911\n",
      "Epoch 72/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.2313 - accuracy: 0.8235 - f1_metric: 0.8236\n",
      "Epoch 00072: val_f1_metric did not improve from 0.81805\n",
      "2797/2797 [==============================] - 0s 93us/sample - loss: 0.2424 - accuracy: 0.8273 - f1_metric: 0.8282 - val_loss: 0.4163 - val_accuracy: 0.8057 - val_f1_metric: 0.8057\n",
      "Epoch 73/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.2206 - accuracy: 0.8286 - f1_metric: 0.8286\n",
      "Epoch 00073: val_f1_metric did not improve from 0.81805\n",
      "2797/2797 [==============================] - 0s 99us/sample - loss: 0.2238 - accuracy: 0.8338 - f1_metric: 0.8343 - val_loss: 0.4113 - val_accuracy: 0.8057 - val_f1_metric: 0.8057\n",
      "Epoch 74/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.2378 - accuracy: 0.8329 - f1_metric: 0.8329\n",
      "Epoch 00074: val_f1_metric did not improve from 0.81805\n",
      "2797/2797 [==============================] - 0s 101us/sample - loss: 0.2368 - accuracy: 0.8280 - f1_metric: 0.8281 - val_loss: 0.4132 - val_accuracy: 0.8115 - val_f1_metric: 0.8115\n",
      "Epoch 75/3000\n",
      "2112/2797 [=====================>........] - ETA: 0s - loss: 0.2260 - accuracy: 0.8419 - f1_metric: 0.8419\n",
      "Epoch 00075: val_f1_metric did not improve from 0.81805\n",
      "2797/2797 [==============================] - 0s 101us/sample - loss: 0.2222 - accuracy: 0.8377 - f1_metric: 0.8374 - val_loss: 0.4355 - val_accuracy: 0.7940 - val_f1_metric: 0.7942\n",
      "Epoch 76/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.2343 - accuracy: 0.8298 - f1_metric: 0.8299\n",
      "Epoch 00076: val_f1_metric did not improve from 0.81805\n",
      "2797/2797 [==============================] - 0s 92us/sample - loss: 0.2345 - accuracy: 0.8262 - f1_metric: 0.8261 - val_loss: 0.4175 - val_accuracy: 0.8090 - val_f1_metric: 0.8090\n",
      "Epoch 77/3000\n",
      "2560/2797 [==========================>...] - ETA: 0s - loss: 0.2257 - accuracy: 0.8293 - f1_metric: 0.8295\n",
      "Epoch 00077: val_f1_metric did not improve from 0.81805\n",
      "2797/2797 [==============================] - 0s 155us/sample - loss: 0.2318 - accuracy: 0.8277 - f1_metric: 0.8276 - val_loss: 0.4330 - val_accuracy: 0.7948 - val_f1_metric: 0.7950\n",
      "Epoch 78/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.2224 - accuracy: 0.8348 - f1_metric: 0.8348\n",
      "Epoch 00078: val_f1_metric did not improve from 0.81805\n",
      "2797/2797 [==============================] - 0s 99us/sample - loss: 0.2251 - accuracy: 0.8305 - f1_metric: 0.8308 - val_loss: 0.4023 - val_accuracy: 0.8182 - val_f1_metric: 0.8180\n",
      "Epoch 79/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.2142 - accuracy: 0.8421 - f1_metric: 0.8421\n",
      "Epoch 00079: val_f1_metric did not improve from 0.81805\n",
      "2797/2797 [==============================] - 0s 90us/sample - loss: 0.2175 - accuracy: 0.8459 - f1_metric: 0.8462 - val_loss: 0.4193 - val_accuracy: 0.8065 - val_f1_metric: 0.8065\n",
      "Epoch 80/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.2174 - accuracy: 0.8368 - f1_metric: 0.8368\n",
      "Epoch 00080: val_f1_metric did not improve from 0.81805\n",
      "2797/2797 [==============================] - 0s 97us/sample - loss: 0.2267 - accuracy: 0.8380 - f1_metric: 0.8378 - val_loss: 0.4021 - val_accuracy: 0.8132 - val_f1_metric: 0.8131\n",
      "Epoch 81/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.2170 - accuracy: 0.8424 - f1_metric: 0.8424\n",
      "Epoch 00081: val_f1_metric did not improve from 0.81805\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.2226 - accuracy: 0.8423 - f1_metric: 0.8423 - val_loss: 0.4076 - val_accuracy: 0.8123 - val_f1_metric: 0.8123\n",
      "Epoch 82/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.2264 - accuracy: 0.8326 - f1_metric: 0.8326\n",
      "Epoch 00082: val_f1_metric did not improve from 0.81805\n",
      "2797/2797 [==============================] - 0s 92us/sample - loss: 0.2251 - accuracy: 0.8302 - f1_metric: 0.8303 - val_loss: 0.4039 - val_accuracy: 0.8140 - val_f1_metric: 0.8139\n",
      "Epoch 83/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.2260 - accuracy: 0.8391 - f1_metric: 0.8391\n",
      "Epoch 00083: val_f1_metric did not improve from 0.81805\n",
      "2797/2797 [==============================] - 0s 94us/sample - loss: 0.2292 - accuracy: 0.8388 - f1_metric: 0.8392 - val_loss: 0.4224 - val_accuracy: 0.8057 - val_f1_metric: 0.8057\n",
      "Epoch 84/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.2161 - accuracy: 0.8341 - f1_metric: 0.8341\n",
      "Epoch 00084: val_f1_metric did not improve from 0.81805\n",
      "2797/2797 [==============================] - 0s 100us/sample - loss: 0.2328 - accuracy: 0.8334 - f1_metric: 0.8329 - val_loss: 0.4051 - val_accuracy: 0.8148 - val_f1_metric: 0.8148\n",
      "Epoch 85/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.2284 - accuracy: 0.8260 - f1_metric: 0.8260\n",
      "Epoch 00085: val_f1_metric did not improve from 0.81805\n",
      "2797/2797 [==============================] - 0s 97us/sample - loss: 0.2263 - accuracy: 0.8348 - f1_metric: 0.8353 - val_loss: 0.4027 - val_accuracy: 0.8165 - val_f1_metric: 0.8164\n",
      "Epoch 86/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.2173 - accuracy: 0.8416 - f1_metric: 0.8416\n",
      "Epoch 00086: val_f1_metric improved from 0.81805 to 0.82164, saving model to ./best_model_keras\n",
      "2797/2797 [==============================] - 1s 388us/sample - loss: 0.2154 - accuracy: 0.8398 - f1_metric: 0.8403 - val_loss: 0.3924 - val_accuracy: 0.8215 - val_f1_metric: 0.8216\n",
      "Epoch 87/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.2180 - accuracy: 0.8478 - f1_metric: 0.8478\n",
      "Epoch 00087: val_f1_metric improved from 0.82164 to 0.82298, saving model to ./best_model_keras\n",
      "2797/2797 [==============================] - 1s 444us/sample - loss: 0.2137 - accuracy: 0.8459 - f1_metric: 0.8454 - val_loss: 0.3919 - val_accuracy: 0.8232 - val_f1_metric: 0.8230\n",
      "Epoch 88/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.2322 - accuracy: 0.8377 - f1_metric: 0.8377\n",
      "Epoch 00088: val_f1_metric did not improve from 0.82298\n",
      "2797/2797 [==============================] - 0s 95us/sample - loss: 0.2279 - accuracy: 0.8345 - f1_metric: 0.8341 - val_loss: 0.4052 - val_accuracy: 0.8098 - val_f1_metric: 0.8098\n",
      "Epoch 89/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.2347 - accuracy: 0.8342 - f1_metric: 0.8342\n",
      "Epoch 00089: val_f1_metric did not improve from 0.82298\n",
      "2797/2797 [==============================] - 0s 103us/sample - loss: 0.2318 - accuracy: 0.8334 - f1_metric: 0.8336 - val_loss: 0.3983 - val_accuracy: 0.8123 - val_f1_metric: 0.8123\n",
      "Epoch 90/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.2375 - accuracy: 0.8327 - f1_metric: 0.8327\n",
      "Epoch 00090: val_f1_metric did not improve from 0.82298\n",
      "2797/2797 [==============================] - 0s 101us/sample - loss: 0.2349 - accuracy: 0.8252 - f1_metric: 0.8251 - val_loss: 0.4254 - val_accuracy: 0.8040 - val_f1_metric: 0.8038\n",
      "Epoch 91/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.2548 - accuracy: 0.8203 - f1_metric: 0.8202\n",
      "Epoch 00091: val_f1_metric did not improve from 0.82298\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.2381 - accuracy: 0.8237 - f1_metric: 0.8240 - val_loss: 0.4217 - val_accuracy: 0.8065 - val_f1_metric: 0.8060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.2195 - accuracy: 0.8455 - f1_metric: 0.8455\n",
      "Epoch 00092: val_f1_metric did not improve from 0.82298\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.2273 - accuracy: 0.8438 - f1_metric: 0.8439 - val_loss: 0.4176 - val_accuracy: 0.8065 - val_f1_metric: 0.8065\n",
      "Epoch 93/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.2227 - accuracy: 0.8247 - f1_metric: 0.8249\n",
      "Epoch 00093: val_f1_metric did not improve from 0.82298\n",
      "2797/2797 [==============================] - 0s 92us/sample - loss: 0.2274 - accuracy: 0.8309 - f1_metric: 0.8314 - val_loss: 0.3912 - val_accuracy: 0.8198 - val_f1_metric: 0.8197\n",
      "Epoch 94/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.2196 - accuracy: 0.8396 - f1_metric: 0.8393\n",
      "Epoch 00094: val_f1_metric did not improve from 0.82298\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.2230 - accuracy: 0.8359 - f1_metric: 0.8354 - val_loss: 0.4206 - val_accuracy: 0.8065 - val_f1_metric: 0.8061\n",
      "Epoch 95/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.2140 - accuracy: 0.8355 - f1_metric: 0.8355\n",
      "Epoch 00095: val_f1_metric improved from 0.82298 to 0.83696, saving model to ./best_model_keras\n",
      "2797/2797 [==============================] - 1s 394us/sample - loss: 0.2125 - accuracy: 0.8366 - f1_metric: 0.8362 - val_loss: 0.3652 - val_accuracy: 0.8374 - val_f1_metric: 0.8370\n",
      "Epoch 96/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.2196 - accuracy: 0.8478 - f1_metric: 0.8478\n",
      "Epoch 00096: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 101us/sample - loss: 0.2187 - accuracy: 0.8455 - f1_metric: 0.8460 - val_loss: 0.4156 - val_accuracy: 0.8140 - val_f1_metric: 0.8139\n",
      "Epoch 97/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.2355 - accuracy: 0.8212 - f1_metric: 0.8212\n",
      "Epoch 00097: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.2381 - accuracy: 0.8212 - f1_metric: 0.8218 - val_loss: 0.3981 - val_accuracy: 0.8249 - val_f1_metric: 0.8246\n",
      "Epoch 98/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.2192 - accuracy: 0.8345 - f1_metric: 0.8345\n",
      "Epoch 00098: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 92us/sample - loss: 0.2229 - accuracy: 0.8420 - f1_metric: 0.8418 - val_loss: 0.3866 - val_accuracy: 0.8224 - val_f1_metric: 0.8222\n",
      "Epoch 99/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.2248 - accuracy: 0.8304 - f1_metric: 0.8304\n",
      "Epoch 00099: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 102us/sample - loss: 0.2183 - accuracy: 0.8391 - f1_metric: 0.8391 - val_loss: 0.3838 - val_accuracy: 0.8198 - val_f1_metric: 0.8197\n",
      "Epoch 100/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.2341 - accuracy: 0.8378 - f1_metric: 0.8378\n",
      "Epoch 00100: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 96us/sample - loss: 0.2197 - accuracy: 0.8441 - f1_metric: 0.8444 - val_loss: 0.3977 - val_accuracy: 0.8182 - val_f1_metric: 0.8177\n",
      "Epoch 101/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.2061 - accuracy: 0.8524 - f1_metric: 0.8524\n",
      "Epoch 00101: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 97us/sample - loss: 0.2094 - accuracy: 0.8495 - f1_metric: 0.8497 - val_loss: 0.3849 - val_accuracy: 0.8224 - val_f1_metric: 0.8222\n",
      "Epoch 102/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.2039 - accuracy: 0.8568 - f1_metric: 0.8568\n",
      "Epoch 00102: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 92us/sample - loss: 0.2091 - accuracy: 0.8538 - f1_metric: 0.8540 - val_loss: 0.3806 - val_accuracy: 0.8307 - val_f1_metric: 0.8304\n",
      "Epoch 103/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.2144 - accuracy: 0.8462 - f1_metric: 0.8462\n",
      "Epoch 00103: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 135us/sample - loss: 0.2128 - accuracy: 0.8441 - f1_metric: 0.8441 - val_loss: 0.3776 - val_accuracy: 0.8257 - val_f1_metric: 0.8254\n",
      "Epoch 104/3000\n",
      "2688/2797 [===========================>..] - ETA: 0s - loss: 0.2106 - accuracy: 0.8486 - f1_metric: 0.8486\n",
      "Epoch 00104: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 106us/sample - loss: 0.2088 - accuracy: 0.8484 - f1_metric: 0.8484 - val_loss: 0.4002 - val_accuracy: 0.8182 - val_f1_metric: 0.8180\n",
      "Epoch 105/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.2309 - accuracy: 0.8385 - f1_metric: 0.8385\n",
      "Epoch 00105: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 96us/sample - loss: 0.2281 - accuracy: 0.8413 - f1_metric: 0.8411 - val_loss: 0.4255 - val_accuracy: 0.8032 - val_f1_metric: 0.8032\n",
      "Epoch 106/3000\n",
      "2496/2797 [=========================>....] - ETA: 0s - loss: 0.2348 - accuracy: 0.8345 - f1_metric: 0.8345\n",
      "Epoch 00106: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 154us/sample - loss: 0.2315 - accuracy: 0.8341 - f1_metric: 0.8339 - val_loss: 0.4369 - val_accuracy: 0.7990 - val_f1_metric: 0.7991\n",
      "Epoch 107/3000\n",
      "2624/2797 [===========================>..] - ETA: 0s - loss: 0.2194 - accuracy: 0.8354 - f1_metric: 0.8351\n",
      "Epoch 00107: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 166us/sample - loss: 0.2169 - accuracy: 0.8355 - f1_metric: 0.8355 - val_loss: 0.3959 - val_accuracy: 0.8173 - val_f1_metric: 0.8172\n",
      "Epoch 108/3000\n",
      "2752/2797 [============================>.] - ETA: 0s - loss: 0.2160 - accuracy: 0.8514 - f1_metric: 0.8514\n",
      "Epoch 00108: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 108us/sample - loss: 0.2155 - accuracy: 0.8509 - f1_metric: 0.8507 - val_loss: 0.4106 - val_accuracy: 0.8182 - val_f1_metric: 0.8180\n",
      "Epoch 109/3000\n",
      "2496/2797 [=========================>....] - ETA: 0s - loss: 0.2041 - accuracy: 0.8409 - f1_metric: 0.8409\n",
      "Epoch 00109: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 110us/sample - loss: 0.2073 - accuracy: 0.8380 - f1_metric: 0.8375 - val_loss: 0.3727 - val_accuracy: 0.8265 - val_f1_metric: 0.8258\n",
      "Epoch 110/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.2002 - accuracy: 0.8594 - f1_metric: 0.8594\n",
      "Epoch 00110: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.2125 - accuracy: 0.8545 - f1_metric: 0.8541 - val_loss: 0.4118 - val_accuracy: 0.8157 - val_f1_metric: 0.8156\n",
      "Epoch 111/3000\n",
      "2496/2797 [=========================>....] - ETA: 0s - loss: 0.1996 - accuracy: 0.8502 - f1_metric: 0.8502\n",
      "Epoch 00111: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 91us/sample - loss: 0.2085 - accuracy: 0.8520 - f1_metric: 0.8516 - val_loss: 0.3902 - val_accuracy: 0.8215 - val_f1_metric: 0.8217\n",
      "Epoch 112/3000\n",
      "2496/2797 [=========================>....] - ETA: 0s - loss: 0.2366 - accuracy: 0.8261 - f1_metric: 0.8264\n",
      "Epoch 00112: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 114us/sample - loss: 0.2345 - accuracy: 0.8259 - f1_metric: 0.8263 - val_loss: 0.4205 - val_accuracy: 0.7973 - val_f1_metric: 0.7969\n",
      "Epoch 113/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.2192 - accuracy: 0.8403 - f1_metric: 0.8400\n",
      "Epoch 00113: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 93us/sample - loss: 0.2179 - accuracy: 0.8402 - f1_metric: 0.8399 - val_loss: 0.3792 - val_accuracy: 0.8307 - val_f1_metric: 0.8304\n",
      "Epoch 114/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.2055 - accuracy: 0.8450 - f1_metric: 0.8450\n",
      "Epoch 00114: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 96us/sample - loss: 0.2100 - accuracy: 0.8427 - f1_metric: 0.8429 - val_loss: 0.3882 - val_accuracy: 0.8198 - val_f1_metric: 0.8200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/3000\n",
      "2112/2797 [=====================>........] - ETA: 0s - loss: 0.2261 - accuracy: 0.8423 - f1_metric: 0.8423\n",
      "Epoch 00115: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 100us/sample - loss: 0.2250 - accuracy: 0.8409 - f1_metric: 0.8415 - val_loss: 0.3976 - val_accuracy: 0.8249 - val_f1_metric: 0.8246\n",
      "Epoch 116/3000\n",
      "2560/2797 [==========================>...] - ETA: 0s - loss: 0.2129 - accuracy: 0.8492 - f1_metric: 0.8492\n",
      "Epoch 00116: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 157us/sample - loss: 0.2078 - accuracy: 0.8520 - f1_metric: 0.8522 - val_loss: 0.3752 - val_accuracy: 0.8274 - val_f1_metric: 0.8271\n",
      "Epoch 117/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.2208 - accuracy: 0.8464 - f1_metric: 0.8464\n",
      "Epoch 00117: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 154us/sample - loss: 0.2186 - accuracy: 0.8459 - f1_metric: 0.8460 - val_loss: 0.3784 - val_accuracy: 0.8274 - val_f1_metric: 0.8274\n",
      "Epoch 118/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.2182 - accuracy: 0.8398 - f1_metric: 0.8398\n",
      "Epoch 00118: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 94us/sample - loss: 0.2141 - accuracy: 0.8420 - f1_metric: 0.8423 - val_loss: 0.3866 - val_accuracy: 0.8232 - val_f1_metric: 0.8230\n",
      "Epoch 119/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.2268 - accuracy: 0.8396 - f1_metric: 0.8398\n",
      "Epoch 00119: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 99us/sample - loss: 0.2195 - accuracy: 0.8409 - f1_metric: 0.8408 - val_loss: 0.3789 - val_accuracy: 0.8224 - val_f1_metric: 0.8222\n",
      "Epoch 120/3000\n",
      "2752/2797 [============================>.] - ETA: 0s - loss: 0.2078 - accuracy: 0.8492 - f1_metric: 0.8492\n",
      "Epoch 00120: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 104us/sample - loss: 0.2075 - accuracy: 0.8491 - f1_metric: 0.8491 - val_loss: 0.3821 - val_accuracy: 0.8198 - val_f1_metric: 0.8197\n",
      "Epoch 121/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.2144 - accuracy: 0.8520 - f1_metric: 0.8520\n",
      "Epoch 00121: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 91us/sample - loss: 0.2194 - accuracy: 0.8513 - f1_metric: 0.8508 - val_loss: 0.4085 - val_accuracy: 0.8115 - val_f1_metric: 0.8115\n",
      "Epoch 122/3000\n",
      "2112/2797 [=====================>........] - ETA: 0s - loss: 0.2064 - accuracy: 0.8456 - f1_metric: 0.8456\n",
      "Epoch 00122: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 103us/sample - loss: 0.2130 - accuracy: 0.8448 - f1_metric: 0.8445 - val_loss: 0.4009 - val_accuracy: 0.8240 - val_f1_metric: 0.8238\n",
      "Epoch 123/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.2050 - accuracy: 0.8473 - f1_metric: 0.8473\n",
      "Epoch 00123: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 96us/sample - loss: 0.2106 - accuracy: 0.8488 - f1_metric: 0.8490 - val_loss: 0.4103 - val_accuracy: 0.8132 - val_f1_metric: 0.8131\n",
      "Epoch 124/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.2095 - accuracy: 0.8321 - f1_metric: 0.8321\n",
      "Epoch 00124: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.2157 - accuracy: 0.8338 - f1_metric: 0.8337 - val_loss: 0.3834 - val_accuracy: 0.8265 - val_f1_metric: 0.8263\n",
      "Epoch 125/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.2032 - accuracy: 0.8545 - f1_metric: 0.8545\n",
      "Epoch 00125: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 96us/sample - loss: 0.2015 - accuracy: 0.8541 - f1_metric: 0.8541 - val_loss: 0.3705 - val_accuracy: 0.8290 - val_f1_metric: 0.8287\n",
      "Epoch 126/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.2095 - accuracy: 0.8420 - f1_metric: 0.8420\n",
      "Epoch 00126: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 96us/sample - loss: 0.2134 - accuracy: 0.8445 - f1_metric: 0.8445 - val_loss: 0.3644 - val_accuracy: 0.8290 - val_f1_metric: 0.8287\n",
      "Epoch 127/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.2123 - accuracy: 0.8473 - f1_metric: 0.8473\n",
      "Epoch 00127: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 97us/sample - loss: 0.2080 - accuracy: 0.8473 - f1_metric: 0.8472 - val_loss: 0.3845 - val_accuracy: 0.8265 - val_f1_metric: 0.8263\n",
      "Epoch 128/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.2183 - accuracy: 0.8479 - f1_metric: 0.8479\n",
      "Epoch 00128: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 96us/sample - loss: 0.2124 - accuracy: 0.8455 - f1_metric: 0.8455 - val_loss: 0.3720 - val_accuracy: 0.8299 - val_f1_metric: 0.8296\n",
      "Epoch 129/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.2140 - accuracy: 0.8479 - f1_metric: 0.8479\n",
      "Epoch 00129: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 108us/sample - loss: 0.2112 - accuracy: 0.8498 - f1_metric: 0.8495 - val_loss: 0.3883 - val_accuracy: 0.8207 - val_f1_metric: 0.8205\n",
      "Epoch 130/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.2207 - accuracy: 0.8323 - f1_metric: 0.8323\n",
      "Epoch 00130: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 97us/sample - loss: 0.2127 - accuracy: 0.8395 - f1_metric: 0.8398 - val_loss: 0.3829 - val_accuracy: 0.8215 - val_f1_metric: 0.8213\n",
      "Epoch 131/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.2093 - accuracy: 0.8509 - f1_metric: 0.8509\n",
      "Epoch 00131: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 95us/sample - loss: 0.2125 - accuracy: 0.8527 - f1_metric: 0.8522 - val_loss: 0.3935 - val_accuracy: 0.8157 - val_f1_metric: 0.8156\n",
      "Epoch 132/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.2069 - accuracy: 0.8487 - f1_metric: 0.8487\n",
      "Epoch 00132: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 97us/sample - loss: 0.2048 - accuracy: 0.8495 - f1_metric: 0.8502 - val_loss: 0.3924 - val_accuracy: 0.8182 - val_f1_metric: 0.8180\n",
      "Epoch 133/3000\n",
      "2688/2797 [===========================>..] - ETA: 0s - loss: 0.2025 - accuracy: 0.8560 - f1_metric: 0.8560\n",
      "Epoch 00133: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 130us/sample - loss: 0.2023 - accuracy: 0.8556 - f1_metric: 0.8552 - val_loss: 0.3946 - val_accuracy: 0.8249 - val_f1_metric: 0.8246\n",
      "Epoch 134/3000\n",
      "2048/2797 [====================>.........] - ETA: 0s - loss: 0.2039 - accuracy: 0.8545 - f1_metric: 0.8545\n",
      "Epoch 00134: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 97us/sample - loss: 0.2065 - accuracy: 0.8566 - f1_metric: 0.8570 - val_loss: 0.3966 - val_accuracy: 0.8182 - val_f1_metric: 0.8180\n",
      "Epoch 135/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.2161 - accuracy: 0.8376 - f1_metric: 0.8376\n",
      "Epoch 00135: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 90us/sample - loss: 0.2103 - accuracy: 0.8388 - f1_metric: 0.8388 - val_loss: 0.3916 - val_accuracy: 0.8123 - val_f1_metric: 0.8123\n",
      "Epoch 136/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.2181 - accuracy: 0.8400 - f1_metric: 0.8402\n",
      "Epoch 00136: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 128us/sample - loss: 0.2156 - accuracy: 0.8395 - f1_metric: 0.8398 - val_loss: 0.3812 - val_accuracy: 0.8282 - val_f1_metric: 0.8279\n",
      "Epoch 137/3000\n",
      "2496/2797 [=========================>....] - ETA: 0s - loss: 0.2018 - accuracy: 0.8474 - f1_metric: 0.8474\n",
      "Epoch 00137: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 113us/sample - loss: 0.2033 - accuracy: 0.8470 - f1_metric: 0.8467 - val_loss: 0.3935 - val_accuracy: 0.8157 - val_f1_metric: 0.8156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 138/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.2149 - accuracy: 0.8385 - f1_metric: 0.8385\n",
      "Epoch 00138: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 99us/sample - loss: 0.2172 - accuracy: 0.8416 - f1_metric: 0.8413 - val_loss: 0.4018 - val_accuracy: 0.8148 - val_f1_metric: 0.8148\n",
      "Epoch 139/3000\n",
      "2496/2797 [=========================>....] - ETA: 0s - loss: 0.2089 - accuracy: 0.8353 - f1_metric: 0.8353\n",
      "Epoch 00139: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 94us/sample - loss: 0.2092 - accuracy: 0.8395 - f1_metric: 0.8401 - val_loss: 0.3689 - val_accuracy: 0.8290 - val_f1_metric: 0.8287\n",
      "Epoch 140/3000\n",
      "2624/2797 [===========================>..] - ETA: 0s - loss: 0.2115 - accuracy: 0.8510 - f1_metric: 0.8510\n",
      "Epoch 00140: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 108us/sample - loss: 0.2136 - accuracy: 0.8481 - f1_metric: 0.8476 - val_loss: 0.3813 - val_accuracy: 0.8240 - val_f1_metric: 0.8238\n",
      "Epoch 141/3000\n",
      "2112/2797 [=====================>........] - ETA: 0s - loss: 0.2160 - accuracy: 0.8594 - f1_metric: 0.8594\n",
      "Epoch 00141: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 99us/sample - loss: 0.2189 - accuracy: 0.8495 - f1_metric: 0.8493 - val_loss: 0.4144 - val_accuracy: 0.8123 - val_f1_metric: 0.8123\n",
      "Epoch 142/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.2274 - accuracy: 0.8281 - f1_metric: 0.8281\n",
      "Epoch 00142: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 97us/sample - loss: 0.2193 - accuracy: 0.8327 - f1_metric: 0.8334 - val_loss: 0.3675 - val_accuracy: 0.8340 - val_f1_metric: 0.8337\n",
      "Epoch 143/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.2013 - accuracy: 0.8606 - f1_metric: 0.8606\n",
      "Epoch 00143: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 95us/sample - loss: 0.2170 - accuracy: 0.8577 - f1_metric: 0.8579 - val_loss: 0.4192 - val_accuracy: 0.8082 - val_f1_metric: 0.8082\n",
      "Epoch 144/3000\n",
      "2624/2797 [===========================>..] - ETA: 0s - loss: 0.2011 - accuracy: 0.8357 - f1_metric: 0.8359\n",
      "Epoch 00144: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 110us/sample - loss: 0.2026 - accuracy: 0.8380 - f1_metric: 0.8381 - val_loss: 0.3636 - val_accuracy: 0.8332 - val_f1_metric: 0.8328\n",
      "Epoch 145/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.2037 - accuracy: 0.8586 - f1_metric: 0.8586\n",
      "Epoch 00145: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 89us/sample - loss: 0.2040 - accuracy: 0.8573 - f1_metric: 0.8577 - val_loss: 0.3778 - val_accuracy: 0.8257 - val_f1_metric: 0.8254\n",
      "Epoch 146/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.2064 - accuracy: 0.8503 - f1_metric: 0.8503\n",
      "Epoch 00146: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 93us/sample - loss: 0.2082 - accuracy: 0.8509 - f1_metric: 0.8513 - val_loss: 0.3927 - val_accuracy: 0.8207 - val_f1_metric: 0.8205\n",
      "Epoch 147/3000\n",
      "2112/2797 [=====================>........] - ETA: 0s - loss: 0.2000 - accuracy: 0.8442 - f1_metric: 0.8442\n",
      "Epoch 00147: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.2070 - accuracy: 0.8495 - f1_metric: 0.8499 - val_loss: 0.3975 - val_accuracy: 0.8165 - val_f1_metric: 0.8164\n",
      "Epoch 148/3000\n",
      "2048/2797 [====================>.........] - ETA: 0s - loss: 0.2052 - accuracy: 0.8555 - f1_metric: 0.8555\n",
      "Epoch 00148: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 105us/sample - loss: 0.1935 - accuracy: 0.8556 - f1_metric: 0.8559 - val_loss: 0.3788 - val_accuracy: 0.8265 - val_f1_metric: 0.8263\n",
      "Epoch 149/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.2111 - accuracy: 0.8542 - f1_metric: 0.8542\n",
      "Epoch 00149: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 97us/sample - loss: 0.2169 - accuracy: 0.8491 - f1_metric: 0.8491 - val_loss: 0.4230 - val_accuracy: 0.8040 - val_f1_metric: 0.8041\n",
      "Epoch 150/3000\n",
      "2112/2797 [=====================>........] - ETA: 0s - loss: 0.2119 - accuracy: 0.8513 - f1_metric: 0.8513\n",
      "Epoch 00150: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.2043 - accuracy: 0.8481 - f1_metric: 0.8479 - val_loss: 0.4166 - val_accuracy: 0.8090 - val_f1_metric: 0.8090\n",
      "Epoch 151/3000\n",
      "2560/2797 [==========================>...] - ETA: 0s - loss: 0.2038 - accuracy: 0.8555 - f1_metric: 0.8555\n",
      "Epoch 00151: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 115us/sample - loss: 0.2027 - accuracy: 0.8531 - f1_metric: 0.8528 - val_loss: 0.3902 - val_accuracy: 0.8240 - val_f1_metric: 0.8238\n",
      "Epoch 152/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.1973 - accuracy: 0.8534 - f1_metric: 0.8534\n",
      "Epoch 00152: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 104us/sample - loss: 0.1997 - accuracy: 0.8563 - f1_metric: 0.8563 - val_loss: 0.3902 - val_accuracy: 0.8274 - val_f1_metric: 0.8271\n",
      "Epoch 153/3000\n",
      "2496/2797 [=========================>....] - ETA: 0s - loss: 0.2095 - accuracy: 0.8377 - f1_metric: 0.8377\n",
      "Epoch 00153: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 92us/sample - loss: 0.2094 - accuracy: 0.8420 - f1_metric: 0.8417 - val_loss: 0.3847 - val_accuracy: 0.8249 - val_f1_metric: 0.8246\n",
      "Epoch 154/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1966 - accuracy: 0.8573 - f1_metric: 0.8573\n",
      "Epoch 00154: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 92us/sample - loss: 0.2006 - accuracy: 0.8523 - f1_metric: 0.8526 - val_loss: 0.3979 - val_accuracy: 0.8215 - val_f1_metric: 0.8213\n",
      "Epoch 155/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1861 - accuracy: 0.8581 - f1_metric: 0.8581\n",
      "Epoch 00155: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 93us/sample - loss: 0.1917 - accuracy: 0.8566 - f1_metric: 0.8570 - val_loss: 0.3767 - val_accuracy: 0.8290 - val_f1_metric: 0.8287\n",
      "Epoch 156/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1948 - accuracy: 0.8522 - f1_metric: 0.8522\n",
      "Epoch 00156: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 97us/sample - loss: 0.2004 - accuracy: 0.8481 - f1_metric: 0.8483 - val_loss: 0.3842 - val_accuracy: 0.8232 - val_f1_metric: 0.8230\n",
      "Epoch 157/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.2122 - accuracy: 0.8398 - f1_metric: 0.8398\n",
      "Epoch 00157: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 100us/sample - loss: 0.2105 - accuracy: 0.8388 - f1_metric: 0.8386 - val_loss: 0.3899 - val_accuracy: 0.8165 - val_f1_metric: 0.8164\n",
      "Epoch 158/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1889 - accuracy: 0.8573 - f1_metric: 0.8573\n",
      "Epoch 00158: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 97us/sample - loss: 0.2031 - accuracy: 0.8534 - f1_metric: 0.8537 - val_loss: 0.3725 - val_accuracy: 0.8324 - val_f1_metric: 0.8320\n",
      "Epoch 159/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1791 - accuracy: 0.8715 - f1_metric: 0.8715\n",
      "Epoch 00159: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 96us/sample - loss: 0.1878 - accuracy: 0.8659 - f1_metric: 0.8655 - val_loss: 0.3675 - val_accuracy: 0.8299 - val_f1_metric: 0.8296\n",
      "Epoch 160/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1917 - accuracy: 0.8576 - f1_metric: 0.8576\n",
      "Epoch 00160: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.1928 - accuracy: 0.8570 - f1_metric: 0.8574 - val_loss: 0.3666 - val_accuracy: 0.8299 - val_f1_metric: 0.8296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1782 - accuracy: 0.8666 - f1_metric: 0.8666\n",
      "Epoch 00161: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 94us/sample - loss: 0.1900 - accuracy: 0.8656 - f1_metric: 0.8657 - val_loss: 0.3667 - val_accuracy: 0.8290 - val_f1_metric: 0.8287\n",
      "Epoch 162/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.2003 - accuracy: 0.8547 - f1_metric: 0.8547\n",
      "Epoch 00162: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 93us/sample - loss: 0.2048 - accuracy: 0.8534 - f1_metric: 0.8535 - val_loss: 0.3745 - val_accuracy: 0.8324 - val_f1_metric: 0.8320\n",
      "Epoch 163/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.1913 - accuracy: 0.8602 - f1_metric: 0.8602\n",
      "Epoch 00163: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 88us/sample - loss: 0.1876 - accuracy: 0.8606 - f1_metric: 0.8609 - val_loss: 0.3667 - val_accuracy: 0.8332 - val_f1_metric: 0.8328\n",
      "Epoch 164/3000\n",
      "2496/2797 [=========================>....] - ETA: 0s - loss: 0.2122 - accuracy: 0.8514 - f1_metric: 0.8514\n",
      "Epoch 00164: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 94us/sample - loss: 0.2114 - accuracy: 0.8506 - f1_metric: 0.8504 - val_loss: 0.3951 - val_accuracy: 0.8232 - val_f1_metric: 0.8230\n",
      "Epoch 165/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.2096 - accuracy: 0.8526 - f1_metric: 0.8526\n",
      "Epoch 00165: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.2085 - accuracy: 0.8484 - f1_metric: 0.8479 - val_loss: 0.3753 - val_accuracy: 0.8257 - val_f1_metric: 0.8254\n",
      "Epoch 166/3000\n",
      "2112/2797 [=====================>........] - ETA: 0s - loss: 0.1979 - accuracy: 0.8513 - f1_metric: 0.8513\n",
      "Epoch 00166: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 99us/sample - loss: 0.1943 - accuracy: 0.8598 - f1_metric: 0.8597 - val_loss: 0.3694 - val_accuracy: 0.8282 - val_f1_metric: 0.8279\n",
      "Epoch 167/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1895 - accuracy: 0.8516 - f1_metric: 0.8516\n",
      "Epoch 00167: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 93us/sample - loss: 0.1875 - accuracy: 0.8563 - f1_metric: 0.8568 - val_loss: 0.3501 - val_accuracy: 0.8365 - val_f1_metric: 0.8361\n",
      "Epoch 168/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.2076 - accuracy: 0.8533 - f1_metric: 0.8533\n",
      "Epoch 00168: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 101us/sample - loss: 0.2030 - accuracy: 0.8556 - f1_metric: 0.8555 - val_loss: 0.3940 - val_accuracy: 0.8215 - val_f1_metric: 0.8213\n",
      "Epoch 169/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.1950 - accuracy: 0.8617 - f1_metric: 0.8617\n",
      "Epoch 00169: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 99us/sample - loss: 0.2011 - accuracy: 0.8595 - f1_metric: 0.8600 - val_loss: 0.3868 - val_accuracy: 0.8232 - val_f1_metric: 0.8230\n",
      "Epoch 170/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.2087 - accuracy: 0.8500 - f1_metric: 0.8500\n",
      "Epoch 00170: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 96us/sample - loss: 0.2030 - accuracy: 0.8520 - f1_metric: 0.8516 - val_loss: 0.3952 - val_accuracy: 0.8157 - val_f1_metric: 0.8156\n",
      "Epoch 171/3000\n",
      "2560/2797 [==========================>...] - ETA: 0s - loss: 0.1973 - accuracy: 0.8543 - f1_metric: 0.8541\n",
      "Epoch 00171: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 87us/sample - loss: 0.1953 - accuracy: 0.8577 - f1_metric: 0.8577 - val_loss: 0.3658 - val_accuracy: 0.8299 - val_f1_metric: 0.8296\n",
      "Epoch 172/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.2100 - accuracy: 0.8487 - f1_metric: 0.8487\n",
      "Epoch 00172: val_f1_metric did not improve from 0.83696\n",
      "2797/2797 [==============================] - 0s 91us/sample - loss: 0.2040 - accuracy: 0.8502 - f1_metric: 0.8503 - val_loss: 0.3696 - val_accuracy: 0.8265 - val_f1_metric: 0.8263\n",
      "Epoch 173/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1960 - accuracy: 0.8585 - f1_metric: 0.8585\n",
      "Epoch 00173: val_f1_metric improved from 0.83696 to 0.84137, saving model to ./best_model_keras\n",
      "2797/2797 [==============================] - 1s 453us/sample - loss: 0.2037 - accuracy: 0.8581 - f1_metric: 0.8583 - val_loss: 0.3631 - val_accuracy: 0.8415 - val_f1_metric: 0.8414\n",
      "Epoch 174/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1804 - accuracy: 0.8674 - f1_metric: 0.8674\n",
      "Epoch 00174: val_f1_metric did not improve from 0.84137\n",
      "2797/2797 [==============================] - 0s 96us/sample - loss: 0.1898 - accuracy: 0.8627 - f1_metric: 0.8626 - val_loss: 0.3710 - val_accuracy: 0.8315 - val_f1_metric: 0.8315\n",
      "Epoch 175/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.2074 - accuracy: 0.8454 - f1_metric: 0.8454\n",
      "Epoch 00175: val_f1_metric did not improve from 0.84137\n",
      "2797/2797 [==============================] - 0s 92us/sample - loss: 0.1997 - accuracy: 0.8513 - f1_metric: 0.8515 - val_loss: 0.3829 - val_accuracy: 0.8232 - val_f1_metric: 0.8230\n",
      "Epoch 176/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1911 - accuracy: 0.8657 - f1_metric: 0.8655\n",
      "Epoch 00176: val_f1_metric did not improve from 0.84137\n",
      "2797/2797 [==============================] - 0s 97us/sample - loss: 0.1873 - accuracy: 0.8641 - f1_metric: 0.8640 - val_loss: 0.3761 - val_accuracy: 0.8290 - val_f1_metric: 0.8287\n",
      "Epoch 177/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1968 - accuracy: 0.8633 - f1_metric: 0.8633\n",
      "Epoch 00177: val_f1_metric did not improve from 0.84137\n",
      "2797/2797 [==============================] - 0s 94us/sample - loss: 0.1963 - accuracy: 0.8598 - f1_metric: 0.8602 - val_loss: 0.3865 - val_accuracy: 0.8207 - val_f1_metric: 0.8205\n",
      "Epoch 178/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1986 - accuracy: 0.8572 - f1_metric: 0.8572\n",
      "Epoch 00178: val_f1_metric did not improve from 0.84137\n",
      "2797/2797 [==============================] - 0s 94us/sample - loss: 0.2012 - accuracy: 0.8534 - f1_metric: 0.8534 - val_loss: 0.4019 - val_accuracy: 0.8157 - val_f1_metric: 0.8159\n",
      "Epoch 179/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.2017 - accuracy: 0.8550 - f1_metric: 0.8550\n",
      "Epoch 00179: val_f1_metric did not improve from 0.84137\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.2081 - accuracy: 0.8531 - f1_metric: 0.8531 - val_loss: 0.3802 - val_accuracy: 0.8249 - val_f1_metric: 0.8249\n",
      "Epoch 180/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1955 - accuracy: 0.8571 - f1_metric: 0.8571\n",
      "Epoch 00180: val_f1_metric did not improve from 0.84137\n",
      "2797/2797 [==============================] - 0s 95us/sample - loss: 0.1960 - accuracy: 0.8595 - f1_metric: 0.8597 - val_loss: 0.3740 - val_accuracy: 0.8257 - val_f1_metric: 0.8257\n",
      "Epoch 181/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.1957 - accuracy: 0.8470 - f1_metric: 0.8470\n",
      "Epoch 00181: val_f1_metric did not improve from 0.84137\n",
      "2797/2797 [==============================] - 0s 95us/sample - loss: 0.2003 - accuracy: 0.8481 - f1_metric: 0.8476 - val_loss: 0.3657 - val_accuracy: 0.8332 - val_f1_metric: 0.8331\n",
      "Epoch 182/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1891 - accuracy: 0.8563 - f1_metric: 0.8563\n",
      "Epoch 00182: val_f1_metric did not improve from 0.84137\n",
      "2797/2797 [==============================] - 0s 94us/sample - loss: 0.1870 - accuracy: 0.8545 - f1_metric: 0.8547 - val_loss: 0.3679 - val_accuracy: 0.8265 - val_f1_metric: 0.8263\n",
      "Epoch 183/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1889 - accuracy: 0.8720 - f1_metric: 0.8720\n",
      "Epoch 00183: val_f1_metric did not improve from 0.84137\n",
      "2797/2797 [==============================] - 0s 94us/sample - loss: 0.1889 - accuracy: 0.8684 - f1_metric: 0.8687 - val_loss: 0.3649 - val_accuracy: 0.8307 - val_f1_metric: 0.8307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 184/3000\n",
      "2112/2797 [=====================>........] - ETA: 0s - loss: 0.1867 - accuracy: 0.8632 - f1_metric: 0.8632\n",
      "Epoch 00184: val_f1_metric did not improve from 0.84137\n",
      "2797/2797 [==============================] - 0s 100us/sample - loss: 0.1992 - accuracy: 0.8559 - f1_metric: 0.8552 - val_loss: 0.4067 - val_accuracy: 0.8140 - val_f1_metric: 0.8139\n",
      "Epoch 185/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.1926 - accuracy: 0.8470 - f1_metric: 0.8470\n",
      "Epoch 00185: val_f1_metric did not improve from 0.84137\n",
      "2797/2797 [==============================] - 0s 97us/sample - loss: 0.1967 - accuracy: 0.8506 - f1_metric: 0.8502 - val_loss: 0.3714 - val_accuracy: 0.8307 - val_f1_metric: 0.8304\n",
      "Epoch 186/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.2006 - accuracy: 0.8483 - f1_metric: 0.8483\n",
      "Epoch 00186: val_f1_metric did not improve from 0.84137\n",
      "2797/2797 [==============================] - 0s 90us/sample - loss: 0.1995 - accuracy: 0.8509 - f1_metric: 0.8512 - val_loss: 0.3809 - val_accuracy: 0.8215 - val_f1_metric: 0.8213\n",
      "Epoch 187/3000\n",
      "2496/2797 [=========================>....] - ETA: 0s - loss: 0.1953 - accuracy: 0.8538 - f1_metric: 0.8538\n",
      "Epoch 00187: val_f1_metric did not improve from 0.84137\n",
      "2797/2797 [==============================] - 0s 91us/sample - loss: 0.1921 - accuracy: 0.8559 - f1_metric: 0.8564 - val_loss: 0.3576 - val_accuracy: 0.8399 - val_f1_metric: 0.8397\n",
      "Epoch 188/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1947 - accuracy: 0.8640 - f1_metric: 0.8640\n",
      "Epoch 00188: val_f1_metric did not improve from 0.84137\n",
      "2797/2797 [==============================] - 0s 95us/sample - loss: 0.2023 - accuracy: 0.8556 - f1_metric: 0.8555 - val_loss: 0.4188 - val_accuracy: 0.8123 - val_f1_metric: 0.8123\n",
      "Epoch 189/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.1778 - accuracy: 0.8539 - f1_metric: 0.8539\n",
      "Epoch 00189: val_f1_metric improved from 0.84137 to 0.84959, saving model to ./best_model_keras\n",
      "2797/2797 [==============================] - 1s 400us/sample - loss: 0.1851 - accuracy: 0.8577 - f1_metric: 0.8575 - val_loss: 0.3453 - val_accuracy: 0.8499 - val_f1_metric: 0.8496\n",
      "Epoch 190/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.1844 - accuracy: 0.8621 - f1_metric: 0.8621\n",
      "Epoch 00190: val_f1_metric did not improve from 0.84959\n",
      "2797/2797 [==============================] - 0s 100us/sample - loss: 0.1859 - accuracy: 0.8641 - f1_metric: 0.8642 - val_loss: 0.3653 - val_accuracy: 0.8340 - val_f1_metric: 0.8337\n",
      "Epoch 191/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1833 - accuracy: 0.8656 - f1_metric: 0.8656\n",
      "Epoch 00191: val_f1_metric did not improve from 0.84959\n",
      "2797/2797 [==============================] - 0s 97us/sample - loss: 0.1898 - accuracy: 0.8638 - f1_metric: 0.8637 - val_loss: 0.3716 - val_accuracy: 0.8349 - val_f1_metric: 0.8345\n",
      "Epoch 192/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1811 - accuracy: 0.8670 - f1_metric: 0.8670\n",
      "Epoch 00192: val_f1_metric did not improve from 0.84959\n",
      "2797/2797 [==============================] - 0s 94us/sample - loss: 0.1920 - accuracy: 0.8641 - f1_metric: 0.8645 - val_loss: 0.3549 - val_accuracy: 0.8415 - val_f1_metric: 0.8414\n",
      "Epoch 193/3000\n",
      "2112/2797 [=====================>........] - ETA: 0s - loss: 0.1940 - accuracy: 0.8598 - f1_metric: 0.8598\n",
      "Epoch 00193: val_f1_metric did not improve from 0.84959\n",
      "2797/2797 [==============================] - 0s 104us/sample - loss: 0.1900 - accuracy: 0.8602 - f1_metric: 0.8600 - val_loss: 0.3794 - val_accuracy: 0.8315 - val_f1_metric: 0.8315\n",
      "Epoch 194/3000\n",
      "2112/2797 [=====================>........] - ETA: 0s - loss: 0.1790 - accuracy: 0.8651 - f1_metric: 0.8651\n",
      "Epoch 00194: val_f1_metric did not improve from 0.84959\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.1899 - accuracy: 0.8631 - f1_metric: 0.8628 - val_loss: 0.3609 - val_accuracy: 0.8407 - val_f1_metric: 0.8405\n",
      "Epoch 195/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1797 - accuracy: 0.8653 - f1_metric: 0.8653\n",
      "Epoch 00195: val_f1_metric did not improve from 0.84959\n",
      "2797/2797 [==============================] - 0s 92us/sample - loss: 0.1820 - accuracy: 0.8677 - f1_metric: 0.8677 - val_loss: 0.3577 - val_accuracy: 0.8432 - val_f1_metric: 0.8430\n",
      "Epoch 196/3000\n",
      "2048/2797 [====================>.........] - ETA: 0s - loss: 0.1995 - accuracy: 0.8599 - f1_metric: 0.8599\n",
      "Epoch 00196: val_f1_metric did not improve from 0.84959\n",
      "2797/2797 [==============================] - 0s 100us/sample - loss: 0.2036 - accuracy: 0.8588 - f1_metric: 0.8582 - val_loss: 0.3962 - val_accuracy: 0.8224 - val_f1_metric: 0.8222\n",
      "Epoch 197/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.1837 - accuracy: 0.8621 - f1_metric: 0.8621\n",
      "Epoch 00197: val_f1_metric did not improve from 0.84959\n",
      "2797/2797 [==============================] - 0s 99us/sample - loss: 0.1945 - accuracy: 0.8591 - f1_metric: 0.8593 - val_loss: 0.3805 - val_accuracy: 0.8332 - val_f1_metric: 0.8331\n",
      "Epoch 198/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1861 - accuracy: 0.8594 - f1_metric: 0.8594\n",
      "Epoch 00198: val_f1_metric did not improve from 0.84959\n",
      "2797/2797 [==============================] - 0s 99us/sample - loss: 0.1899 - accuracy: 0.8606 - f1_metric: 0.8608 - val_loss: 0.3672 - val_accuracy: 0.8349 - val_f1_metric: 0.8345\n",
      "Epoch 199/3000\n",
      "2752/2797 [============================>.] - ETA: 0s - loss: 0.2140 - accuracy: 0.8423 - f1_metric: 0.8423\n",
      "Epoch 00199: val_f1_metric did not improve from 0.84959\n",
      "2797/2797 [==============================] - 0s 99us/sample - loss: 0.2123 - accuracy: 0.8430 - f1_metric: 0.8434 - val_loss: 0.3746 - val_accuracy: 0.8324 - val_f1_metric: 0.8320\n",
      "Epoch 200/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1970 - accuracy: 0.8539 - f1_metric: 0.8539\n",
      "Epoch 00200: val_f1_metric did not improve from 0.84959\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.1900 - accuracy: 0.8591 - f1_metric: 0.8592 - val_loss: 0.3475 - val_accuracy: 0.8449 - val_f1_metric: 0.8444\n",
      "Epoch 201/3000\n",
      "2688/2797 [===========================>..] - ETA: 0s - loss: 0.1909 - accuracy: 0.8694 - f1_metric: 0.8694\n",
      "Epoch 00201: val_f1_metric did not improve from 0.84959\n",
      "2797/2797 [==============================] - 0s 110us/sample - loss: 0.1895 - accuracy: 0.8684 - f1_metric: 0.8684 - val_loss: 0.3719 - val_accuracy: 0.8349 - val_f1_metric: 0.8345\n",
      "Epoch 202/3000\n",
      "2752/2797 [============================>.] - ETA: 0s - loss: 0.1922 - accuracy: 0.8608 - f1_metric: 0.8608\n",
      "Epoch 00202: val_f1_metric did not improve from 0.84959\n",
      "2797/2797 [==============================] - 0s 113us/sample - loss: 0.1907 - accuracy: 0.8613 - f1_metric: 0.8615 - val_loss: 0.3750 - val_accuracy: 0.8324 - val_f1_metric: 0.8320\n",
      "Epoch 203/3000\n",
      "2624/2797 [===========================>..] - ETA: 0s - loss: 0.2010 - accuracy: 0.8510 - f1_metric: 0.8510\n",
      "Epoch 00203: val_f1_metric did not improve from 0.84959\n",
      "2797/2797 [==============================] - 0s 126us/sample - loss: 0.2013 - accuracy: 0.8484 - f1_metric: 0.8482 - val_loss: 0.3701 - val_accuracy: 0.8357 - val_f1_metric: 0.8356\n",
      "Epoch 204/3000\n",
      "2112/2797 [=====================>........] - ETA: 0s - loss: 0.1761 - accuracy: 0.8750 - f1_metric: 0.8750\n",
      "Epoch 00204: val_f1_metric improved from 0.84959 to 0.85042, saving model to ./best_model_keras\n",
      "2797/2797 [==============================] - 1s 395us/sample - loss: 0.1838 - accuracy: 0.8745 - f1_metric: 0.8748 - val_loss: 0.3436 - val_accuracy: 0.8507 - val_f1_metric: 0.8504\n",
      "Epoch 205/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1892 - accuracy: 0.8607 - f1_metric: 0.8607\n",
      "Epoch 00205: val_f1_metric improved from 0.85042 to 0.85288, saving model to ./best_model_keras\n",
      "2797/2797 [==============================] - 1s 449us/sample - loss: 0.1891 - accuracy: 0.8649 - f1_metric: 0.8652 - val_loss: 0.3368 - val_accuracy: 0.8532 - val_f1_metric: 0.8529\n",
      "Epoch 206/3000\n",
      "2624/2797 [===========================>..] - ETA: 0s - loss: 0.1886 - accuracy: 0.8700 - f1_metric: 0.8700\n",
      "Epoch 00206: val_f1_metric did not improve from 0.85288\n",
      "2797/2797 [==============================] - 0s 109us/sample - loss: 0.1911 - accuracy: 0.8677 - f1_metric: 0.8677 - val_loss: 0.3712 - val_accuracy: 0.8315 - val_f1_metric: 0.8315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 207/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.1889 - accuracy: 0.8618 - f1_metric: 0.8618\n",
      "Epoch 00207: val_f1_metric did not improve from 0.85288\n",
      "2797/2797 [==============================] - 0s 91us/sample - loss: 0.1879 - accuracy: 0.8638 - f1_metric: 0.8638 - val_loss: 0.3738 - val_accuracy: 0.8282 - val_f1_metric: 0.8282\n",
      "Epoch 208/3000\n",
      "2112/2797 [=====================>........] - ETA: 0s - loss: 0.1726 - accuracy: 0.8674 - f1_metric: 0.8674\n",
      "Epoch 00208: val_f1_metric did not improve from 0.85288\n",
      "2797/2797 [==============================] - 0s 101us/sample - loss: 0.1745 - accuracy: 0.8674 - f1_metric: 0.8671 - val_loss: 0.3426 - val_accuracy: 0.8524 - val_f1_metric: 0.8521\n",
      "Epoch 209/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1991 - accuracy: 0.8661 - f1_metric: 0.8661\n",
      "Epoch 00209: val_f1_metric did not improve from 0.85288\n",
      "2797/2797 [==============================] - 0s 95us/sample - loss: 0.2002 - accuracy: 0.8613 - f1_metric: 0.8616 - val_loss: 0.4098 - val_accuracy: 0.8148 - val_f1_metric: 0.8148\n",
      "Epoch 210/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1729 - accuracy: 0.8668 - f1_metric: 0.8668\n",
      "Epoch 00210: val_f1_metric improved from 0.85288 to 0.86522, saving model to ./best_model_keras\n",
      "2797/2797 [==============================] - 1s 397us/sample - loss: 0.1716 - accuracy: 0.8691 - f1_metric: 0.8688 - val_loss: 0.3149 - val_accuracy: 0.8657 - val_f1_metric: 0.8652\n",
      "Epoch 211/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.1840 - accuracy: 0.8773 - f1_metric: 0.8773\n",
      "Epoch 00211: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 100us/sample - loss: 0.1843 - accuracy: 0.8809 - f1_metric: 0.8808 - val_loss: 0.3640 - val_accuracy: 0.8399 - val_f1_metric: 0.8397\n",
      "Epoch 212/3000\n",
      "2560/2797 [==========================>...] - ETA: 0s - loss: 0.1888 - accuracy: 0.8574 - f1_metric: 0.8574\n",
      "Epoch 00212: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 106us/sample - loss: 0.1884 - accuracy: 0.8577 - f1_metric: 0.8575 - val_loss: 0.3480 - val_accuracy: 0.8449 - val_f1_metric: 0.8447\n",
      "Epoch 213/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1884 - accuracy: 0.8641 - f1_metric: 0.8641\n",
      "Epoch 00213: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 92us/sample - loss: 0.1787 - accuracy: 0.8706 - f1_metric: 0.8710 - val_loss: 0.3502 - val_accuracy: 0.8482 - val_f1_metric: 0.8479\n",
      "Epoch 214/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.1702 - accuracy: 0.8758 - f1_metric: 0.8758\n",
      "Epoch 00214: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 95us/sample - loss: 0.1716 - accuracy: 0.8781 - f1_metric: 0.8783 - val_loss: 0.3501 - val_accuracy: 0.8482 - val_f1_metric: 0.8479\n",
      "Epoch 215/3000\n",
      "2112/2797 [=====================>........] - ETA: 0s - loss: 0.1759 - accuracy: 0.8660 - f1_metric: 0.8660\n",
      "Epoch 00215: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 96us/sample - loss: 0.1730 - accuracy: 0.8749 - f1_metric: 0.8747 - val_loss: 0.3593 - val_accuracy: 0.8415 - val_f1_metric: 0.8414\n",
      "Epoch 216/3000\n",
      "2496/2797 [=========================>....] - ETA: 0s - loss: 0.1843 - accuracy: 0.8646 - f1_metric: 0.8646\n",
      "Epoch 00216: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 89us/sample - loss: 0.1893 - accuracy: 0.8616 - f1_metric: 0.8612 - val_loss: 0.3561 - val_accuracy: 0.8440 - val_f1_metric: 0.8438\n",
      "Epoch 217/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1994 - accuracy: 0.8568 - f1_metric: 0.8568\n",
      "Epoch 00217: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 99us/sample - loss: 0.2020 - accuracy: 0.8588 - f1_metric: 0.8588 - val_loss: 0.3711 - val_accuracy: 0.8365 - val_f1_metric: 0.8364\n",
      "Epoch 218/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1890 - accuracy: 0.8559 - f1_metric: 0.8559\n",
      "Epoch 00218: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 96us/sample - loss: 0.1898 - accuracy: 0.8570 - f1_metric: 0.8568 - val_loss: 0.3571 - val_accuracy: 0.8424 - val_f1_metric: 0.8422\n",
      "Epoch 219/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1732 - accuracy: 0.8652 - f1_metric: 0.8652\n",
      "Epoch 00219: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 97us/sample - loss: 0.1796 - accuracy: 0.8666 - f1_metric: 0.8665 - val_loss: 0.3368 - val_accuracy: 0.8490 - val_f1_metric: 0.8488\n",
      "Epoch 220/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.1828 - accuracy: 0.8729 - f1_metric: 0.8729\n",
      "Epoch 00220: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 89us/sample - loss: 0.1830 - accuracy: 0.8699 - f1_metric: 0.8698 - val_loss: 0.3459 - val_accuracy: 0.8440 - val_f1_metric: 0.8438\n",
      "Epoch 221/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1908 - accuracy: 0.8687 - f1_metric: 0.8687\n",
      "Epoch 00221: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 96us/sample - loss: 0.1903 - accuracy: 0.8656 - f1_metric: 0.8654 - val_loss: 0.3658 - val_accuracy: 0.8307 - val_f1_metric: 0.8307\n",
      "Epoch 222/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.1791 - accuracy: 0.8681 - f1_metric: 0.8681\n",
      "Epoch 00222: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 97us/sample - loss: 0.1847 - accuracy: 0.8720 - f1_metric: 0.8724 - val_loss: 0.3382 - val_accuracy: 0.8449 - val_f1_metric: 0.8447\n",
      "Epoch 223/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.1700 - accuracy: 0.8754 - f1_metric: 0.8754\n",
      "Epoch 00223: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 89us/sample - loss: 0.1724 - accuracy: 0.8724 - f1_metric: 0.8723 - val_loss: 0.3559 - val_accuracy: 0.8432 - val_f1_metric: 0.8430\n",
      "Epoch 224/3000\n",
      "2496/2797 [=========================>....] - ETA: 0s - loss: 0.2057 - accuracy: 0.8602 - f1_metric: 0.8602\n",
      "Epoch 00224: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 87us/sample - loss: 0.2019 - accuracy: 0.8602 - f1_metric: 0.8600 - val_loss: 0.4220 - val_accuracy: 0.8098 - val_f1_metric: 0.8098\n",
      "Epoch 225/3000\n",
      "2624/2797 [===========================>..] - ETA: 0s - loss: 0.1867 - accuracy: 0.8575 - f1_metric: 0.8575\n",
      "Epoch 00225: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 107us/sample - loss: 0.1891 - accuracy: 0.8581 - f1_metric: 0.8580 - val_loss: 0.3510 - val_accuracy: 0.8399 - val_f1_metric: 0.8397\n",
      "Epoch 226/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1805 - accuracy: 0.8616 - f1_metric: 0.8616\n",
      "Epoch 00226: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.1788 - accuracy: 0.8649 - f1_metric: 0.8652 - val_loss: 0.3392 - val_accuracy: 0.8499 - val_f1_metric: 0.8494\n",
      "Epoch 227/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1995 - accuracy: 0.8531 - f1_metric: 0.8531\n",
      "Epoch 00227: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 92us/sample - loss: 0.1908 - accuracy: 0.8538 - f1_metric: 0.8542 - val_loss: 0.3552 - val_accuracy: 0.8415 - val_f1_metric: 0.8409\n",
      "Epoch 228/3000\n",
      "2496/2797 [=========================>....] - ETA: 0s - loss: 0.1795 - accuracy: 0.8746 - f1_metric: 0.8746\n",
      "Epoch 00228: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 89us/sample - loss: 0.1787 - accuracy: 0.8706 - f1_metric: 0.8701 - val_loss: 0.3759 - val_accuracy: 0.8315 - val_f1_metric: 0.8315\n",
      "Epoch 229/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1861 - accuracy: 0.8640 - f1_metric: 0.8640\n",
      "Epoch 00229: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 120us/sample - loss: 0.1887 - accuracy: 0.8609 - f1_metric: 0.8608 - val_loss: 0.4002 - val_accuracy: 0.8157 - val_f1_metric: 0.8156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 230/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.1797 - accuracy: 0.8680 - f1_metric: 0.8680\n",
      "Epoch 00230: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 111us/sample - loss: 0.1810 - accuracy: 0.8691 - f1_metric: 0.8694 - val_loss: 0.3847 - val_accuracy: 0.8282 - val_f1_metric: 0.8282\n",
      "Epoch 231/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1732 - accuracy: 0.8656 - f1_metric: 0.8656\n",
      "Epoch 00231: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 103us/sample - loss: 0.1727 - accuracy: 0.8684 - f1_metric: 0.8690 - val_loss: 0.3332 - val_accuracy: 0.8532 - val_f1_metric: 0.8532\n",
      "Epoch 232/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1711 - accuracy: 0.8776 - f1_metric: 0.8776\n",
      "Epoch 00232: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 101us/sample - loss: 0.1683 - accuracy: 0.8763 - f1_metric: 0.8761 - val_loss: 0.3341 - val_accuracy: 0.8524 - val_f1_metric: 0.8524\n",
      "Epoch 233/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1827 - accuracy: 0.8911 - f1_metric: 0.8911\n",
      "Epoch 00233: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 99us/sample - loss: 0.1810 - accuracy: 0.8849 - f1_metric: 0.8846 - val_loss: 0.3938 - val_accuracy: 0.8290 - val_f1_metric: 0.8290\n",
      "Epoch 234/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1669 - accuracy: 0.8689 - f1_metric: 0.8689\n",
      "Epoch 00234: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 95us/sample - loss: 0.1720 - accuracy: 0.8695 - f1_metric: 0.8693 - val_loss: 0.3597 - val_accuracy: 0.8440 - val_f1_metric: 0.8438\n",
      "Epoch 235/3000\n",
      "2496/2797 [=========================>....] - ETA: 0s - loss: 0.2018 - accuracy: 0.8638 - f1_metric: 0.8638\n",
      "Epoch 00235: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 88us/sample - loss: 0.2005 - accuracy: 0.8627 - f1_metric: 0.8627 - val_loss: 0.3949 - val_accuracy: 0.8224 - val_f1_metric: 0.8225\n",
      "Epoch 236/3000\n",
      "2560/2797 [==========================>...] - ETA: 0s - loss: 0.1830 - accuracy: 0.8680 - f1_metric: 0.8680\n",
      "Epoch 00236: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 91us/sample - loss: 0.1814 - accuracy: 0.8691 - f1_metric: 0.8694 - val_loss: 0.3577 - val_accuracy: 0.8399 - val_f1_metric: 0.8397\n",
      "Epoch 237/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1870 - accuracy: 0.8674 - f1_metric: 0.8674\n",
      "Epoch 00237: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 103us/sample - loss: 0.1864 - accuracy: 0.8638 - f1_metric: 0.8632 - val_loss: 0.3687 - val_accuracy: 0.8349 - val_f1_metric: 0.8348\n",
      "Epoch 238/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1833 - accuracy: 0.8714 - f1_metric: 0.8714\n",
      "Epoch 00238: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 95us/sample - loss: 0.1827 - accuracy: 0.8724 - f1_metric: 0.8728 - val_loss: 0.3841 - val_accuracy: 0.8257 - val_f1_metric: 0.8257\n",
      "Epoch 239/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1719 - accuracy: 0.8659 - f1_metric: 0.8659\n",
      "Epoch 00239: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 95us/sample - loss: 0.1691 - accuracy: 0.8727 - f1_metric: 0.8725 - val_loss: 0.3465 - val_accuracy: 0.8474 - val_f1_metric: 0.8471\n",
      "Epoch 240/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.1842 - accuracy: 0.8759 - f1_metric: 0.8761\n",
      "Epoch 00240: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.1792 - accuracy: 0.8720 - f1_metric: 0.8724 - val_loss: 0.3703 - val_accuracy: 0.8332 - val_f1_metric: 0.8328\n",
      "Epoch 241/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1811 - accuracy: 0.8716 - f1_metric: 0.8716\n",
      "Epoch 00241: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 97us/sample - loss: 0.1796 - accuracy: 0.8749 - f1_metric: 0.8748 - val_loss: 0.3685 - val_accuracy: 0.8374 - val_f1_metric: 0.8373\n",
      "Epoch 242/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1764 - accuracy: 0.8621 - f1_metric: 0.8621\n",
      "Epoch 00242: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 94us/sample - loss: 0.1799 - accuracy: 0.8570 - f1_metric: 0.8565 - val_loss: 0.3681 - val_accuracy: 0.8374 - val_f1_metric: 0.8373\n",
      "Epoch 243/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.1780 - accuracy: 0.8664 - f1_metric: 0.8664\n",
      "Epoch 00243: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 90us/sample - loss: 0.1829 - accuracy: 0.8641 - f1_metric: 0.8636 - val_loss: 0.3524 - val_accuracy: 0.8432 - val_f1_metric: 0.8430\n",
      "Epoch 244/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1689 - accuracy: 0.8714 - f1_metric: 0.8714\n",
      "Epoch 00244: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 103us/sample - loss: 0.1748 - accuracy: 0.8681 - f1_metric: 0.8685 - val_loss: 0.3364 - val_accuracy: 0.8549 - val_f1_metric: 0.8548\n",
      "Epoch 245/3000\n",
      "2688/2797 [===========================>..] - ETA: 0s - loss: 0.1908 - accuracy: 0.8694 - f1_metric: 0.8694\n",
      "Epoch 00245: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 108us/sample - loss: 0.1901 - accuracy: 0.8677 - f1_metric: 0.8671 - val_loss: 0.3808 - val_accuracy: 0.8282 - val_f1_metric: 0.8285\n",
      "Epoch 246/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.2088 - accuracy: 0.8527 - f1_metric: 0.8527\n",
      "Epoch 00246: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 96us/sample - loss: 0.1978 - accuracy: 0.8559 - f1_metric: 0.8563 - val_loss: 0.3682 - val_accuracy: 0.8365 - val_f1_metric: 0.8364\n",
      "Epoch 247/3000\n",
      "2560/2797 [==========================>...] - ETA: 0s - loss: 0.1909 - accuracy: 0.8676 - f1_metric: 0.8676\n",
      "Epoch 00247: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 89us/sample - loss: 0.1908 - accuracy: 0.8677 - f1_metric: 0.8674 - val_loss: 0.3587 - val_accuracy: 0.8374 - val_f1_metric: 0.8373\n",
      "Epoch 248/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1742 - accuracy: 0.8746 - f1_metric: 0.8746\n",
      "Epoch 00248: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 95us/sample - loss: 0.1764 - accuracy: 0.8720 - f1_metric: 0.8712 - val_loss: 0.3562 - val_accuracy: 0.8440 - val_f1_metric: 0.8438\n",
      "Epoch 249/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.1805 - accuracy: 0.8755 - f1_metric: 0.8755\n",
      "Epoch 00249: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.1771 - accuracy: 0.8781 - f1_metric: 0.8783 - val_loss: 0.3579 - val_accuracy: 0.8449 - val_f1_metric: 0.8447\n",
      "Epoch 250/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1879 - accuracy: 0.8623 - f1_metric: 0.8623\n",
      "Epoch 00250: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 92us/sample - loss: 0.1911 - accuracy: 0.8641 - f1_metric: 0.8636 - val_loss: 0.3702 - val_accuracy: 0.8274 - val_f1_metric: 0.8274\n",
      "Epoch 251/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1772 - accuracy: 0.8666 - f1_metric: 0.8666\n",
      "Epoch 00251: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 90us/sample - loss: 0.1743 - accuracy: 0.8688 - f1_metric: 0.8694 - val_loss: 0.3480 - val_accuracy: 0.8440 - val_f1_metric: 0.8441\n",
      "Epoch 252/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1813 - accuracy: 0.8742 - f1_metric: 0.8742\n",
      "Epoch 00252: val_f1_metric did not improve from 0.86522\n",
      "2797/2797 [==============================] - 0s 95us/sample - loss: 0.1779 - accuracy: 0.8770 - f1_metric: 0.8769 - val_loss: 0.3695 - val_accuracy: 0.8349 - val_f1_metric: 0.8348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 253/3000\n",
      "2752/2797 [============================>.] - ETA: 0s - loss: 0.1771 - accuracy: 0.8714 - f1_metric: 0.8714\n",
      "Epoch 00253: val_f1_metric improved from 0.86522 to 0.87187, saving model to ./best_model_keras\n",
      "2797/2797 [==============================] - 1s 402us/sample - loss: 0.1758 - accuracy: 0.8716 - f1_metric: 0.8718 - val_loss: 0.3116 - val_accuracy: 0.8716 - val_f1_metric: 0.8719\n",
      "Epoch 254/3000\n",
      "2752/2797 [============================>.] - ETA: 0s - loss: 0.1780 - accuracy: 0.8801 - f1_metric: 0.8801\n",
      "Epoch 00254: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 105us/sample - loss: 0.1773 - accuracy: 0.8813 - f1_metric: 0.8818 - val_loss: 0.3639 - val_accuracy: 0.8407 - val_f1_metric: 0.8405\n",
      "Epoch 255/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1817 - accuracy: 0.8665 - f1_metric: 0.8665\n",
      "Epoch 00255: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 94us/sample - loss: 0.1813 - accuracy: 0.8645 - f1_metric: 0.8641 - val_loss: 0.3504 - val_accuracy: 0.8449 - val_f1_metric: 0.8447\n",
      "Epoch 256/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1600 - accuracy: 0.8860 - f1_metric: 0.8860\n",
      "Epoch 00256: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 94us/sample - loss: 0.1684 - accuracy: 0.8820 - f1_metric: 0.8819 - val_loss: 0.3486 - val_accuracy: 0.8415 - val_f1_metric: 0.8414\n",
      "Epoch 257/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1774 - accuracy: 0.8804 - f1_metric: 0.8804\n",
      "Epoch 00257: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 96us/sample - loss: 0.1768 - accuracy: 0.8734 - f1_metric: 0.8725 - val_loss: 0.3842 - val_accuracy: 0.8240 - val_f1_metric: 0.8241\n",
      "Epoch 258/3000\n",
      "2688/2797 [===========================>..] - ETA: 0s - loss: 0.1789 - accuracy: 0.8694 - f1_metric: 0.8694\n",
      "Epoch 00258: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 106us/sample - loss: 0.1779 - accuracy: 0.8670 - f1_metric: 0.8665 - val_loss: 0.3630 - val_accuracy: 0.8457 - val_f1_metric: 0.8455\n",
      "Epoch 259/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1621 - accuracy: 0.8893 - f1_metric: 0.8893\n",
      "Epoch 00259: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 99us/sample - loss: 0.1751 - accuracy: 0.8809 - f1_metric: 0.8804 - val_loss: 0.3590 - val_accuracy: 0.8424 - val_f1_metric: 0.8422\n",
      "Epoch 260/3000\n",
      "2752/2797 [============================>.] - ETA: 0s - loss: 0.1764 - accuracy: 0.8703 - f1_metric: 0.8703\n",
      "Epoch 00260: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 102us/sample - loss: 0.1754 - accuracy: 0.8706 - f1_metric: 0.8707 - val_loss: 0.3426 - val_accuracy: 0.8490 - val_f1_metric: 0.8491\n",
      "Epoch 261/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1747 - accuracy: 0.8708 - f1_metric: 0.8708\n",
      "Epoch 00261: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 94us/sample - loss: 0.1757 - accuracy: 0.8702 - f1_metric: 0.8699 - val_loss: 0.3672 - val_accuracy: 0.8365 - val_f1_metric: 0.8364\n",
      "Epoch 262/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1799 - accuracy: 0.8655 - f1_metric: 0.8655\n",
      "Epoch 00262: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 95us/sample - loss: 0.1840 - accuracy: 0.8656 - f1_metric: 0.8654 - val_loss: 0.3519 - val_accuracy: 0.8482 - val_f1_metric: 0.8482\n",
      "Epoch 263/3000\n",
      "2496/2797 [=========================>....] - ETA: 0s - loss: 0.1702 - accuracy: 0.8718 - f1_metric: 0.8718\n",
      "Epoch 00263: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 151us/sample - loss: 0.1737 - accuracy: 0.8738 - f1_metric: 0.8740 - val_loss: 0.3501 - val_accuracy: 0.8457 - val_f1_metric: 0.8455\n",
      "Epoch 264/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.1790 - accuracy: 0.8594 - f1_metric: 0.8594\n",
      "Epoch 00264: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 89us/sample - loss: 0.1786 - accuracy: 0.8627 - f1_metric: 0.8627 - val_loss: 0.3312 - val_accuracy: 0.8515 - val_f1_metric: 0.8515\n",
      "Epoch 265/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.1847 - accuracy: 0.8681 - f1_metric: 0.8678\n",
      "Epoch 00265: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 99us/sample - loss: 0.1856 - accuracy: 0.8699 - f1_metric: 0.8695 - val_loss: 0.3453 - val_accuracy: 0.8432 - val_f1_metric: 0.8433\n",
      "Epoch 266/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1803 - accuracy: 0.8746 - f1_metric: 0.8746\n",
      "Epoch 00266: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.1734 - accuracy: 0.8788 - f1_metric: 0.8792 - val_loss: 0.3677 - val_accuracy: 0.8332 - val_f1_metric: 0.8334\n",
      "Epoch 267/3000\n",
      "2112/2797 [=====================>........] - ETA: 0s - loss: 0.1710 - accuracy: 0.8864 - f1_metric: 0.8864\n",
      "Epoch 00267: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 99us/sample - loss: 0.1685 - accuracy: 0.8834 - f1_metric: 0.8838 - val_loss: 0.3541 - val_accuracy: 0.8390 - val_f1_metric: 0.8392\n",
      "Epoch 268/3000\n",
      "2048/2797 [====================>.........] - ETA: 0s - loss: 0.1705 - accuracy: 0.8828 - f1_metric: 0.8828\n",
      "Epoch 00268: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 102us/sample - loss: 0.1783 - accuracy: 0.8813 - f1_metric: 0.8814 - val_loss: 0.3657 - val_accuracy: 0.8390 - val_f1_metric: 0.8392\n",
      "Epoch 269/3000\n",
      "2112/2797 [=====================>........] - ETA: 0s - loss: 0.2016 - accuracy: 0.8551 - f1_metric: 0.8551\n",
      "Epoch 00269: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 99us/sample - loss: 0.1936 - accuracy: 0.8527 - f1_metric: 0.8525 - val_loss: 0.3560 - val_accuracy: 0.8407 - val_f1_metric: 0.8408\n",
      "Epoch 270/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1848 - accuracy: 0.8649 - f1_metric: 0.8649\n",
      "Epoch 00270: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 92us/sample - loss: 0.1883 - accuracy: 0.8631 - f1_metric: 0.8631 - val_loss: 0.3708 - val_accuracy: 0.8324 - val_f1_metric: 0.8326\n",
      "Epoch 271/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1691 - accuracy: 0.8666 - f1_metric: 0.8666\n",
      "Epoch 00271: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 94us/sample - loss: 0.1683 - accuracy: 0.8713 - f1_metric: 0.8714 - val_loss: 0.3168 - val_accuracy: 0.8590 - val_f1_metric: 0.8589\n",
      "Epoch 272/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1803 - accuracy: 0.8674 - f1_metric: 0.8674\n",
      "Epoch 00272: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 94us/sample - loss: 0.1752 - accuracy: 0.8702 - f1_metric: 0.8703 - val_loss: 0.3529 - val_accuracy: 0.8449 - val_f1_metric: 0.8450\n",
      "Epoch 273/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1690 - accuracy: 0.8777 - f1_metric: 0.8777\n",
      "Epoch 00273: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.1665 - accuracy: 0.8738 - f1_metric: 0.8737 - val_loss: 0.3388 - val_accuracy: 0.8490 - val_f1_metric: 0.8491\n",
      "Epoch 274/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1623 - accuracy: 0.8819 - f1_metric: 0.8819\n",
      "Epoch 00274: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 93us/sample - loss: 0.1625 - accuracy: 0.8870 - f1_metric: 0.8873 - val_loss: 0.3482 - val_accuracy: 0.8465 - val_f1_metric: 0.8463\n",
      "Epoch 275/3000\n",
      "2560/2797 [==========================>...] - ETA: 0s - loss: 0.1821 - accuracy: 0.8668 - f1_metric: 0.8666\n",
      "Epoch 00275: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 87us/sample - loss: 0.1805 - accuracy: 0.8670 - f1_metric: 0.8668 - val_loss: 0.3523 - val_accuracy: 0.8390 - val_f1_metric: 0.8389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 276/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1782 - accuracy: 0.8711 - f1_metric: 0.8711\n",
      "Epoch 00276: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 93us/sample - loss: 0.1755 - accuracy: 0.8706 - f1_metric: 0.8705 - val_loss: 0.3511 - val_accuracy: 0.8474 - val_f1_metric: 0.8471\n",
      "Epoch 277/3000\n",
      "2688/2797 [===========================>..] - ETA: 0s - loss: 0.1743 - accuracy: 0.8757 - f1_metric: 0.8757\n",
      "Epoch 00277: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 104us/sample - loss: 0.1738 - accuracy: 0.8749 - f1_metric: 0.8754 - val_loss: 0.3585 - val_accuracy: 0.8407 - val_f1_metric: 0.8405\n",
      "Epoch 278/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1706 - accuracy: 0.8733 - f1_metric: 0.8733\n",
      "Epoch 00278: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 99us/sample - loss: 0.1737 - accuracy: 0.8706 - f1_metric: 0.8708 - val_loss: 0.3296 - val_accuracy: 0.8540 - val_f1_metric: 0.8540\n",
      "Epoch 279/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1552 - accuracy: 0.8856 - f1_metric: 0.8856\n",
      "Epoch 00279: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 92us/sample - loss: 0.1614 - accuracy: 0.8838 - f1_metric: 0.8835 - val_loss: 0.3459 - val_accuracy: 0.8474 - val_f1_metric: 0.8474\n",
      "Epoch 280/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1797 - accuracy: 0.8728 - f1_metric: 0.8728\n",
      "Epoch 00280: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 95us/sample - loss: 0.1744 - accuracy: 0.8731 - f1_metric: 0.8732 - val_loss: 0.3637 - val_accuracy: 0.8324 - val_f1_metric: 0.8323\n",
      "Epoch 281/3000\n",
      "2752/2797 [============================>.] - ETA: 0s - loss: 0.1653 - accuracy: 0.8823 - f1_metric: 0.8823\n",
      "Epoch 00281: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 104us/sample - loss: 0.1643 - accuracy: 0.8827 - f1_metric: 0.8829 - val_loss: 0.3400 - val_accuracy: 0.8499 - val_f1_metric: 0.8499\n",
      "Epoch 282/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1832 - accuracy: 0.8828 - f1_metric: 0.8828\n",
      "Epoch 00282: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.1791 - accuracy: 0.8777 - f1_metric: 0.8774 - val_loss: 0.3797 - val_accuracy: 0.8274 - val_f1_metric: 0.8274\n",
      "Epoch 283/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.1905 - accuracy: 0.8713 - f1_metric: 0.8713\n",
      "Epoch 00283: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 103us/sample - loss: 0.1840 - accuracy: 0.8706 - f1_metric: 0.8705 - val_loss: 0.3650 - val_accuracy: 0.8365 - val_f1_metric: 0.8367\n",
      "Epoch 284/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1949 - accuracy: 0.8620 - f1_metric: 0.8620\n",
      "Epoch 00284: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.1868 - accuracy: 0.8616 - f1_metric: 0.8618 - val_loss: 0.3465 - val_accuracy: 0.8399 - val_f1_metric: 0.8400\n",
      "Epoch 285/3000\n",
      "2560/2797 [==========================>...] - ETA: 0s - loss: 0.1588 - accuracy: 0.8914 - f1_metric: 0.8914\n",
      "Epoch 00285: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 151us/sample - loss: 0.1598 - accuracy: 0.8852 - f1_metric: 0.8847 - val_loss: 0.3406 - val_accuracy: 0.8524 - val_f1_metric: 0.8521\n",
      "Epoch 286/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1622 - accuracy: 0.8877 - f1_metric: 0.8877\n",
      "Epoch 00286: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 92us/sample - loss: 0.1710 - accuracy: 0.8852 - f1_metric: 0.8851 - val_loss: 0.3406 - val_accuracy: 0.8490 - val_f1_metric: 0.8488\n",
      "Epoch 287/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1675 - accuracy: 0.8784 - f1_metric: 0.8784\n",
      "Epoch 00287: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 95us/sample - loss: 0.1670 - accuracy: 0.8799 - f1_metric: 0.8799 - val_loss: 0.3571 - val_accuracy: 0.8490 - val_f1_metric: 0.8488\n",
      "Epoch 288/3000\n",
      "2688/2797 [===========================>..] - ETA: 0s - loss: 0.1736 - accuracy: 0.8776 - f1_metric: 0.8776\n",
      "Epoch 00288: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 103us/sample - loss: 0.1722 - accuracy: 0.8763 - f1_metric: 0.8761 - val_loss: 0.3580 - val_accuracy: 0.8424 - val_f1_metric: 0.8425\n",
      "Epoch 289/3000\n",
      "2496/2797 [=========================>....] - ETA: 0s - loss: 0.1662 - accuracy: 0.8750 - f1_metric: 0.8750\n",
      "Epoch 00289: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 88us/sample - loss: 0.1640 - accuracy: 0.8774 - f1_metric: 0.8776 - val_loss: 0.3374 - val_accuracy: 0.8490 - val_f1_metric: 0.8491\n",
      "Epoch 290/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.1697 - accuracy: 0.8799 - f1_metric: 0.8799\n",
      "Epoch 00290: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 91us/sample - loss: 0.1736 - accuracy: 0.8788 - f1_metric: 0.8792 - val_loss: 0.3364 - val_accuracy: 0.8507 - val_f1_metric: 0.8507\n",
      "Epoch 291/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1856 - accuracy: 0.8623 - f1_metric: 0.8623\n",
      "Epoch 00291: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 96us/sample - loss: 0.1802 - accuracy: 0.8638 - f1_metric: 0.8643 - val_loss: 0.3361 - val_accuracy: 0.8499 - val_f1_metric: 0.8499\n",
      "Epoch 292/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.1772 - accuracy: 0.8801 - f1_metric: 0.8801\n",
      "Epoch 00292: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.1721 - accuracy: 0.8784 - f1_metric: 0.8782 - val_loss: 0.3660 - val_accuracy: 0.8324 - val_f1_metric: 0.8330\n",
      "Epoch 293/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.1624 - accuracy: 0.8758 - f1_metric: 0.8758\n",
      "Epoch 00293: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 90us/sample - loss: 0.1616 - accuracy: 0.8759 - f1_metric: 0.8756 - val_loss: 0.3185 - val_accuracy: 0.8657 - val_f1_metric: 0.8655\n",
      "Epoch 294/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.1652 - accuracy: 0.8890 - f1_metric: 0.8887\n",
      "Epoch 00294: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 92us/sample - loss: 0.1673 - accuracy: 0.8859 - f1_metric: 0.8856 - val_loss: 0.3597 - val_accuracy: 0.8440 - val_f1_metric: 0.8441\n",
      "Epoch 295/3000\n",
      "2752/2797 [============================>.] - ETA: 0s - loss: 0.1741 - accuracy: 0.8772 - f1_metric: 0.8772\n",
      "Epoch 00295: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 104us/sample - loss: 0.1761 - accuracy: 0.8759 - f1_metric: 0.8754 - val_loss: 0.3539 - val_accuracy: 0.8449 - val_f1_metric: 0.8450\n",
      "Epoch 296/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.1824 - accuracy: 0.8626 - f1_metric: 0.8626\n",
      "Epoch 00296: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 102us/sample - loss: 0.1874 - accuracy: 0.8638 - f1_metric: 0.8634 - val_loss: 0.3517 - val_accuracy: 0.8474 - val_f1_metric: 0.8474\n",
      "Epoch 297/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1699 - accuracy: 0.8720 - f1_metric: 0.8720\n",
      "Epoch 00297: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 93us/sample - loss: 0.1750 - accuracy: 0.8724 - f1_metric: 0.8726 - val_loss: 0.3403 - val_accuracy: 0.8490 - val_f1_metric: 0.8491\n",
      "Epoch 298/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1563 - accuracy: 0.8826 - f1_metric: 0.8826\n",
      "Epoch 00298: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 93us/sample - loss: 0.1556 - accuracy: 0.8842 - f1_metric: 0.8837 - val_loss: 0.3164 - val_accuracy: 0.8590 - val_f1_metric: 0.8589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1720 - accuracy: 0.8754 - f1_metric: 0.8754\n",
      "Epoch 00299: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 143us/sample - loss: 0.1675 - accuracy: 0.8795 - f1_metric: 0.8797 - val_loss: 0.3226 - val_accuracy: 0.8574 - val_f1_metric: 0.8573\n",
      "Epoch 300/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1778 - accuracy: 0.8775 - f1_metric: 0.8775\n",
      "Epoch 00300: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 95us/sample - loss: 0.1734 - accuracy: 0.8777 - f1_metric: 0.8775 - val_loss: 0.3515 - val_accuracy: 0.8474 - val_f1_metric: 0.8474\n",
      "Epoch 301/3000\n",
      "2496/2797 [=========================>....] - ETA: 0s - loss: 0.1699 - accuracy: 0.8742 - f1_metric: 0.8742\n",
      "Epoch 00301: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 161us/sample - loss: 0.1685 - accuracy: 0.8742 - f1_metric: 0.8741 - val_loss: 0.3353 - val_accuracy: 0.8449 - val_f1_metric: 0.8450\n",
      "Epoch 302/3000\n",
      "2624/2797 [===========================>..] - ETA: 0s - loss: 0.1813 - accuracy: 0.8769 - f1_metric: 0.8769\n",
      "Epoch 00302: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 170us/sample - loss: 0.1804 - accuracy: 0.8788 - f1_metric: 0.8790 - val_loss: 0.3539 - val_accuracy: 0.8415 - val_f1_metric: 0.8417\n",
      "Epoch 303/3000\n",
      "2560/2797 [==========================>...] - ETA: 0s - loss: 0.1611 - accuracy: 0.8836 - f1_metric: 0.8836\n",
      "Epoch 00303: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 109us/sample - loss: 0.1579 - accuracy: 0.8849 - f1_metric: 0.8848 - val_loss: 0.3460 - val_accuracy: 0.8490 - val_f1_metric: 0.8491\n",
      "Epoch 304/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.1612 - accuracy: 0.8836 - f1_metric: 0.8836\n",
      "Epoch 00304: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 113us/sample - loss: 0.1619 - accuracy: 0.8813 - f1_metric: 0.8808 - val_loss: 0.3480 - val_accuracy: 0.8465 - val_f1_metric: 0.8466\n",
      "Epoch 305/3000\n",
      "2688/2797 [===========================>..] - ETA: 0s - loss: 0.1784 - accuracy: 0.8743 - f1_metric: 0.8743\n",
      "Epoch 00305: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 108us/sample - loss: 0.1753 - accuracy: 0.8756 - f1_metric: 0.8755 - val_loss: 0.3489 - val_accuracy: 0.8449 - val_f1_metric: 0.8450\n",
      "Epoch 306/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1598 - accuracy: 0.8885 - f1_metric: 0.8885\n",
      "Epoch 00306: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 96us/sample - loss: 0.1616 - accuracy: 0.8834 - f1_metric: 0.8830 - val_loss: 0.3562 - val_accuracy: 0.8415 - val_f1_metric: 0.8417\n",
      "Epoch 307/3000\n",
      "2048/2797 [====================>.........] - ETA: 0s - loss: 0.1575 - accuracy: 0.8809 - f1_metric: 0.8809\n",
      "Epoch 00307: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 104us/sample - loss: 0.1650 - accuracy: 0.8827 - f1_metric: 0.8829 - val_loss: 0.3293 - val_accuracy: 0.8540 - val_f1_metric: 0.8540\n",
      "Epoch 308/3000\n",
      "2752/2797 [============================>.] - ETA: 0s - loss: 0.1732 - accuracy: 0.8808 - f1_metric: 0.8808\n",
      "Epoch 00308: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 103us/sample - loss: 0.1719 - accuracy: 0.8806 - f1_metric: 0.8805 - val_loss: 0.3715 - val_accuracy: 0.8415 - val_f1_metric: 0.8414\n",
      "Epoch 309/3000\n",
      "2752/2797 [============================>.] - ETA: 0s - loss: 0.1781 - accuracy: 0.8699 - f1_metric: 0.8699\n",
      "Epoch 00309: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 103us/sample - loss: 0.1787 - accuracy: 0.8695 - f1_metric: 0.8693 - val_loss: 0.3736 - val_accuracy: 0.8349 - val_f1_metric: 0.8348\n",
      "Epoch 310/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1628 - accuracy: 0.8759 - f1_metric: 0.8759\n",
      "Epoch 00310: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.1640 - accuracy: 0.8759 - f1_metric: 0.8760 - val_loss: 0.3440 - val_accuracy: 0.8507 - val_f1_metric: 0.8507\n",
      "Epoch 311/3000\n",
      "2688/2797 [===========================>..] - ETA: 0s - loss: 0.1765 - accuracy: 0.8739 - f1_metric: 0.8739\n",
      "Epoch 00311: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 112us/sample - loss: 0.1754 - accuracy: 0.8742 - f1_metric: 0.8743 - val_loss: 0.3521 - val_accuracy: 0.8424 - val_f1_metric: 0.8422\n",
      "Epoch 312/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1704 - accuracy: 0.8676 - f1_metric: 0.8676\n",
      "Epoch 00312: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 96us/sample - loss: 0.1655 - accuracy: 0.8709 - f1_metric: 0.8712 - val_loss: 0.3377 - val_accuracy: 0.8540 - val_f1_metric: 0.8540\n",
      "Epoch 313/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1771 - accuracy: 0.8777 - f1_metric: 0.8777\n",
      "Epoch 00313: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 95us/sample - loss: 0.1731 - accuracy: 0.8827 - f1_metric: 0.8822 - val_loss: 0.3466 - val_accuracy: 0.8482 - val_f1_metric: 0.8482\n",
      "Epoch 314/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1602 - accuracy: 0.8788 - f1_metric: 0.8788\n",
      "Epoch 00314: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 92us/sample - loss: 0.1629 - accuracy: 0.8817 - f1_metric: 0.8813 - val_loss: 0.3172 - val_accuracy: 0.8616 - val_f1_metric: 0.8614\n",
      "Epoch 315/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.1630 - accuracy: 0.8833 - f1_metric: 0.8833\n",
      "Epoch 00315: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 105us/sample - loss: 0.1623 - accuracy: 0.8806 - f1_metric: 0.8809 - val_loss: 0.3213 - val_accuracy: 0.8582 - val_f1_metric: 0.8581\n",
      "Epoch 316/3000\n",
      "2688/2797 [===========================>..] - ETA: 0s - loss: 0.1620 - accuracy: 0.8836 - f1_metric: 0.8836\n",
      "Epoch 00316: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 108us/sample - loss: 0.1611 - accuracy: 0.8838 - f1_metric: 0.8838 - val_loss: 0.3468 - val_accuracy: 0.8499 - val_f1_metric: 0.8499\n",
      "Epoch 317/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1563 - accuracy: 0.8847 - f1_metric: 0.8847\n",
      "Epoch 00317: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 92us/sample - loss: 0.1642 - accuracy: 0.8809 - f1_metric: 0.8805 - val_loss: 0.3478 - val_accuracy: 0.8565 - val_f1_metric: 0.8562\n",
      "Epoch 318/3000\n",
      "2560/2797 [==========================>...] - ETA: 0s - loss: 0.1634 - accuracy: 0.8816 - f1_metric: 0.8816\n",
      "Epoch 00318: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 141us/sample - loss: 0.1632 - accuracy: 0.8813 - f1_metric: 0.8818 - val_loss: 0.3358 - val_accuracy: 0.8549 - val_f1_metric: 0.8548\n",
      "Epoch 319/3000\n",
      "2688/2797 [===========================>..] - ETA: 0s - loss: 0.1648 - accuracy: 0.8824 - f1_metric: 0.8824\n",
      "Epoch 00319: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 164us/sample - loss: 0.1643 - accuracy: 0.8827 - f1_metric: 0.8828 - val_loss: 0.3277 - val_accuracy: 0.8590 - val_f1_metric: 0.8589\n",
      "Epoch 320/3000\n",
      "2496/2797 [=========================>....] - ETA: 0s - loss: 0.1617 - accuracy: 0.8866 - f1_metric: 0.8866\n",
      "Epoch 00320: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 110us/sample - loss: 0.1611 - accuracy: 0.8842 - f1_metric: 0.8843 - val_loss: 0.3402 - val_accuracy: 0.8499 - val_f1_metric: 0.8499\n",
      "Epoch 321/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1664 - accuracy: 0.8839 - f1_metric: 0.8839\n",
      "Epoch 00321: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 94us/sample - loss: 0.1609 - accuracy: 0.8870 - f1_metric: 0.8870 - val_loss: 0.3298 - val_accuracy: 0.8565 - val_f1_metric: 0.8565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 322/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.1669 - accuracy: 0.8842 - f1_metric: 0.8842\n",
      "Epoch 00322: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 100us/sample - loss: 0.1641 - accuracy: 0.8849 - f1_metric: 0.8854 - val_loss: 0.3646 - val_accuracy: 0.8399 - val_f1_metric: 0.8397\n",
      "Epoch 323/3000\n",
      "2624/2797 [===========================>..] - ETA: 0s - loss: 0.1707 - accuracy: 0.8800 - f1_metric: 0.8800\n",
      "Epoch 00323: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 109us/sample - loss: 0.1724 - accuracy: 0.8784 - f1_metric: 0.8785 - val_loss: 0.3448 - val_accuracy: 0.8515 - val_f1_metric: 0.8515\n",
      "Epoch 324/3000\n",
      "2560/2797 [==========================>...] - ETA: 0s - loss: 0.1632 - accuracy: 0.8711 - f1_metric: 0.8711\n",
      "Epoch 00324: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 112us/sample - loss: 0.1647 - accuracy: 0.8731 - f1_metric: 0.8733 - val_loss: 0.3100 - val_accuracy: 0.8649 - val_f1_metric: 0.8647\n",
      "Epoch 325/3000\n",
      "2496/2797 [=========================>....] - ETA: 0s - loss: 0.1592 - accuracy: 0.8886 - f1_metric: 0.8886\n",
      "Epoch 00325: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 91us/sample - loss: 0.1656 - accuracy: 0.8892 - f1_metric: 0.8892 - val_loss: 0.3304 - val_accuracy: 0.8565 - val_f1_metric: 0.8565\n",
      "Epoch 326/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1651 - accuracy: 0.8763 - f1_metric: 0.8763\n",
      "Epoch 00326: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 95us/sample - loss: 0.1662 - accuracy: 0.8763 - f1_metric: 0.8759 - val_loss: 0.3576 - val_accuracy: 0.8399 - val_f1_metric: 0.8400\n",
      "Epoch 327/3000\n",
      "2688/2797 [===========================>..] - ETA: 0s - loss: 0.1761 - accuracy: 0.8698 - f1_metric: 0.8698\n",
      "Epoch 00327: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 109us/sample - loss: 0.1727 - accuracy: 0.8716 - f1_metric: 0.8716 - val_loss: 0.3637 - val_accuracy: 0.8365 - val_f1_metric: 0.8367\n",
      "Epoch 328/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.1638 - accuracy: 0.8828 - f1_metric: 0.8828\n",
      "Epoch 00328: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 110us/sample - loss: 0.1628 - accuracy: 0.8813 - f1_metric: 0.8818 - val_loss: 0.3454 - val_accuracy: 0.8449 - val_f1_metric: 0.8450\n",
      "Epoch 329/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1539 - accuracy: 0.8848 - f1_metric: 0.8848\n",
      "Epoch 00329: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 100us/sample - loss: 0.1628 - accuracy: 0.8859 - f1_metric: 0.8861 - val_loss: 0.3364 - val_accuracy: 0.8540 - val_f1_metric: 0.8540\n",
      "Epoch 330/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.1640 - accuracy: 0.8773 - f1_metric: 0.8773\n",
      "Epoch 00330: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 104us/sample - loss: 0.1669 - accuracy: 0.8788 - f1_metric: 0.8786 - val_loss: 0.3244 - val_accuracy: 0.8641 - val_f1_metric: 0.8639\n",
      "Epoch 331/3000\n",
      "2688/2797 [===========================>..] - ETA: 0s - loss: 0.1664 - accuracy: 0.8824 - f1_metric: 0.8822\n",
      "Epoch 00331: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 166us/sample - loss: 0.1662 - accuracy: 0.8824 - f1_metric: 0.8824 - val_loss: 0.3458 - val_accuracy: 0.8515 - val_f1_metric: 0.8515\n",
      "Epoch 332/3000\n",
      "2688/2797 [===========================>..] - ETA: 0s - loss: 0.1705 - accuracy: 0.8783 - f1_metric: 0.8783\n",
      "Epoch 00332: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 104us/sample - loss: 0.1707 - accuracy: 0.8788 - f1_metric: 0.8784 - val_loss: 0.3468 - val_accuracy: 0.8424 - val_f1_metric: 0.8422\n",
      "Epoch 333/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1685 - accuracy: 0.8772 - f1_metric: 0.8772\n",
      "Epoch 00333: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 102us/sample - loss: 0.1648 - accuracy: 0.8767 - f1_metric: 0.8772 - val_loss: 0.3348 - val_accuracy: 0.8549 - val_f1_metric: 0.8548\n",
      "Epoch 334/3000\n",
      "2496/2797 [=========================>....] - ETA: 0s - loss: 0.1598 - accuracy: 0.8890 - f1_metric: 0.8890\n",
      "Epoch 00334: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 135us/sample - loss: 0.1633 - accuracy: 0.8874 - f1_metric: 0.8871 - val_loss: 0.3511 - val_accuracy: 0.8499 - val_f1_metric: 0.8499\n",
      "Epoch 335/3000\n",
      "2752/2797 [============================>.] - ETA: 0s - loss: 0.1646 - accuracy: 0.8772 - f1_metric: 0.8772\n",
      "Epoch 00335: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 105us/sample - loss: 0.1669 - accuracy: 0.8774 - f1_metric: 0.8774 - val_loss: 0.3310 - val_accuracy: 0.8557 - val_f1_metric: 0.8556\n",
      "Epoch 336/3000\n",
      "2496/2797 [=========================>....] - ETA: 0s - loss: 0.1568 - accuracy: 0.8830 - f1_metric: 0.8830\n",
      "Epoch 00336: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 93us/sample - loss: 0.1538 - accuracy: 0.8842 - f1_metric: 0.8842 - val_loss: 0.3342 - val_accuracy: 0.8565 - val_f1_metric: 0.8565\n",
      "Epoch 337/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1734 - accuracy: 0.8799 - f1_metric: 0.8799\n",
      "Epoch 00337: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 100us/sample - loss: 0.1703 - accuracy: 0.8759 - f1_metric: 0.8754 - val_loss: 0.3635 - val_accuracy: 0.8390 - val_f1_metric: 0.8389\n",
      "Epoch 338/3000\n",
      "2112/2797 [=====================>........] - ETA: 0s - loss: 0.1691 - accuracy: 0.8793 - f1_metric: 0.8793\n",
      "Epoch 00338: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.1642 - accuracy: 0.8792 - f1_metric: 0.8792 - val_loss: 0.3234 - val_accuracy: 0.8641 - val_f1_metric: 0.8639\n",
      "Epoch 339/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1655 - accuracy: 0.8876 - f1_metric: 0.8876\n",
      "Epoch 00339: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 101us/sample - loss: 0.1655 - accuracy: 0.8863 - f1_metric: 0.8862 - val_loss: 0.3739 - val_accuracy: 0.8415 - val_f1_metric: 0.8417\n",
      "Epoch 340/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.1564 - accuracy: 0.8734 - f1_metric: 0.8734\n",
      "Epoch 00340: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 99us/sample - loss: 0.1568 - accuracy: 0.8752 - f1_metric: 0.8750 - val_loss: 0.3101 - val_accuracy: 0.8682 - val_f1_metric: 0.8680\n",
      "Epoch 341/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.1530 - accuracy: 0.8851 - f1_metric: 0.8851\n",
      "Epoch 00341: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 101us/sample - loss: 0.1647 - accuracy: 0.8838 - f1_metric: 0.8843 - val_loss: 0.3449 - val_accuracy: 0.8507 - val_f1_metric: 0.8507\n",
      "Epoch 342/3000\n",
      "2048/2797 [====================>.........] - ETA: 0s - loss: 0.1603 - accuracy: 0.8745 - f1_metric: 0.8745\n",
      "Epoch 00342: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 100us/sample - loss: 0.1586 - accuracy: 0.8799 - f1_metric: 0.8804 - val_loss: 0.3204 - val_accuracy: 0.8624 - val_f1_metric: 0.8622\n",
      "Epoch 343/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1506 - accuracy: 0.8872 - f1_metric: 0.8872\n",
      "Epoch 00343: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 94us/sample - loss: 0.1507 - accuracy: 0.8867 - f1_metric: 0.8870 - val_loss: 0.3057 - val_accuracy: 0.8666 - val_f1_metric: 0.8660\n",
      "Epoch 344/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1475 - accuracy: 0.8946 - f1_metric: 0.8946\n",
      "Epoch 00344: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 103us/sample - loss: 0.1503 - accuracy: 0.8920 - f1_metric: 0.8925 - val_loss: 0.3333 - val_accuracy: 0.8590 - val_f1_metric: 0.8589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 345/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1492 - accuracy: 0.8950 - f1_metric: 0.8950\n",
      "Epoch 00345: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 95us/sample - loss: 0.1540 - accuracy: 0.8920 - f1_metric: 0.8917 - val_loss: 0.3262 - val_accuracy: 0.8590 - val_f1_metric: 0.8589\n",
      "Epoch 346/3000\n",
      "2496/2797 [=========================>....] - ETA: 0s - loss: 0.1656 - accuracy: 0.8866 - f1_metric: 0.8866\n",
      "Epoch 00346: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 90us/sample - loss: 0.1693 - accuracy: 0.8842 - f1_metric: 0.8842 - val_loss: 0.3557 - val_accuracy: 0.8507 - val_f1_metric: 0.8507\n",
      "Epoch 347/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1493 - accuracy: 0.8763 - f1_metric: 0.8763\n",
      "Epoch 00347: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 97us/sample - loss: 0.1531 - accuracy: 0.8834 - f1_metric: 0.8836 - val_loss: 0.3042 - val_accuracy: 0.8716 - val_f1_metric: 0.8713\n",
      "Epoch 348/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1622 - accuracy: 0.8806 - f1_metric: 0.8806\n",
      "Epoch 00348: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.1633 - accuracy: 0.8834 - f1_metric: 0.8835 - val_loss: 0.3447 - val_accuracy: 0.8524 - val_f1_metric: 0.8524\n",
      "Epoch 349/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.1462 - accuracy: 0.8865 - f1_metric: 0.8865\n",
      "Epoch 00349: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 92us/sample - loss: 0.1546 - accuracy: 0.8852 - f1_metric: 0.8844 - val_loss: 0.3295 - val_accuracy: 0.8624 - val_f1_metric: 0.8622\n",
      "Epoch 350/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1639 - accuracy: 0.8819 - f1_metric: 0.8819\n",
      "Epoch 00350: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 95us/sample - loss: 0.1618 - accuracy: 0.8838 - f1_metric: 0.8838 - val_loss: 0.3239 - val_accuracy: 0.8607 - val_f1_metric: 0.8606\n",
      "Epoch 351/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1610 - accuracy: 0.8911 - f1_metric: 0.8911\n",
      "Epoch 00351: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 93us/sample - loss: 0.1581 - accuracy: 0.8924 - f1_metric: 0.8927 - val_loss: 0.3509 - val_accuracy: 0.8482 - val_f1_metric: 0.8482\n",
      "Epoch 352/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1692 - accuracy: 0.8759 - f1_metric: 0.8759\n",
      "Epoch 00352: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.1613 - accuracy: 0.8809 - f1_metric: 0.8810 - val_loss: 0.3201 - val_accuracy: 0.8599 - val_f1_metric: 0.8598\n",
      "Epoch 353/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1519 - accuracy: 0.8940 - f1_metric: 0.8940\n",
      "Epoch 00353: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 95us/sample - loss: 0.1519 - accuracy: 0.8956 - f1_metric: 0.8954 - val_loss: 0.3201 - val_accuracy: 0.8599 - val_f1_metric: 0.8593\n",
      "Epoch 354/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1769 - accuracy: 0.8725 - f1_metric: 0.8725\n",
      "Epoch 00354: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 93us/sample - loss: 0.1752 - accuracy: 0.8727 - f1_metric: 0.8736 - val_loss: 0.3513 - val_accuracy: 0.8507 - val_f1_metric: 0.8507\n",
      "Epoch 355/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.1538 - accuracy: 0.8836 - f1_metric: 0.8836\n",
      "Epoch 00355: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 92us/sample - loss: 0.1643 - accuracy: 0.8827 - f1_metric: 0.8822 - val_loss: 0.3264 - val_accuracy: 0.8599 - val_f1_metric: 0.8598\n",
      "Epoch 356/3000\n",
      "2752/2797 [============================>.] - ETA: 0s - loss: 0.1775 - accuracy: 0.8775 - f1_metric: 0.8775\n",
      "Epoch 00356: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 106us/sample - loss: 0.1771 - accuracy: 0.8777 - f1_metric: 0.8778 - val_loss: 0.3309 - val_accuracy: 0.8616 - val_f1_metric: 0.8614\n",
      "Epoch 357/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.1582 - accuracy: 0.8820 - f1_metric: 0.8820\n",
      "Epoch 00357: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 90us/sample - loss: 0.1599 - accuracy: 0.8852 - f1_metric: 0.8853 - val_loss: 0.3518 - val_accuracy: 0.8474 - val_f1_metric: 0.8474\n",
      "Epoch 358/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1599 - accuracy: 0.8750 - f1_metric: 0.8750\n",
      "Epoch 00358: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 95us/sample - loss: 0.1640 - accuracy: 0.8756 - f1_metric: 0.8763 - val_loss: 0.3185 - val_accuracy: 0.8657 - val_f1_metric: 0.8655\n",
      "Epoch 359/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1679 - accuracy: 0.8792 - f1_metric: 0.8792\n",
      "Epoch 00359: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 93us/sample - loss: 0.1691 - accuracy: 0.8759 - f1_metric: 0.8762 - val_loss: 0.3757 - val_accuracy: 0.8399 - val_f1_metric: 0.8400\n",
      "Epoch 360/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.1663 - accuracy: 0.8704 - f1_metric: 0.8704\n",
      "Epoch 00360: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 97us/sample - loss: 0.1695 - accuracy: 0.8720 - f1_metric: 0.8715 - val_loss: 0.3259 - val_accuracy: 0.8607 - val_f1_metric: 0.8606\n",
      "Epoch 361/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1685 - accuracy: 0.8788 - f1_metric: 0.8788\n",
      "Epoch 00361: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 91us/sample - loss: 0.1630 - accuracy: 0.8824 - f1_metric: 0.8826 - val_loss: 0.3283 - val_accuracy: 0.8616 - val_f1_metric: 0.8614\n",
      "Epoch 362/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1669 - accuracy: 0.8830 - f1_metric: 0.8830\n",
      "Epoch 00362: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.1632 - accuracy: 0.8827 - f1_metric: 0.8829 - val_loss: 0.3509 - val_accuracy: 0.8515 - val_f1_metric: 0.8515\n",
      "Epoch 363/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1634 - accuracy: 0.8790 - f1_metric: 0.8790\n",
      "Epoch 00363: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 99us/sample - loss: 0.1656 - accuracy: 0.8795 - f1_metric: 0.8790 - val_loss: 0.3405 - val_accuracy: 0.8540 - val_f1_metric: 0.8540\n",
      "Epoch 364/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.1568 - accuracy: 0.8796 - f1_metric: 0.8796\n",
      "Epoch 00364: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 99us/sample - loss: 0.1598 - accuracy: 0.8845 - f1_metric: 0.8844 - val_loss: 0.3631 - val_accuracy: 0.8457 - val_f1_metric: 0.8458\n",
      "Epoch 365/3000\n",
      "2496/2797 [=========================>....] - ETA: 0s - loss: 0.1624 - accuracy: 0.8834 - f1_metric: 0.8834\n",
      "Epoch 00365: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 91us/sample - loss: 0.1621 - accuracy: 0.8813 - f1_metric: 0.8814 - val_loss: 0.3546 - val_accuracy: 0.8432 - val_f1_metric: 0.8433\n",
      "Epoch 366/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.1558 - accuracy: 0.8923 - f1_metric: 0.8923\n",
      "Epoch 00366: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 89us/sample - loss: 0.1579 - accuracy: 0.8902 - f1_metric: 0.8901 - val_loss: 0.3289 - val_accuracy: 0.8565 - val_f1_metric: 0.8565\n",
      "Epoch 367/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1794 - accuracy: 0.8724 - f1_metric: 0.8724\n",
      "Epoch 00367: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 156us/sample - loss: 0.1719 - accuracy: 0.8738 - f1_metric: 0.8740 - val_loss: 0.3389 - val_accuracy: 0.8532 - val_f1_metric: 0.8532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 368/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.1631 - accuracy: 0.8836 - f1_metric: 0.8836\n",
      "Epoch 00368: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 90us/sample - loss: 0.1630 - accuracy: 0.8834 - f1_metric: 0.8838 - val_loss: 0.3268 - val_accuracy: 0.8599 - val_f1_metric: 0.8598\n",
      "Epoch 369/3000\n",
      "2496/2797 [=========================>....] - ETA: 0s - loss: 0.1579 - accuracy: 0.8946 - f1_metric: 0.8946\n",
      "Epoch 00369: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 89us/sample - loss: 0.1572 - accuracy: 0.8956 - f1_metric: 0.8956 - val_loss: 0.3595 - val_accuracy: 0.8407 - val_f1_metric: 0.8408\n",
      "Epoch 370/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1586 - accuracy: 0.8793 - f1_metric: 0.8793\n",
      "Epoch 00370: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 102us/sample - loss: 0.1537 - accuracy: 0.8781 - f1_metric: 0.8782 - val_loss: 0.3271 - val_accuracy: 0.8599 - val_f1_metric: 0.8598\n",
      "Epoch 371/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.1652 - accuracy: 0.8902 - f1_metric: 0.8902\n",
      "Epoch 00371: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 102us/sample - loss: 0.1720 - accuracy: 0.8824 - f1_metric: 0.8823 - val_loss: 0.3853 - val_accuracy: 0.8299 - val_f1_metric: 0.8302\n",
      "Epoch 372/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1658 - accuracy: 0.8750 - f1_metric: 0.8750\n",
      "Epoch 00372: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 94us/sample - loss: 0.1630 - accuracy: 0.8759 - f1_metric: 0.8748 - val_loss: 0.3440 - val_accuracy: 0.8532 - val_f1_metric: 0.8532\n",
      "Epoch 373/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.1456 - accuracy: 0.9001 - f1_metric: 0.9001\n",
      "Epoch 00373: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 90us/sample - loss: 0.1452 - accuracy: 0.8970 - f1_metric: 0.8965 - val_loss: 0.3317 - val_accuracy: 0.8590 - val_f1_metric: 0.8589\n",
      "Epoch 374/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.1641 - accuracy: 0.8851 - f1_metric: 0.8851\n",
      "Epoch 00374: val_f1_metric did not improve from 0.87187\n",
      "2797/2797 [==============================] - 0s 96us/sample - loss: 0.1610 - accuracy: 0.8827 - f1_metric: 0.8823 - val_loss: 0.3520 - val_accuracy: 0.8524 - val_f1_metric: 0.8521\n",
      "Epoch 375/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1486 - accuracy: 0.8847 - f1_metric: 0.8847\n",
      "Epoch 00375: val_f1_metric improved from 0.87187 to 0.87815, saving model to ./best_model_keras\n",
      "2797/2797 [==============================] - 1s 390us/sample - loss: 0.1470 - accuracy: 0.8877 - f1_metric: 0.8879 - val_loss: 0.2901 - val_accuracy: 0.8782 - val_f1_metric: 0.8781\n",
      "Epoch 376/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1657 - accuracy: 0.8845 - f1_metric: 0.8845\n",
      "Epoch 00376: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 99us/sample - loss: 0.1609 - accuracy: 0.8820 - f1_metric: 0.8816 - val_loss: 0.3272 - val_accuracy: 0.8607 - val_f1_metric: 0.8606\n",
      "Epoch 377/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.1425 - accuracy: 0.8975 - f1_metric: 0.8975\n",
      "Epoch 00377: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 100us/sample - loss: 0.1517 - accuracy: 0.8935 - f1_metric: 0.8930 - val_loss: 0.3332 - val_accuracy: 0.8574 - val_f1_metric: 0.8573\n",
      "Epoch 378/3000\n",
      "2112/2797 [=====================>........] - ETA: 0s - loss: 0.1712 - accuracy: 0.8807 - f1_metric: 0.8807\n",
      "Epoch 00378: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 101us/sample - loss: 0.1626 - accuracy: 0.8859 - f1_metric: 0.8863 - val_loss: 0.3347 - val_accuracy: 0.8565 - val_f1_metric: 0.8565\n",
      "Epoch 379/3000\n",
      "2112/2797 [=====================>........] - ETA: 0s - loss: 0.1558 - accuracy: 0.8887 - f1_metric: 0.8887\n",
      "Epoch 00379: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.1545 - accuracy: 0.8906 - f1_metric: 0.8907 - val_loss: 0.3276 - val_accuracy: 0.8624 - val_f1_metric: 0.8622\n",
      "Epoch 380/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1687 - accuracy: 0.8817 - f1_metric: 0.8817\n",
      "Epoch 00380: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 100us/sample - loss: 0.1672 - accuracy: 0.8824 - f1_metric: 0.8826 - val_loss: 0.3336 - val_accuracy: 0.8540 - val_f1_metric: 0.8540\n",
      "Epoch 381/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.1490 - accuracy: 0.8898 - f1_metric: 0.8898\n",
      "Epoch 00381: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 94us/sample - loss: 0.1506 - accuracy: 0.8931 - f1_metric: 0.8934 - val_loss: 0.3019 - val_accuracy: 0.8757 - val_f1_metric: 0.8754\n",
      "Epoch 382/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1527 - accuracy: 0.8906 - f1_metric: 0.8906\n",
      "Epoch 00382: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 97us/sample - loss: 0.1508 - accuracy: 0.8927 - f1_metric: 0.8927 - val_loss: 0.3201 - val_accuracy: 0.8666 - val_f1_metric: 0.8663\n",
      "Epoch 383/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1584 - accuracy: 0.8844 - f1_metric: 0.8844\n",
      "Epoch 00383: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 99us/sample - loss: 0.1561 - accuracy: 0.8831 - f1_metric: 0.8825 - val_loss: 0.3166 - val_accuracy: 0.8649 - val_f1_metric: 0.8647\n",
      "Epoch 384/3000\n",
      "2112/2797 [=====================>........] - ETA: 0s - loss: 0.1567 - accuracy: 0.8939 - f1_metric: 0.8939\n",
      "Epoch 00384: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 102us/sample - loss: 0.1723 - accuracy: 0.8913 - f1_metric: 0.8916 - val_loss: 0.3380 - val_accuracy: 0.8549 - val_f1_metric: 0.8548\n",
      "Epoch 385/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1484 - accuracy: 0.8864 - f1_metric: 0.8864\n",
      "Epoch 00385: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 99us/sample - loss: 0.1522 - accuracy: 0.8831 - f1_metric: 0.8834 - val_loss: 0.3408 - val_accuracy: 0.8507 - val_f1_metric: 0.8507\n",
      "Epoch 386/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1380 - accuracy: 0.8976 - f1_metric: 0.8976\n",
      "Epoch 00386: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 100us/sample - loss: 0.1408 - accuracy: 0.8931 - f1_metric: 0.8929 - val_loss: 0.3179 - val_accuracy: 0.8632 - val_f1_metric: 0.8630\n",
      "Epoch 387/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1627 - accuracy: 0.8950 - f1_metric: 0.8950\n",
      "Epoch 00387: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 101us/sample - loss: 0.1597 - accuracy: 0.8895 - f1_metric: 0.8897 - val_loss: 0.3640 - val_accuracy: 0.8457 - val_f1_metric: 0.8458\n",
      "Epoch 388/3000\n",
      "2624/2797 [===========================>..] - ETA: 0s - loss: 0.1537 - accuracy: 0.8899 - f1_metric: 0.8899\n",
      "Epoch 00388: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 107us/sample - loss: 0.1526 - accuracy: 0.8888 - f1_metric: 0.8887 - val_loss: 0.3441 - val_accuracy: 0.8540 - val_f1_metric: 0.8540\n",
      "Epoch 389/3000\n",
      "2688/2797 [===========================>..] - ETA: 0s - loss: 0.1492 - accuracy: 0.8943 - f1_metric: 0.8943\n",
      "Epoch 00389: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 106us/sample - loss: 0.1468 - accuracy: 0.8952 - f1_metric: 0.8952 - val_loss: 0.3567 - val_accuracy: 0.8482 - val_f1_metric: 0.8479\n",
      "Epoch 390/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1546 - accuracy: 0.8844 - f1_metric: 0.8844\n",
      "Epoch 00390: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 97us/sample - loss: 0.1545 - accuracy: 0.8849 - f1_metric: 0.8848 - val_loss: 0.3447 - val_accuracy: 0.8515 - val_f1_metric: 0.8515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 391/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1381 - accuracy: 0.8929 - f1_metric: 0.8929\n",
      "Epoch 00391: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 102us/sample - loss: 0.1477 - accuracy: 0.8885 - f1_metric: 0.8883 - val_loss: 0.3294 - val_accuracy: 0.8641 - val_f1_metric: 0.8639\n",
      "Epoch 392/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1388 - accuracy: 0.8941 - f1_metric: 0.8941\n",
      "Epoch 00392: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 94us/sample - loss: 0.1398 - accuracy: 0.8967 - f1_metric: 0.8962 - val_loss: 0.3114 - val_accuracy: 0.8707 - val_f1_metric: 0.8705\n",
      "Epoch 393/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.1616 - accuracy: 0.8856 - f1_metric: 0.8856\n",
      "Epoch 00393: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 99us/sample - loss: 0.1529 - accuracy: 0.8849 - f1_metric: 0.8849 - val_loss: 0.3503 - val_accuracy: 0.8524 - val_f1_metric: 0.8524\n",
      "Epoch 394/3000\n",
      "2496/2797 [=========================>....] - ETA: 0s - loss: 0.1579 - accuracy: 0.8938 - f1_metric: 0.8938\n",
      "Epoch 00394: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 89us/sample - loss: 0.1580 - accuracy: 0.8931 - f1_metric: 0.8931 - val_loss: 0.3419 - val_accuracy: 0.8515 - val_f1_metric: 0.8515\n",
      "Epoch 395/3000\n",
      "2624/2797 [===========================>..] - ETA: 0s - loss: 0.1478 - accuracy: 0.8861 - f1_metric: 0.8861\n",
      "Epoch 00395: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 108us/sample - loss: 0.1507 - accuracy: 0.8852 - f1_metric: 0.8854 - val_loss: 0.3112 - val_accuracy: 0.8716 - val_f1_metric: 0.8713\n",
      "Epoch 396/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.1622 - accuracy: 0.8874 - f1_metric: 0.8874\n",
      "Epoch 00396: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.1637 - accuracy: 0.8824 - f1_metric: 0.8827 - val_loss: 0.3643 - val_accuracy: 0.8440 - val_f1_metric: 0.8441\n",
      "Epoch 397/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.1580 - accuracy: 0.8828 - f1_metric: 0.8828\n",
      "Epoch 00397: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 101us/sample - loss: 0.1587 - accuracy: 0.8784 - f1_metric: 0.8782 - val_loss: 0.3328 - val_accuracy: 0.8557 - val_f1_metric: 0.8556\n",
      "Epoch 398/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1722 - accuracy: 0.8813 - f1_metric: 0.8812\n",
      "Epoch 00398: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 97us/sample - loss: 0.1670 - accuracy: 0.8781 - f1_metric: 0.8780 - val_loss: 0.3147 - val_accuracy: 0.8632 - val_f1_metric: 0.8630\n",
      "Epoch 399/3000\n",
      "2752/2797 [============================>.] - ETA: 0s - loss: 0.1529 - accuracy: 0.8917 - f1_metric: 0.8917\n",
      "Epoch 00399: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 146us/sample - loss: 0.1526 - accuracy: 0.8913 - f1_metric: 0.8911 - val_loss: 0.3591 - val_accuracy: 0.8474 - val_f1_metric: 0.8474\n",
      "Epoch 400/3000\n",
      "2112/2797 [=====================>........] - ETA: 0s - loss: 0.1516 - accuracy: 0.8797 - f1_metric: 0.8797\n",
      "Epoch 00400: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 102us/sample - loss: 0.1523 - accuracy: 0.8813 - f1_metric: 0.8814 - val_loss: 0.3064 - val_accuracy: 0.8741 - val_f1_metric: 0.8737\n",
      "Epoch 401/3000\n",
      "2624/2797 [===========================>..] - ETA: 0s - loss: 0.1388 - accuracy: 0.9032 - f1_metric: 0.9032\n",
      "Epoch 00401: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 108us/sample - loss: 0.1414 - accuracy: 0.9028 - f1_metric: 0.9024 - val_loss: 0.3550 - val_accuracy: 0.8499 - val_f1_metric: 0.8499\n",
      "Epoch 402/3000\n",
      "2624/2797 [===========================>..] - ETA: 0s - loss: 0.1480 - accuracy: 0.8838 - f1_metric: 0.8838\n",
      "Epoch 00402: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 108us/sample - loss: 0.1491 - accuracy: 0.8859 - f1_metric: 0.8863 - val_loss: 0.3209 - val_accuracy: 0.8649 - val_f1_metric: 0.8647\n",
      "Epoch 403/3000\n",
      "2752/2797 [============================>.] - ETA: 0s - loss: 0.1534 - accuracy: 0.8852 - f1_metric: 0.8852\n",
      "Epoch 00403: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 105us/sample - loss: 0.1539 - accuracy: 0.8856 - f1_metric: 0.8858 - val_loss: 0.3311 - val_accuracy: 0.8624 - val_f1_metric: 0.8622\n",
      "Epoch 404/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1485 - accuracy: 0.8971 - f1_metric: 0.8971\n",
      "Epoch 00404: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 95us/sample - loss: 0.1514 - accuracy: 0.8949 - f1_metric: 0.8948 - val_loss: 0.3805 - val_accuracy: 0.8399 - val_f1_metric: 0.8397\n",
      "Epoch 405/3000\n",
      "2624/2797 [===========================>..] - ETA: 0s - loss: 0.1566 - accuracy: 0.8826 - f1_metric: 0.8826\n",
      "Epoch 00405: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 110us/sample - loss: 0.1553 - accuracy: 0.8838 - f1_metric: 0.8838 - val_loss: 0.3195 - val_accuracy: 0.8649 - val_f1_metric: 0.8647\n",
      "Epoch 406/3000\n",
      "2112/2797 [=====================>........] - ETA: 0s - loss: 0.1402 - accuracy: 0.9010 - f1_metric: 0.9010\n",
      "Epoch 00406: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 105us/sample - loss: 0.1477 - accuracy: 0.8952 - f1_metric: 0.8945 - val_loss: 0.3347 - val_accuracy: 0.8582 - val_f1_metric: 0.8581\n",
      "Epoch 407/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1499 - accuracy: 0.8879 - f1_metric: 0.8879\n",
      "Epoch 00407: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.1468 - accuracy: 0.8892 - f1_metric: 0.8890 - val_loss: 0.3173 - val_accuracy: 0.8691 - val_f1_metric: 0.8685\n",
      "Epoch 408/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.1570 - accuracy: 0.8856 - f1_metric: 0.8856\n",
      "Epoch 00408: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 106us/sample - loss: 0.1575 - accuracy: 0.8881 - f1_metric: 0.8879 - val_loss: 0.3337 - val_accuracy: 0.8599 - val_f1_metric: 0.8598\n",
      "Epoch 409/3000\n",
      "2496/2797 [=========================>....] - ETA: 0s - loss: 0.1425 - accuracy: 0.8990 - f1_metric: 0.8990\n",
      "Epoch 00409: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 111us/sample - loss: 0.1428 - accuracy: 0.8963 - f1_metric: 0.8966 - val_loss: 0.3458 - val_accuracy: 0.8532 - val_f1_metric: 0.8532\n",
      "Epoch 410/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1344 - accuracy: 0.9033 - f1_metric: 0.9033\n",
      "Epoch 00410: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.1358 - accuracy: 0.8999 - f1_metric: 0.8994 - val_loss: 0.2982 - val_accuracy: 0.8757 - val_f1_metric: 0.8751\n",
      "Epoch 411/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1389 - accuracy: 0.8997 - f1_metric: 0.8997\n",
      "Epoch 00411: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 99us/sample - loss: 0.1363 - accuracy: 0.9024 - f1_metric: 0.9028 - val_loss: 0.3141 - val_accuracy: 0.8691 - val_f1_metric: 0.8685\n",
      "Epoch 412/3000\n",
      "2752/2797 [============================>.] - ETA: 0s - loss: 0.1373 - accuracy: 0.8964 - f1_metric: 0.8964\n",
      "Epoch 00412: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 107us/sample - loss: 0.1390 - accuracy: 0.8960 - f1_metric: 0.8958 - val_loss: 0.3129 - val_accuracy: 0.8724 - val_f1_metric: 0.8718\n",
      "Epoch 413/3000\n",
      "2624/2797 [===========================>..] - ETA: 0s - loss: 0.1443 - accuracy: 0.8948 - f1_metric: 0.8948\n",
      "Epoch 00413: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 110us/sample - loss: 0.1438 - accuracy: 0.8952 - f1_metric: 0.8951 - val_loss: 0.3467 - val_accuracy: 0.8540 - val_f1_metric: 0.8540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 414/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.1456 - accuracy: 0.8961 - f1_metric: 0.8961\n",
      "Epoch 00414: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 101us/sample - loss: 0.1430 - accuracy: 0.8970 - f1_metric: 0.8971 - val_loss: 0.3081 - val_accuracy: 0.8757 - val_f1_metric: 0.8754\n",
      "Epoch 415/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1683 - accuracy: 0.8801 - f1_metric: 0.8801\n",
      "Epoch 00415: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 94us/sample - loss: 0.1615 - accuracy: 0.8834 - f1_metric: 0.8839 - val_loss: 0.3065 - val_accuracy: 0.8707 - val_f1_metric: 0.8705\n",
      "Epoch 416/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1468 - accuracy: 0.8970 - f1_metric: 0.8970\n",
      "Epoch 00416: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 97us/sample - loss: 0.1526 - accuracy: 0.8985 - f1_metric: 0.8981 - val_loss: 0.3325 - val_accuracy: 0.8599 - val_f1_metric: 0.8598\n",
      "Epoch 417/3000\n",
      "2560/2797 [==========================>...] - ETA: 0s - loss: 0.1562 - accuracy: 0.8895 - f1_metric: 0.8895\n",
      "Epoch 00417: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 113us/sample - loss: 0.1541 - accuracy: 0.8899 - f1_metric: 0.8905 - val_loss: 0.3193 - val_accuracy: 0.8632 - val_f1_metric: 0.8630\n",
      "Epoch 418/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1485 - accuracy: 0.8924 - f1_metric: 0.8924\n",
      "Epoch 00418: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 96us/sample - loss: 0.1475 - accuracy: 0.8920 - f1_metric: 0.8923 - val_loss: 0.3339 - val_accuracy: 0.8574 - val_f1_metric: 0.8573\n",
      "Epoch 419/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1441 - accuracy: 0.8948 - f1_metric: 0.8948\n",
      "Epoch 00419: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 96us/sample - loss: 0.1446 - accuracy: 0.9006 - f1_metric: 0.9008 - val_loss: 0.3259 - val_accuracy: 0.8641 - val_f1_metric: 0.8639\n",
      "Epoch 420/3000\n",
      "2752/2797 [============================>.] - ETA: 0s - loss: 0.1515 - accuracy: 0.8783 - f1_metric: 0.8783\n",
      "Epoch 00420: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 105us/sample - loss: 0.1525 - accuracy: 0.8774 - f1_metric: 0.8770 - val_loss: 0.3381 - val_accuracy: 0.8582 - val_f1_metric: 0.8581\n",
      "Epoch 421/3000\n",
      "2112/2797 [=====================>........] - ETA: 0s - loss: 0.1482 - accuracy: 0.8935 - f1_metric: 0.8935\n",
      "Epoch 00421: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 97us/sample - loss: 0.1457 - accuracy: 0.8974 - f1_metric: 0.8976 - val_loss: 0.3299 - val_accuracy: 0.8616 - val_f1_metric: 0.8614\n",
      "Epoch 422/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.1465 - accuracy: 0.8869 - f1_metric: 0.8869\n",
      "Epoch 00422: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 91us/sample - loss: 0.1426 - accuracy: 0.8899 - f1_metric: 0.8902 - val_loss: 0.3051 - val_accuracy: 0.8741 - val_f1_metric: 0.8737\n",
      "Epoch 423/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.1389 - accuracy: 0.9040 - f1_metric: 0.9040\n",
      "Epoch 00423: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 101us/sample - loss: 0.1433 - accuracy: 0.8974 - f1_metric: 0.8972 - val_loss: 0.3168 - val_accuracy: 0.8691 - val_f1_metric: 0.8688\n",
      "Epoch 424/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1378 - accuracy: 0.9022 - f1_metric: 0.9022\n",
      "Epoch 00424: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 97us/sample - loss: 0.1355 - accuracy: 0.9042 - f1_metric: 0.9038 - val_loss: 0.3178 - val_accuracy: 0.8707 - val_f1_metric: 0.8707\n",
      "Epoch 425/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1586 - accuracy: 0.9027 - f1_metric: 0.9027\n",
      "Epoch 00425: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 97us/sample - loss: 0.1606 - accuracy: 0.8970 - f1_metric: 0.8967 - val_loss: 0.3506 - val_accuracy: 0.8557 - val_f1_metric: 0.8556\n",
      "Epoch 426/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1377 - accuracy: 0.8919 - f1_metric: 0.8919\n",
      "Epoch 00426: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 97us/sample - loss: 0.1387 - accuracy: 0.8927 - f1_metric: 0.8929 - val_loss: 0.3219 - val_accuracy: 0.8699 - val_f1_metric: 0.8696\n",
      "Epoch 427/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.1482 - accuracy: 0.8971 - f1_metric: 0.8971\n",
      "Epoch 00427: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 101us/sample - loss: 0.1427 - accuracy: 0.8974 - f1_metric: 0.8970 - val_loss: 0.3374 - val_accuracy: 0.8599 - val_f1_metric: 0.8598\n",
      "Epoch 428/3000\n",
      "2688/2797 [===========================>..] - ETA: 0s - loss: 0.1527 - accuracy: 0.8940 - f1_metric: 0.8940\n",
      "Epoch 00428: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 106us/sample - loss: 0.1517 - accuracy: 0.8924 - f1_metric: 0.8913 - val_loss: 0.3488 - val_accuracy: 0.8499 - val_f1_metric: 0.8499\n",
      "Epoch 429/3000\n",
      "2752/2797 [============================>.] - ETA: 0s - loss: 0.1473 - accuracy: 0.8946 - f1_metric: 0.8946\n",
      "Epoch 00429: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 103us/sample - loss: 0.1463 - accuracy: 0.8952 - f1_metric: 0.8955 - val_loss: 0.3169 - val_accuracy: 0.8657 - val_f1_metric: 0.8658\n",
      "Epoch 430/3000\n",
      "2752/2797 [============================>.] - ETA: 0s - loss: 0.1450 - accuracy: 0.9052 - f1_metric: 0.9052\n",
      "Epoch 00430: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 105us/sample - loss: 0.1459 - accuracy: 0.9042 - f1_metric: 0.9038 - val_loss: 0.3413 - val_accuracy: 0.8624 - val_f1_metric: 0.8622\n",
      "Epoch 431/3000\n",
      "2112/2797 [=====================>........] - ETA: 0s - loss: 0.1391 - accuracy: 0.8897 - f1_metric: 0.8897\n",
      "Epoch 00431: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 101us/sample - loss: 0.1420 - accuracy: 0.8942 - f1_metric: 0.8944 - val_loss: 0.3103 - val_accuracy: 0.8757 - val_f1_metric: 0.8757\n",
      "Epoch 432/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1448 - accuracy: 0.8989 - f1_metric: 0.8989\n",
      "Epoch 00432: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 93us/sample - loss: 0.1461 - accuracy: 0.8970 - f1_metric: 0.8970 - val_loss: 0.3364 - val_accuracy: 0.8649 - val_f1_metric: 0.8647\n",
      "Epoch 433/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1426 - accuracy: 0.8995 - f1_metric: 0.8995\n",
      "Epoch 00433: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 92us/sample - loss: 0.1407 - accuracy: 0.9006 - f1_metric: 0.9010 - val_loss: 0.3248 - val_accuracy: 0.8649 - val_f1_metric: 0.8647\n",
      "Epoch 434/3000\n",
      "2560/2797 [==========================>...] - ETA: 0s - loss: 0.1478 - accuracy: 0.8922 - f1_metric: 0.8922\n",
      "Epoch 00434: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 139us/sample - loss: 0.1461 - accuracy: 0.8942 - f1_metric: 0.8944 - val_loss: 0.3055 - val_accuracy: 0.8732 - val_f1_metric: 0.8732\n",
      "Epoch 435/3000\n",
      "2112/2797 [=====================>........] - ETA: 0s - loss: 0.1457 - accuracy: 0.9029 - f1_metric: 0.9029\n",
      "Epoch 00435: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 102us/sample - loss: 0.1410 - accuracy: 0.9045 - f1_metric: 0.9046 - val_loss: 0.3425 - val_accuracy: 0.8540 - val_f1_metric: 0.8540\n",
      "Epoch 436/3000\n",
      "2048/2797 [====================>.........] - ETA: 0s - loss: 0.1463 - accuracy: 0.8945 - f1_metric: 0.8945\n",
      "Epoch 00436: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 99us/sample - loss: 0.1522 - accuracy: 0.8931 - f1_metric: 0.8935 - val_loss: 0.3418 - val_accuracy: 0.8574 - val_f1_metric: 0.8573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 437/3000\n",
      "2112/2797 [=====================>........] - ETA: 0s - loss: 0.1413 - accuracy: 0.8958 - f1_metric: 0.8958\n",
      "Epoch 00437: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 99us/sample - loss: 0.1413 - accuracy: 0.8924 - f1_metric: 0.8925 - val_loss: 0.3301 - val_accuracy: 0.8616 - val_f1_metric: 0.8614\n",
      "Epoch 438/3000\n",
      "2624/2797 [===========================>..] - ETA: 0s - loss: 0.1493 - accuracy: 0.8910 - f1_metric: 0.8910\n",
      "Epoch 00438: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 108us/sample - loss: 0.1490 - accuracy: 0.8895 - f1_metric: 0.8891 - val_loss: 0.3388 - val_accuracy: 0.8565 - val_f1_metric: 0.8565\n",
      "Epoch 439/3000\n",
      "2688/2797 [===========================>..] - ETA: 0s - loss: 0.1372 - accuracy: 0.9014 - f1_metric: 0.9014\n",
      "Epoch 00439: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 108us/sample - loss: 0.1382 - accuracy: 0.9006 - f1_metric: 0.9004 - val_loss: 0.3222 - val_accuracy: 0.8732 - val_f1_metric: 0.8729\n",
      "Epoch 440/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.1257 - accuracy: 0.9035 - f1_metric: 0.9035\n",
      "Epoch 00440: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.1363 - accuracy: 0.9045 - f1_metric: 0.9046 - val_loss: 0.3191 - val_accuracy: 0.8749 - val_f1_metric: 0.8746\n",
      "Epoch 441/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1511 - accuracy: 0.8889 - f1_metric: 0.8889\n",
      "Epoch 00441: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 96us/sample - loss: 0.1469 - accuracy: 0.8917 - f1_metric: 0.8918 - val_loss: 0.3201 - val_accuracy: 0.8674 - val_f1_metric: 0.8672\n",
      "Epoch 442/3000\n",
      "2112/2797 [=====================>........] - ETA: 0s - loss: 0.1329 - accuracy: 0.9001 - f1_metric: 0.9001\n",
      "Epoch 00442: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 102us/sample - loss: 0.1335 - accuracy: 0.9017 - f1_metric: 0.9011 - val_loss: 0.3193 - val_accuracy: 0.8699 - val_f1_metric: 0.8696\n",
      "Epoch 443/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1441 - accuracy: 0.8969 - f1_metric: 0.8969\n",
      "Epoch 00443: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 95us/sample - loss: 0.1435 - accuracy: 0.8949 - f1_metric: 0.8945 - val_loss: 0.3462 - val_accuracy: 0.8599 - val_f1_metric: 0.8598\n",
      "Epoch 444/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.1396 - accuracy: 0.9058 - f1_metric: 0.9058\n",
      "Epoch 00444: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 95us/sample - loss: 0.1412 - accuracy: 0.9038 - f1_metric: 0.9037 - val_loss: 0.2982 - val_accuracy: 0.8774 - val_f1_metric: 0.8773\n",
      "Epoch 445/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.1418 - accuracy: 0.8984 - f1_metric: 0.8984\n",
      "Epoch 00445: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 92us/sample - loss: 0.1455 - accuracy: 0.9006 - f1_metric: 0.9004 - val_loss: 0.3143 - val_accuracy: 0.8691 - val_f1_metric: 0.8688\n",
      "Epoch 446/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1480 - accuracy: 0.8859 - f1_metric: 0.8859\n",
      "Epoch 00446: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.1475 - accuracy: 0.8845 - f1_metric: 0.8851 - val_loss: 0.3134 - val_accuracy: 0.8724 - val_f1_metric: 0.8724\n",
      "Epoch 447/3000\n",
      "2112/2797 [=====================>........] - ETA: 0s - loss: 0.1462 - accuracy: 0.9020 - f1_metric: 0.9020\n",
      "Epoch 00447: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 100us/sample - loss: 0.1561 - accuracy: 0.8917 - f1_metric: 0.8917 - val_loss: 0.3493 - val_accuracy: 0.8524 - val_f1_metric: 0.8524\n",
      "Epoch 448/3000\n",
      "2496/2797 [=========================>....] - ETA: 0s - loss: 0.1444 - accuracy: 0.8910 - f1_metric: 0.8910\n",
      "Epoch 00448: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 89us/sample - loss: 0.1461 - accuracy: 0.8920 - f1_metric: 0.8920 - val_loss: 0.3174 - val_accuracy: 0.8641 - val_f1_metric: 0.8639\n",
      "Epoch 449/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1430 - accuracy: 0.8924 - f1_metric: 0.8924\n",
      "Epoch 00449: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 96us/sample - loss: 0.1412 - accuracy: 0.8945 - f1_metric: 0.8942 - val_loss: 0.3069 - val_accuracy: 0.8707 - val_f1_metric: 0.8705\n",
      "Epoch 450/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1373 - accuracy: 0.9028 - f1_metric: 0.9028\n",
      "Epoch 00450: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 97us/sample - loss: 0.1370 - accuracy: 0.9020 - f1_metric: 0.9022 - val_loss: 0.3288 - val_accuracy: 0.8682 - val_f1_metric: 0.8680\n",
      "Epoch 451/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.1321 - accuracy: 0.8980 - f1_metric: 0.8980\n",
      "Epoch 00451: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 98us/sample - loss: 0.1318 - accuracy: 0.8988 - f1_metric: 0.8989 - val_loss: 0.3122 - val_accuracy: 0.8732 - val_f1_metric: 0.8732\n",
      "Epoch 452/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.1473 - accuracy: 0.8984 - f1_metric: 0.8984\n",
      "Epoch 00452: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 92us/sample - loss: 0.1454 - accuracy: 0.8945 - f1_metric: 0.8946 - val_loss: 0.3472 - val_accuracy: 0.8524 - val_f1_metric: 0.8524\n",
      "Epoch 453/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1367 - accuracy: 0.8951 - f1_metric: 0.8951\n",
      "Epoch 00453: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 95us/sample - loss: 0.1367 - accuracy: 0.8992 - f1_metric: 0.8994 - val_loss: 0.3076 - val_accuracy: 0.8749 - val_f1_metric: 0.8746\n",
      "Epoch 454/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.1503 - accuracy: 0.8956 - f1_metric: 0.8956\n",
      "Epoch 00454: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 95us/sample - loss: 0.1482 - accuracy: 0.8945 - f1_metric: 0.8945 - val_loss: 0.3362 - val_accuracy: 0.8632 - val_f1_metric: 0.8630\n",
      "Epoch 455/3000\n",
      "2112/2797 [=====================>........] - ETA: 0s - loss: 0.1507 - accuracy: 0.8963 - f1_metric: 0.8963\n",
      "Epoch 00455: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 104us/sample - loss: 0.1540 - accuracy: 0.8942 - f1_metric: 0.8940 - val_loss: 0.3535 - val_accuracy: 0.8540 - val_f1_metric: 0.8537\n",
      "Epoch 456/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1418 - accuracy: 0.8957 - f1_metric: 0.8957\n",
      "Epoch 00456: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 89us/sample - loss: 0.1461 - accuracy: 0.8974 - f1_metric: 0.8978 - val_loss: 0.3156 - val_accuracy: 0.8732 - val_f1_metric: 0.8732\n",
      "Epoch 457/3000\n",
      "2496/2797 [=========================>....] - ETA: 0s - loss: 0.1540 - accuracy: 0.8878 - f1_metric: 0.8878\n",
      "Epoch 00457: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 94us/sample - loss: 0.1486 - accuracy: 0.8906 - f1_metric: 0.8907 - val_loss: 0.3162 - val_accuracy: 0.8699 - val_f1_metric: 0.8696\n",
      "Epoch 458/3000\n",
      "2304/2797 [=======================>......] - ETA: 0s - loss: 0.1602 - accuracy: 0.8950 - f1_metric: 0.8950\n",
      "Epoch 00458: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 101us/sample - loss: 0.1549 - accuracy: 0.8981 - f1_metric: 0.8982 - val_loss: 0.3591 - val_accuracy: 0.8515 - val_f1_metric: 0.8515\n",
      "Epoch 459/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.1339 - accuracy: 0.9035 - f1_metric: 0.9035\n",
      "Epoch 00459: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 99us/sample - loss: 0.1451 - accuracy: 0.8992 - f1_metric: 0.8994 - val_loss: 0.3294 - val_accuracy: 0.8641 - val_f1_metric: 0.8636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 460/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1387 - accuracy: 0.9033 - f1_metric: 0.9033\n",
      "Epoch 00460: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 89us/sample - loss: 0.1415 - accuracy: 0.9003 - f1_metric: 0.9003 - val_loss: 0.3206 - val_accuracy: 0.8699 - val_f1_metric: 0.8696\n",
      "Epoch 461/3000\n",
      "2560/2797 [==========================>...] - ETA: 0s - loss: 0.1412 - accuracy: 0.8996 - f1_metric: 0.8996\n",
      "Epoch 00461: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 89us/sample - loss: 0.1406 - accuracy: 0.8981 - f1_metric: 0.8983 - val_loss: 0.3168 - val_accuracy: 0.8699 - val_f1_metric: 0.8693\n",
      "Epoch 462/3000\n",
      "2496/2797 [=========================>....] - ETA: 0s - loss: 0.1415 - accuracy: 0.9067 - f1_metric: 0.9067\n",
      "Epoch 00462: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 92us/sample - loss: 0.1446 - accuracy: 0.9053 - f1_metric: 0.9047 - val_loss: 0.3407 - val_accuracy: 0.8616 - val_f1_metric: 0.8614\n",
      "Epoch 463/3000\n",
      "2112/2797 [=====================>........] - ETA: 0s - loss: 0.1328 - accuracy: 0.8935 - f1_metric: 0.8935\n",
      "Epoch 00463: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 103us/sample - loss: 0.1415 - accuracy: 0.8924 - f1_metric: 0.8925 - val_loss: 0.3225 - val_accuracy: 0.8699 - val_f1_metric: 0.8699\n",
      "Epoch 464/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1365 - accuracy: 0.8944 - f1_metric: 0.8946\n",
      "Epoch 00464: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 93us/sample - loss: 0.1394 - accuracy: 0.8963 - f1_metric: 0.8961 - val_loss: 0.3161 - val_accuracy: 0.8691 - val_f1_metric: 0.8691\n",
      "Epoch 465/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1352 - accuracy: 0.9027 - f1_metric: 0.9027\n",
      "Epoch 00465: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 101us/sample - loss: 0.1363 - accuracy: 0.9049 - f1_metric: 0.9046 - val_loss: 0.2951 - val_accuracy: 0.8782 - val_f1_metric: 0.8779\n",
      "Epoch 466/3000\n",
      "2688/2797 [===========================>..] - ETA: 0s - loss: 0.1329 - accuracy: 0.9059 - f1_metric: 0.9059\n",
      "Epoch 00466: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 106us/sample - loss: 0.1344 - accuracy: 0.9060 - f1_metric: 0.9056 - val_loss: 0.3167 - val_accuracy: 0.8716 - val_f1_metric: 0.8719\n",
      "Epoch 467/3000\n",
      "2688/2797 [===========================>..] - ETA: 0s - loss: 0.1345 - accuracy: 0.9018 - f1_metric: 0.9018\n",
      "Epoch 00467: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 109us/sample - loss: 0.1347 - accuracy: 0.9028 - f1_metric: 0.9030 - val_loss: 0.3225 - val_accuracy: 0.8682 - val_f1_metric: 0.8680\n",
      "Epoch 468/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.1514 - accuracy: 0.8883 - f1_metric: 0.8883\n",
      "Epoch 00468: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 96us/sample - loss: 0.1462 - accuracy: 0.8895 - f1_metric: 0.8898 - val_loss: 0.3271 - val_accuracy: 0.8632 - val_f1_metric: 0.8630\n",
      "Epoch 469/3000\n",
      "2112/2797 [=====================>........] - ETA: 0s - loss: 0.1342 - accuracy: 0.8954 - f1_metric: 0.8954\n",
      "Epoch 00469: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 99us/sample - loss: 0.1428 - accuracy: 0.8974 - f1_metric: 0.8970 - val_loss: 0.3330 - val_accuracy: 0.8616 - val_f1_metric: 0.8614\n",
      "Epoch 470/3000\n",
      "2560/2797 [==========================>...] - ETA: 0s - loss: 0.1372 - accuracy: 0.9047 - f1_metric: 0.9047\n",
      "Epoch 00470: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 156us/sample - loss: 0.1412 - accuracy: 0.9038 - f1_metric: 0.9036 - val_loss: 0.3267 - val_accuracy: 0.8682 - val_f1_metric: 0.8680\n",
      "Epoch 471/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.1412 - accuracy: 0.8902 - f1_metric: 0.8902\n",
      "Epoch 00471: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 99us/sample - loss: 0.1360 - accuracy: 0.8977 - f1_metric: 0.8978 - val_loss: 0.3026 - val_accuracy: 0.8741 - val_f1_metric: 0.8737\n",
      "Epoch 472/3000\n",
      "2176/2797 [======================>.......] - ETA: 0s - loss: 0.1372 - accuracy: 0.9058 - f1_metric: 0.9058\n",
      "Epoch 00472: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 96us/sample - loss: 0.1409 - accuracy: 0.9056 - f1_metric: 0.9058 - val_loss: 0.3352 - val_accuracy: 0.8599 - val_f1_metric: 0.8601\n",
      "Epoch 473/3000\n",
      "2240/2797 [=======================>......] - ETA: 0s - loss: 0.1489 - accuracy: 0.8862 - f1_metric: 0.8862\n",
      "Epoch 00473: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 107us/sample - loss: 0.1477 - accuracy: 0.8913 - f1_metric: 0.8916 - val_loss: 0.3153 - val_accuracy: 0.8666 - val_f1_metric: 0.8660\n",
      "Epoch 474/3000\n",
      "2368/2797 [========================>.....] - ETA: 0s - loss: 0.1434 - accuracy: 0.8961 - f1_metric: 0.8961\n",
      "Epoch 00474: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 95us/sample - loss: 0.1437 - accuracy: 0.8999 - f1_metric: 0.9004 - val_loss: 0.3021 - val_accuracy: 0.8741 - val_f1_metric: 0.8734\n",
      "Epoch 475/3000\n",
      "2432/2797 [=========================>....] - ETA: 0s - loss: 0.1393 - accuracy: 0.9021 - f1_metric: 0.9021\n",
      "Epoch 00475: val_f1_metric did not improve from 0.87815\n",
      "2797/2797 [==============================] - 0s 92us/sample - loss: 0.1376 - accuracy: 0.9010 - f1_metric: 0.9010 - val_loss: 0.3315 - val_accuracy: 0.8632 - val_f1_metric: 0.8630\n",
      "Epoch 00475: early stopping\n",
      "CPU times: user 3min 5s, sys: 19.5 s, total: 3min 25s\n",
      "Wall time: 2min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = model.fit(x_train, y_train, epochs=params['epochs'], verbose=1,\n",
    "                            batch_size=64, shuffle=True,\n",
    "                            # validation_split=0.3,\n",
    "                            validation_data=(x_cv, y_cv),\n",
    "                            callbacks=[mcp, rlp, es]\n",
    "                            , sample_weight=sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "InteractiveShell.ast_node_interactivity = \"last\"\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['f1_metric'])\n",
    "plt.plot(history.history['val_f1_metric'])\n",
    "\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train_loss', 'val_loss', 'f1', 'val_f1'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras evaluate= [0.28813228535652163, 0.889, 0.8857421]\n",
      "size of test set 1000\n",
      "TP class counts (array([0, 1, 2]), array([ 57,  55, 777]))\n",
      "True class counts (array([0, 1, 2]), array([ 64,  61, 875]))\n",
      "Pred class counts (array([0, 1, 2]), array([105, 105, 790]))\n",
      "baseline acc: 87.5\n",
      "[[ 57   0   7]\n",
      " [  0  55   6]\n",
      " [ 48  50 777]]\n",
      "F1 score (weighted) 0.9002599510467907\n",
      "F1 score (macro) 0.7568467162535745\n",
      "F1 score (micro) 0.889\n",
      "cohen's Kappa 0.624524312896406\n",
      "precision of class 0 = 0.89\n",
      "precision of class 1 = 0.9\n",
      "precision of class 2 = 0.89\n",
      "precision avg 0.8933333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_auc_score, cohen_kappa_score\n",
    "import seaborn as sns\n",
    "\n",
    "model = load_model(best_model_path)\n",
    "test_res = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"keras evaluate=\", test_res)\n",
    "pred = model.predict(x_test)\n",
    "pred_classes = np.argmax(pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "check_baseline(pred_classes, y_test_classes)\n",
    "conf_mat = confusion_matrix(y_test_classes, pred_classes)\n",
    "print(conf_mat)\n",
    "labels = [0,1,2]\n",
    "# ax = sns.heatmap(conf_mat, xticklabels=labels, yticklabels=labels, annot=True)\n",
    "# ax.xaxis.set_ticks_position('top')\n",
    "f1_weighted = f1_score(y_test_classes, pred_classes, labels=None, \n",
    "         average='weighted', sample_weight=None)\n",
    "print(\"F1 score (weighted)\", f1_weighted)\n",
    "print(\"F1 score (macro)\", f1_score(y_test_classes, pred_classes, labels=None, \n",
    "         average='macro', sample_weight=None))\n",
    "print(\"F1 score (micro)\", f1_score(y_test_classes, pred_classes, labels=None, \n",
    "         average='micro', sample_weight=None))  # weighted and micro preferred in case of imbalance\n",
    "# https://scikit-learn.org/stable/modules/model_evaluation.html#cohen-s-kappa --> supports multiclass; ref: https://stats.stackexchange.com/questions/82162/cohens-kappa-in-plain-english\n",
    "print(\"cohen's Kappa\", cohen_kappa_score(y_test_classes, pred_classes))\n",
    "\n",
    "prec = []\n",
    "for i, row in enumerate(conf_mat):\n",
    "    prec.append(np.round(row[i]/np.sum(row), 2))\n",
    "    print(\"precision of class {} = {}\".format(i, prec[i]))\n",
    "print(\"precision avg\", sum(prec)/len(prec))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* > baseline acc: 87.33333333333333\n",
    "> [[ 46   0  12]\n",
    ">  [  0  43  13]\n",
    ">  [ 43  40 703]]\n",
    "> F1 score (weighted) 0.8898645661513367\n",
    "> F1 score (macro) 0.7244070540348785\n",
    "> F1 score (micro) 0.88\n",
    "> precision of class 0 = 0.79\n",
    "> precision of class 1 = 0.77\n",
    "> precision of class 2 = 0.89\n",
    "-------------------------------------------------------------------------\n",
    "{'batch_size': 80, 'conv2d_layers': {'conv2d_do_1': 0.01, 'conv2d_filters_1': 25, 'conv2d_kernel_size_1': 2, 'conv2d_mp_1': 2, 'conv2d_strides_1': 1, 'kernel_regularizer_1': 0.01, 'conv2d_do_2': 0.01, 'conv2d_filters_2': 12, 'conv2d_kernel_size_2': 2, 'conv2d_mp_2': 2, 'conv2d_strides_2': 2, 'kernel_regularizer_2': 0.0, 'layers': 'two'}, 'dense_layers': {'dense_do_1': 0.01, 'dense_nodes_1': 100, 'kernel_regularizer_1': 0.01, 'layers': 'one'}, 'epochs': 3000, 'lr': 0.001, 'optimizer': 'adam'}\n",
    "> baseline acc: 87.33333333333333\n",
    "> [[ 47   0  11]\n",
    ">  [  0  40  16]\n",
    ">  [ 42  38 706]]\n",
    "> F1 score (weighted) 0.8901717584267965\n",
    "> F1 score (macro) 0.7220098760094632\n",
    "> F1 score (micro) 0.8811111111111111\n",
    "> precision of class 0 = 0.81\n",
    "> precision of class 1 = 0.71\n",
    "> precision of class 2 = 0.9\n",
    "-------------------------------------------------------------------------------\n",
    "{'batch_size': 80, 'conv2d_layers': {'conv2d_do_1': 0.015, 'conv2d_filters_1': 25, 'conv2d_kernel_size_1': 2, 'conv2d_mp_1': 2, 'conv2d_strides_1': 1, 'kernel_regularizer_1': 0.01, 'conv2d_do_2': 0.015, 'conv2d_filters_2': 12, 'conv2d_kernel_size_2': 2, 'conv2d_mp_2': 2, 'conv2d_strides_2': 2, 'kernel_regularizer_2': 0.0, 'layers': 'two'}, 'dense_layers': {'dense_do_1': 0.015, 'dense_nodes_1': 100, 'kernel_regularizer_1': 0.01, 'layers': 'one'}, 'epochs': 3000, 'lr': 0.001, 'optimizer': 'adam'}\n",
    "\n",
    "baseline acc: 87.33333333333333\n",
    "[[ 51   0   7]\n",
    " [  0  40  16]\n",
    " [ 34  33 719]]\n",
    "F1 score (weighted) 0.9064483333587193\n",
    "F1 score (macro) 0.7581804094953979\n",
    "F1 score (micro) 0.9\n",
    "precision of class 0 = 0.88\n",
    "precision of class 1 = 0.71\n",
    "precision of class 2 = 0.91\n",
    "\n",
    "--------------------------------------------\n",
    "{'batch_size': 80, 'conv2d_layers': {'conv2d_do_1': 0.015, 'conv2d_filters_1': 25, 'conv2d_kernel_size_1': 2, 'conv2d_mp_1': 2, 'conv2d_strides_1': 1, 'kernel_regularizer_1': 0.01, 'conv2d_do_2': 0.015, 'conv2d_filters_2': 12, 'conv2d_kernel_size_2': 2, 'conv2d_mp_2': 2, 'conv2d_strides_2': 2, 'kernel_regularizer_2': 0.01, 'layers': 'two'}, 'dense_layers': {'dense_do_1': 0.015, 'dense_nodes_1': 100, 'kernel_regularizer_1': 0.01, 'layers': 'one'}, 'epochs': 3000, 'lr': 0.001, 'optimizer': 'adam'}\n",
    "baseline acc: 87.33333333333333\n",
    "[[ 48   0  10]\n",
    " [  0  42  14]\n",
    " [ 63  51 672]]\n",
    "F1 score (weighted) 0.8636965910666424\n",
    "F1 score (macro) 0.679562772544323\n",
    "F1 score (micro) 0.8466666666666667\n",
    "precision of class 0 = 0.83\n",
    "precision of class 1 = 0.75\n",
    "precision of class 2 = 0.85\n",
    "\n",
    "---------------------------------------------------------------\n",
    "{'batch_size': 80, 'conv2d_layers': {'conv2d_do_1': 0.015, 'conv2d_filters_1': 25, 'conv2d_kernel_size_1': 2, 'conv2d_mp_1': 2, 'conv2d_strides_1': 1, 'kernel_regularizer_1': 0.01, 'conv2d_do_2': 0.015, 'conv2d_filters_2': 12, 'conv2d_kernel_size_2': 2, 'conv2d_mp_2': 2, 'conv2d_strides_2': 2, 'kernel_regularizer_2': 0.0, 'layers': 'two'}, 'dense_layers': {'dense_do_1': 0.015, 'dense_nodes_1': 100, 'kernel_regularizer_1': 0.02, 'layers': 'one'}, 'epochs': 3000, 'lr': 0.001, 'optimizer': 'adam'}\n",
    "\n",
    "baseline acc: 87.33333333333333\n",
    "[[ 51   0   7]\n",
    " [  0  41  15]\n",
    " [ 34  31 721]]\n",
    "F1 score (weighted) 0.9094692008630857\n",
    "F1 score (macro) 0.7656705928963122\n",
    "F1 score (micro) 0.9033333333333333\n",
    "precision of class 0 = 0.88\n",
    "precision of class 1 = 0.73\n",
    "precision of class 2 = 0.92\n",
    "\n",
    "------------------------------------------------------------------------------\n",
    "{'batch_size': 80, 'conv2d_layers': {'conv2d_do_1': 0.015, 'conv2d_filters_1': 25, 'conv2d_kernel_size_1': 2,\n",
    "                                               'conv2d_mp_1': 2, 'conv2d_strides_1': 1, 'kernel_regularizer_1': 0.01, \n",
    "                                               'conv2d_do_2': 0.015, 'conv2d_filters_2': 12, 'conv2d_kernel_size_2': 2, \n",
    "                                               'conv2d_mp_2': 2, 'conv2d_strides_2': 2, 'kernel_regularizer_2': 0.0, 'layers': 'two'},\n",
    "           'dense_layers': {'dense_do_1': 0.015, 'dense_nodes_1': 100, 'kernel_regularizer_1': 0.015, 'layers': 'one'}, 'epochs': 3000,\n",
    "           'lr': 0.001, 'optimizer': 'adam'}\n",
    "\n",
    "[[ 52   0   6]\n",
    " [  0  42  14]\n",
    " [ 42  41 703]]\n",
    "F1 score (weighted) 0.8954175974904913\n",
    "F1 score (macro) 0.7400899830517504\n",
    "F1 score (micro) 0.8855555555555555\n",
    "precision of class 0 = 0.9\n",
    "precision of class 1 = 0.75\n",
    "precision of class 2 = 0.89\n",
    "\n",
    "-------------------------------------------------------\n",
    "IBM\n",
    "----------------------------------------------------------------------------\n",
    "* **{'batch_size': 80, 'conv2d_layers': {'conv2d_do_1': 0.2, 'conv2d_filters_1': 30, 'conv2d_kernel_size_1': 2, 'conv2d_mp_1': 2, 'conv2d_strides_1': 1, 'kernel_regularizer_1': 0.0, 'conv2d_do_2': 0.05, 'conv2d_filters_2': 15, 'conv2d_kernel_size_2': 2, 'conv2d_mp_2': 2, 'conv2d_strides_2': 2, 'kernel_regularizer_2': 0.0, 'layers': 'two'}, 'dense_layers': {'dense_do_1': 0.2, 'dense_nodes_1': 100, 'kernel_regularizer_1': 0.0, 'layers': 'one'}, 'epochs': 3000, 'lr': 0.001, 'optimizer': 'adam'}**\n",
    "\n",
    "baseline acc: 87.44444444444444\n",
    "\n",
    "[[ 46   0  12]\n",
    " [  0  47   8]\n",
    " [ 35  53 699]]\n",
    "\n",
    "F1 score (weighted) 0.8914500898959538\n",
    "\n",
    "F1 score (macro) 0.7322029896966632\n",
    "\n",
    "F1 score (micro) 0.88\n",
    "\n",
    "cohen's Kappa 0.5845248323352525, sometimes 0.61\n",
    "\n",
    "precision of class 0 = 0.79\n",
    "\n",
    "precision of class 1 = 0.85\n",
    "\n",
    "precision of class 2 = 0.89\n",
    "\n",
    "--------------------------------------------------\n",
    "* {'batch_size': 80, 'conv2d_layers': {'conv2d_do_1': 0.2, 'conv2d_filters_1': 30, 'conv2d_kernel_size_1': 2,\n",
    "                                               'conv2d_mp_1': 2, 'conv2d_strides_1': 1, 'kernel_regularizer_1': 0.0, \n",
    "                                               'conv2d_do_2': 0.07, 'conv2d_filters_2': 20, 'conv2d_kernel_size_2': 2, \n",
    "                                               'conv2d_mp_2': 3, 'conv2d_strides_2': 2, 'kernel_regularizer_2': 0.0, 'layers': 'two'},\n",
    "           'dense_layers': {'dense_do_1': 0.2, 'dense_nodes_1': 100, 'kernel_regularizer_1': 0.0, 'layers': 'one'}, 'epochs': 3000,\n",
    "           'lr': 0.001, 'optimizer': 'adam'}\n",
    "\n",
    "baseline acc: 87.44444444444444\n",
    "[[ 42   0  16]\n",
    " [  0  44  11]\n",
    " [ 31  39 717]]\n",
    "F1 score (weighted) 0.8993345798690069\n",
    "F1 score (macro) 0.7385150835481354\n",
    "F1 score (micro) 0.8922222222222224\n",
    "cohen's Kappa 0.5952205422097341\n",
    "precision of class 0 = 0.72\n",
    "precision of class 1 = 0.8\n",
    "precision of class 2 = 0.91\n",
    "precision avg 0.81\n",
    "\n",
    "WMT\n",
    "------------------------------------------------------\n",
    "{'batch_size': 80, 'conv2d_layers': {'conv2d_do_1': 0.2, 'conv2d_filters_1': 30, 'conv2d_kernel_size_1': 2, 'conv2d_mp_1': 2, 'conv2d_strides_1': 1, 'kernel_regularizer_1': 0.0, 'conv2d_do_2': 0.05, 'conv2d_filters_2': 15, 'conv2d_kernel_size_2': 2, 'conv2d_mp_2': 2, 'conv2d_strides_2': 2, 'kernel_regularizer_2': 0.0, 'layers': 'two'}, 'dense_layers': {'dense_do_1': 0.2, 'dense_nodes_1': 100, 'kernel_regularizer_1': 0.0, 'layers': 'one'}, 'epochs': 3000, 'lr': 0.001, 'optimizer': 'adam'}\n",
    "\n",
    "baseline acc: 87.33333333333333\n",
    "[[ 52   0   6]\n",
    " [  0  44  12]\n",
    " [ 47  42 697]]\n",
    "F1 score (weighted) 0.8923266236465278\n",
    "F1 score (macro) 0.7369509608548\n",
    "F1 score (micro) 0.8811111111111111\n",
    "cohen's Kappa 0.5944443508582787\n",
    "precision of class 0 = 0.9\n",
    "precision of class 1 = 0.79\n",
    "precision of class 2 = 0.89\n",
    "precision avg 0.86\n",
    "NOTE: same config with strides_1=2 gave exceptional (90+%) accuracy on classes 0,1 but class2 84%.\n",
    "\n",
    "* **{'batch_size': 80, 'conv2d_layers': {'conv2d_do_1': 0.22, 'conv2d_filters_1': 35, 'conv2d_kernel_size_1': 2, 'conv2d_mp_1': 2, 'conv2d_strides_1': 1, 'kernel_regularizer_1': 0.0, 'conv2d_do_2': 0.05, 'conv2d_filters_2': 20, 'conv2d_kernel_size_2': 2, 'conv2d_mp_2': 2, 'conv2d_strides_2': 2, 'kernel_regularizer_2': 0.0, 'layers': 'two'}, 'dense_layers': {'dense_do_1': 0.22, 'dense_nodes_1': 100, 'kernel_regularizer_1': 0.0, 'layers': 'one'}, 'epochs': 3000, 'lr': 0.001, 'optimizer': 'adam'}**\n",
    "    \n",
    "baseline acc: 87.33333333333333\n",
    "[[ 53   0   5]\n",
    " [  0  45  11]\n",
    " [ 39  30 717]]\n",
    "F1 score (weighted) 0.9127522951482708\n",
    "F1 score (macro) 0.7792439001374168\n",
    "F1 score (micro) 0.9055555555555556\n",
    "cohen's Kappa 0.6589784510043419\n",
    "precision of class 0 = 0.91\n",
    "precision of class 1 = 0.8\n",
    "precision of class 2 = 0.91\n",
    "precision avg 0.8733333333333334"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suffix = '_'+str(np.round(test_res[1],2))+'_'+str(np.round(f1_weighted,2))\n",
    "# model_path = os.path.join('..', 'outputs', 'model_'+company_code+suffix+'.h5')\n",
    "# model.save(model_path)\n",
    "# print(\"model save path\", os.path.join('..', 'outputs', 'model_'+company_code+suffix+'.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_cell_guid": "",
    "_uuid": ""
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6428571428571429"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 0.9\n",
    "r = 0.5\n",
    "\n",
    "(2*p*r)/(p+r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
